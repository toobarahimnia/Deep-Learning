{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECSE 552 A1, Q2 \n",
        "\n"
      ],
      "metadata": {
        "id": "iSQwyDHdp214"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dictionaries"
      ],
      "metadata": {
        "id": "YKFFHLd8rFD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import torch\n",
        "import math \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch import Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Wzd8-ge5IaWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generating dataset"
      ],
      "metadata": {
        "id": "-WqexDYwIcKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SIZE = 900\n",
        "data = []\n",
        "label = [0, 1]\n",
        "for i in range(DATA_SIZE):\n",
        "  r = np.random.normal(0, 1)\n",
        "  t = np.random.uniform(0, 2 * math.pi)\n",
        "  if np.random.rand() > 0.5:\n",
        "    x1 = r * math.cos(t)\n",
        "    x2 = r * math.sin(t) \n",
        "    data += ([x1, x2, label[0]])\n",
        "    \n",
        "  else:\n",
        "    x1 = (r + 5) * math.cos(t)\n",
        "    x2 = (r + 5) * math.sin(t) \n",
        "    data += ([x1, x2, label[1]])\n",
        "\n",
        "\n",
        "# convert pandas dataframe to pytorch tensor\n",
        "targets_df = pd.DataFrame(data=data)\n",
        "targets_df.columns = ['targets']\n",
        "torch_tensor = torch.tensor(targets_df['targets'].values)\n",
        "torch_tensor = np.reshape(torch_tensor, (DATA_SIZE, 3))\n",
        "train, valid = random_split(torch_tensor,[600,300])\n",
        "trainloader = DataLoader(train, batch_size=3)\n",
        "validloader = DataLoader(valid, batch_size=3)\n",
        "all_data_loader = DataLoader(torch_tensor, batch_size=3)\n",
        "\n",
        "\n",
        "for i, batch in enumerate(validloader):\n",
        "  X_valid = batch[:, :2]\n",
        "  y_valid = batch[:, 2] \n",
        "  plt.scatter(X_valid[:,0],X_valid[:,1],s=40,c=y_valid)\n",
        "  plt.title('Sample data plots for classification')\n",
        "\n",
        "\n",
        "for i, batch in enumerate(trainloader):\n",
        "  X_train = batch[:, :2]\n",
        "  y_train = batch[:, 2]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "UVdTuIM9cmCZ",
        "outputId": "2bc98aef-922a-4122-d89a-cb3c45eb37e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## designing Neural Network"
      ],
      "metadata": {
        "id": "yTQ3R3CerK-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZJ3EUHZppAx"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        # network consists of 3 layers. 1 input(2 nodes), 1 hidden(4 nodes) and 1 output(2 nodes) layer\n",
        "        # This applies Linear transformation to input data \n",
        "        self.fc1 = nn.Linear(2,5)\n",
        "        # This applies linear transformation to produce output data\n",
        "        self.fc2 = nn.Linear(5, 2)\n",
        "\n",
        "   \n",
        "    def forward(self,x):         \n",
        "        #Activation function is Relu\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #This produces output\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training model"
      ],
      "metadata": {
        "id": "OIk5FUAhSMAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the model        \n",
        "model = Net()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "#Define loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Define the optimizer (try both)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "X_train = X_train.float()\n",
        "y_train = y_train.type(torch.LongTensor)\n",
        "\n",
        "X_valid = X_valid.float()\n",
        "y_valid = y_valid.type(torch.LongTensor)\n",
        "\n",
        "#Number of epochs\n",
        "epochs = 1000\n",
        "\n",
        "min_valid_loss = np.inf\n",
        "\n",
        "# accuracy arrays for plot\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "\n",
        "for e in range(epochs):\n",
        "  train_loss = 0.0\n",
        "  if torch.cuda.is_available():\n",
        " \n",
        "    X_train, y_train = X_train.cuda(), y_train.cuda()\n",
        "\n",
        "  # Clear the gradients\n",
        "  optimizer.zero_grad()\n",
        "  # Forward Pass                               \n",
        "  target = model.forward(X_train)\n",
        "  \n",
        "  # Find the Loss\n",
        "  loss = criterion(target, y_train)\n",
        "  # Calculate gradients\n",
        "  loss.backward()\n",
        "  # Update Weights\n",
        "  optimizer.step()\n",
        "  # Calculate Loss\n",
        "  train_loss += loss.item()\n",
        "  train_acc.append(1 - (train_loss / len(trainloader)))\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  model.eval()     # Optional when not using Model Specific layer\n",
        " \n",
        "  if torch.cuda.is_available():\n",
        "    X_valid, y_valid = X_valid.cuda(), y_valid.cuda()\n",
        "        \n",
        "  # Forward Pass\n",
        "  target = model.forward(X_valid)\n",
        "  # Find the Loss\n",
        "  loss = criterion(target, y_valid)\n",
        "  # Calculate Loss\n",
        "  valid_loss += loss.item()\n",
        "  valid_acc.append(1 - (valid_loss / len(validloader)))\n",
        "  print(f'Epoch {e+1} \\t\\t Training Accuracy: {1 - (train_loss / len(trainloader))} \\t\\t Validation Accuracy: {1 - (valid_loss / len(validloader))}')\n",
        "    \n",
        "  if min_valid_loss > valid_loss:\n",
        "      print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "      min_valid_loss = valid_loss\n",
        "        \n",
        "      # Saving State Dict\n",
        "      torch.save(model.state_dict(), 'saved_model.pth')\n"
      ],
      "metadata": {
        "id": "M7GoTaaCJMhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fa73e4-c035-4430-951b-187093cb5349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Accuracy: 0.997065604031086 \t\t Validation Accuracy: 0.9929064387083053\n",
            "Validation Loss Decreased(inf--->0.709356) \t Saving The Model\n",
            "Epoch 2 \t\t Training Accuracy: 0.9970810532569885 \t\t Validation Accuracy: 0.9929294174909592\n",
            "Validation Loss Decreased(0.709356--->0.707058) \t Saving The Model\n",
            "Epoch 3 \t\t Training Accuracy: 0.9970965161919594 \t\t Validation Accuracy: 0.9929524493217469\n",
            "Validation Loss Decreased(0.707058--->0.704755) \t Saving The Model\n",
            "Epoch 4 \t\t Training Accuracy: 0.9971119904518128 \t\t Validation Accuracy: 0.9929755336046219\n",
            "Validation Loss Decreased(0.704755--->0.702447) \t Saving The Model\n",
            "Epoch 5 \t\t Training Accuracy: 0.9971274730563163 \t\t Validation Accuracy: 0.992998668551445\n",
            "Validation Loss Decreased(0.702447--->0.700133) \t Saving The Model\n",
            "Epoch 6 \t\t Training Accuracy: 0.9971429598331452 \t\t Validation Accuracy: 0.9930218535661698\n",
            "Validation Loss Decreased(0.700133--->0.697815) \t Saving The Model\n",
            "Epoch 7 \t\t Training Accuracy: 0.9971584495902062 \t\t Validation Accuracy: 0.9930450874567032\n",
            "Validation Loss Decreased(0.697815--->0.695491) \t Saving The Model\n",
            "Epoch 8 \t\t Training Accuracy: 0.997173937857151 \t\t Validation Accuracy: 0.993068368434906\n",
            "Validation Loss Decreased(0.695491--->0.693163) \t Saving The Model\n",
            "Epoch 9 \t\t Training Accuracy: 0.9971894219517707 \t\t Validation Accuracy: 0.9930916953086854\n",
            "Validation Loss Decreased(0.693163--->0.690830) \t Saving The Model\n",
            "Epoch 10 \t\t Training Accuracy: 0.9972048982977867 \t\t Validation Accuracy: 0.9931150650978089\n",
            "Validation Loss Decreased(0.690830--->0.688493) \t Saving The Model\n",
            "Epoch 11 \t\t Training Accuracy: 0.9972203633189202 \t\t Validation Accuracy: 0.9931384772062302\n",
            "Validation Loss Decreased(0.688493--->0.686152) \t Saving The Model\n",
            "Epoch 12 \t\t Training Accuracy: 0.9972358134388923 \t\t Validation Accuracy: 0.9931619280576706\n",
            "Validation Loss Decreased(0.686152--->0.683807) \t Saving The Model\n",
            "Epoch 13 \t\t Training Accuracy: 0.9972512456774711 \t\t Validation Accuracy: 0.9931854146718979\n",
            "Validation Loss Decreased(0.683807--->0.681459) \t Saving The Model\n",
            "Epoch 14 \t\t Training Accuracy: 0.9972666558623314 \t\t Validation Accuracy: 0.9932089358568191\n",
            "Validation Loss Decreased(0.681459--->0.679106) \t Saving The Model\n",
            "Epoch 15 \t\t Training Accuracy: 0.9972820407152176 \t\t Validation Accuracy: 0.9932324892282486\n",
            "Validation Loss Decreased(0.679106--->0.676751) \t Saving The Model\n",
            "Epoch 16 \t\t Training Accuracy: 0.9972973966598511 \t\t Validation Accuracy: 0.9932560712099076\n",
            "Validation Loss Decreased(0.676751--->0.674393) \t Saving The Model\n",
            "Epoch 17 \t\t Training Accuracy: 0.9973127198219299 \t\t Validation Accuracy: 0.9932796794176102\n",
            "Validation Loss Decreased(0.674393--->0.672032) \t Saving The Model\n",
            "Epoch 18 \t\t Training Accuracy: 0.9973280063271522 \t\t Validation Accuracy: 0.9933033102750778\n",
            "Validation Loss Decreased(0.672032--->0.669669) \t Saving The Model\n",
            "Epoch 19 \t\t Training Accuracy: 0.9973432537913323 \t\t Validation Accuracy: 0.9933269596099854\n",
            "Validation Loss Decreased(0.669669--->0.667304) \t Saving The Model\n",
            "Epoch 20 \t\t Training Accuracy: 0.9973584568500519 \t\t Validation Accuracy: 0.9933506268262863\n",
            "Validation Loss Decreased(0.667304--->0.664937) \t Saving The Model\n",
            "Epoch 21 \t\t Training Accuracy: 0.9973736131191253 \t\t Validation Accuracy: 0.9933743065595627\n",
            "Validation Loss Decreased(0.664937--->0.662569) \t Saving The Model\n",
            "Epoch 22 \t\t Training Accuracy: 0.997388719022274 \t\t Validation Accuracy: 0.9933979970216751\n",
            "Validation Loss Decreased(0.662569--->0.660200) \t Saving The Model\n",
            "Epoch 23 \t\t Training Accuracy: 0.9974037709832192 \t\t Validation Accuracy: 0.9934216922521591\n",
            "Validation Loss Decreased(0.660200--->0.657831) \t Saving The Model\n",
            "Epoch 24 \t\t Training Accuracy: 0.9974187657237052 \t\t Validation Accuracy: 0.9934453910589218\n",
            "Validation Loss Decreased(0.657831--->0.655461) \t Saving The Model\n",
            "Epoch 25 \t\t Training Accuracy: 0.9974336996674538 \t\t Validation Accuracy: 0.9934690886735916\n",
            "Validation Loss Decreased(0.655461--->0.653091) \t Saving The Model\n",
            "Epoch 26 \t\t Training Accuracy: 0.9974485698342324 \t\t Validation Accuracy: 0.9934927821159363\n",
            "Validation Loss Decreased(0.653091--->0.650722) \t Saving The Model\n",
            "Epoch 27 \t\t Training Accuracy: 0.9974633726477623 \t\t Validation Accuracy: 0.9935164672136306\n",
            "Validation Loss Decreased(0.650722--->0.648353) \t Saving The Model\n",
            "Epoch 28 \t\t Training Accuracy: 0.9974781057238579 \t\t Validation Accuracy: 0.9935401403903961\n",
            "Validation Loss Decreased(0.648353--->0.645986) \t Saving The Model\n",
            "Epoch 29 \t\t Training Accuracy: 0.9974927654862404 \t\t Validation Accuracy: 0.9935637980699539\n",
            "Validation Loss Decreased(0.645986--->0.643620) \t Saving The Model\n",
            "Epoch 30 \t\t Training Accuracy: 0.9975073496997356 \t\t Validation Accuracy: 0.9935874372720719\n",
            "Validation Loss Decreased(0.643620--->0.641256) \t Saving The Model\n",
            "Epoch 31 \t\t Training Accuracy: 0.9975218546390533 \t\t Validation Accuracy: 0.9936110526323318\n",
            "Validation Loss Decreased(0.641256--->0.638895) \t Saving The Model\n",
            "Epoch 32 \t\t Training Accuracy: 0.9975362785160542 \t\t Validation Accuracy: 0.993634642958641\n",
            "Validation Loss Decreased(0.638895--->0.636536) \t Saving The Model\n",
            "Epoch 33 \t\t Training Accuracy: 0.997550618648529 \t\t Validation Accuracy: 0.9936582022905349\n",
            "Validation Loss Decreased(0.636536--->0.634180) \t Saving The Model\n",
            "Epoch 34 \t\t Training Accuracy: 0.9975648725032806 \t\t Validation Accuracy: 0.9936817282438278\n",
            "Validation Loss Decreased(0.634180--->0.631827) \t Saving The Model\n",
            "Epoch 35 \t\t Training Accuracy: 0.9975790379941464 \t\t Validation Accuracy: 0.993705216050148\n",
            "Validation Loss Decreased(0.631827--->0.629478) \t Saving The Model\n",
            "Epoch 36 \t\t Training Accuracy: 0.9975931124389171 \t\t Validation Accuracy: 0.9937286639213562\n",
            "Validation Loss Decreased(0.629478--->0.627134) \t Saving The Model\n",
            "Epoch 37 \t\t Training Accuracy: 0.9976070940494537 \t\t Validation Accuracy: 0.9937520664930344\n",
            "Validation Loss Decreased(0.627134--->0.624793) \t Saving The Model\n",
            "Epoch 38 \t\t Training Accuracy: 0.9976209814846516 \t\t Validation Accuracy: 0.9937754225730896\n",
            "Validation Loss Decreased(0.624793--->0.622458) \t Saving The Model\n",
            "Epoch 39 \t\t Training Accuracy: 0.9976347722113132 \t\t Validation Accuracy: 0.9937987262010575\n",
            "Validation Loss Decreased(0.622458--->0.620127) \t Saving The Model\n",
            "Epoch 40 \t\t Training Accuracy: 0.9976484648883343 \t\t Validation Accuracy: 0.9938219767808915\n",
            "Validation Loss Decreased(0.620127--->0.617802) \t Saving The Model\n",
            "Epoch 41 \t\t Training Accuracy: 0.9976620577275753 \t\t Validation Accuracy: 0.9938451689481735\n",
            "Validation Loss Decreased(0.617802--->0.615483) \t Saving The Model\n",
            "Epoch 42 \t\t Training Accuracy: 0.9976755495369435 \t\t Validation Accuracy: 0.9938682997226715\n",
            "Validation Loss Decreased(0.615483--->0.613170) \t Saving The Model\n",
            "Epoch 43 \t\t Training Accuracy: 0.9976889391243458 \t\t Validation Accuracy: 0.993891367316246\n",
            "Validation Loss Decreased(0.613170--->0.610863) \t Saving The Model\n",
            "Epoch 44 \t\t Training Accuracy: 0.9977022249996662 \t\t Validation Accuracy: 0.9939143669605255\n",
            "Validation Loss Decreased(0.610863--->0.608563) \t Saving The Model\n",
            "Epoch 45 \t\t Training Accuracy: 0.9977154064178467 \t\t Validation Accuracy: 0.9939372974634171\n",
            "Validation Loss Decreased(0.608563--->0.606270) \t Saving The Model\n",
            "Epoch 46 \t\t Training Accuracy: 0.9977284824848175 \t\t Validation Accuracy: 0.9939601546525956\n",
            "Validation Loss Decreased(0.606270--->0.603985) \t Saving The Model\n",
            "Epoch 47 \t\t Training Accuracy: 0.9977414520084857 \t\t Validation Accuracy: 0.9939829367399216\n",
            "Validation Loss Decreased(0.603985--->0.601706) \t Saving The Model\n",
            "Epoch 48 \t\t Training Accuracy: 0.9977543146908283 \t\t Validation Accuracy: 0.9940056395530701\n",
            "Validation Loss Decreased(0.601706--->0.599436) \t Saving The Model\n",
            "Epoch 49 \t\t Training Accuracy: 0.9977670697867871 \t\t Validation Accuracy: 0.9940282630920411\n",
            "Validation Loss Decreased(0.599436--->0.597174) \t Saving The Model\n",
            "Epoch 50 \t\t Training Accuracy: 0.9977797169983387 \t\t Validation Accuracy: 0.9940508025884628\n",
            "Validation Loss Decreased(0.597174--->0.594920) \t Saving The Model\n",
            "Epoch 51 \t\t Training Accuracy: 0.9977922554314137 \t\t Validation Accuracy: 0.9940732562541962\n",
            "Validation Loss Decreased(0.594920--->0.592674) \t Saving The Model\n",
            "Epoch 52 \t\t Training Accuracy: 0.9978046853840351 \t\t Validation Accuracy: 0.9940956217050553\n",
            "Validation Loss Decreased(0.592674--->0.590438) \t Saving The Model\n",
            "Epoch 53 \t\t Training Accuracy: 0.997817006111145 \t\t Validation Accuracy: 0.9941178977489471\n",
            "Validation Loss Decreased(0.590438--->0.588210) \t Saving The Model\n",
            "Epoch 54 \t\t Training Accuracy: 0.9978292180597782 \t\t Validation Accuracy: 0.9941400808095932\n",
            "Validation Loss Decreased(0.588210--->0.585992) \t Saving The Model\n",
            "Epoch 55 \t\t Training Accuracy: 0.997841321080923 \t\t Validation Accuracy: 0.994162169098854\n",
            "Validation Loss Decreased(0.585992--->0.583783) \t Saving The Model\n",
            "Epoch 56 \t\t Training Accuracy: 0.9978533147275448 \t\t Validation Accuracy: 0.9941841620206833\n",
            "Validation Loss Decreased(0.583783--->0.581584) \t Saving The Model\n",
            "Epoch 57 \t\t Training Accuracy: 0.997865199893713 \t\t Validation Accuracy: 0.9942060565948486\n",
            "Validation Loss Decreased(0.581584--->0.579394) \t Saving The Model\n",
            "Epoch 58 \t\t Training Accuracy: 0.9978769762814045 \t\t Validation Accuracy: 0.9942278510332108\n",
            "Validation Loss Decreased(0.579394--->0.577215) \t Saving The Model\n",
            "Epoch 59 \t\t Training Accuracy: 0.9978886444866657 \t\t Validation Accuracy: 0.9942495447397232\n",
            "Validation Loss Decreased(0.577215--->0.575046) \t Saving The Model\n",
            "Epoch 60 \t\t Training Accuracy: 0.9979002051055431 \t\t Validation Accuracy: 0.9942711347341537\n",
            "Validation Loss Decreased(0.575046--->0.572887) \t Saving The Model\n",
            "Epoch 61 \t\t Training Accuracy: 0.9979116582870483 \t\t Validation Accuracy: 0.9942926210165024\n",
            "Validation Loss Decreased(0.572887--->0.570738) \t Saving The Model\n",
            "Epoch 62 \t\t Training Accuracy: 0.9979230040311813 \t\t Validation Accuracy: 0.9943140017986297\n",
            "Validation Loss Decreased(0.570738--->0.568600) \t Saving The Model\n",
            "Epoch 63 \t\t Training Accuracy: 0.997934243530035 \t\t Validation Accuracy: 0.9943352752923965\n",
            "Validation Loss Decreased(0.568600--->0.566472) \t Saving The Model\n",
            "Epoch 64 \t\t Training Accuracy: 0.9979453776776791 \t\t Validation Accuracy: 0.9943564403057098\n",
            "Validation Loss Decreased(0.566472--->0.564356) \t Saving The Model\n",
            "Epoch 65 \t\t Training Accuracy: 0.9979564063251019 \t\t Validation Accuracy: 0.9943774956464767\n",
            "Validation Loss Decreased(0.564356--->0.562250) \t Saving The Model\n",
            "Epoch 66 \t\t Training Accuracy: 0.9979673306643962 \t\t Validation Accuracy: 0.9943984407186508\n",
            "Validation Loss Decreased(0.562250--->0.560156) \t Saving The Model\n",
            "Epoch 67 \t\t Training Accuracy: 0.9979781509935856 \t\t Validation Accuracy: 0.9944192743301392\n",
            "Validation Loss Decreased(0.560156--->0.558073) \t Saving The Model\n",
            "Epoch 68 \t\t Training Accuracy: 0.9979888683557511 \t\t Validation Accuracy: 0.9944399970769883\n",
            "Validation Loss Decreased(0.558073--->0.556000) \t Saving The Model\n",
            "Epoch 69 \t\t Training Accuracy: 0.9979994840919971 \t\t Validation Accuracy: 0.9944606053829194\n",
            "Validation Loss Decreased(0.556000--->0.553939) \t Saving The Model\n",
            "Epoch 70 \t\t Training Accuracy: 0.9980099983513355 \t\t Validation Accuracy: 0.9944810998439789\n",
            "Validation Loss Decreased(0.553939--->0.551890) \t Saving The Model\n",
            "Epoch 71 \t\t Training Accuracy: 0.998020412325859 \t\t Validation Accuracy: 0.994501480460167\n",
            "Validation Loss Decreased(0.551890--->0.549852) \t Saving The Model\n",
            "Epoch 72 \t\t Training Accuracy: 0.9980307269096375 \t\t Validation Accuracy: 0.9945217460393906\n",
            "Validation Loss Decreased(0.549852--->0.547825) \t Saving The Model\n",
            "Epoch 73 \t\t Training Accuracy: 0.998040943145752 \t\t Validation Accuracy: 0.9945418959856034\n",
            "Validation Loss Decreased(0.547825--->0.545810) \t Saving The Model\n",
            "Epoch 74 \t\t Training Accuracy: 0.9980510614812375 \t\t Validation Accuracy: 0.9945619302988052\n",
            "Validation Loss Decreased(0.545810--->0.543807) \t Saving The Model\n",
            "Epoch 75 \t\t Training Accuracy: 0.9980610835552216 \t\t Validation Accuracy: 0.9945818489789963\n",
            "Validation Loss Decreased(0.543807--->0.541815) \t Saving The Model\n",
            "Epoch 76 \t\t Training Accuracy: 0.998071009516716 \t\t Validation Accuracy: 0.9946016502380371\n",
            "Validation Loss Decreased(0.541815--->0.539835) \t Saving The Model\n",
            "Epoch 77 \t\t Training Accuracy: 0.9980808413028717 \t\t Validation Accuracy: 0.9946213346719742\n",
            "Validation Loss Decreased(0.539835--->0.537867) \t Saving The Model\n",
            "Epoch 78 \t\t Training Accuracy: 0.9980905790627003 \t\t Validation Accuracy: 0.994640901684761\n",
            "Validation Loss Decreased(0.537867--->0.535910) \t Saving The Model\n",
            "Epoch 79 \t\t Training Accuracy: 0.9981002248823643 \t\t Validation Accuracy: 0.9946603518724442\n",
            "Validation Loss Decreased(0.535910--->0.533965) \t Saving The Model\n",
            "Epoch 80 \t\t Training Accuracy: 0.9981097789108753 \t\t Validation Accuracy: 0.9946796852350235\n",
            "Validation Loss Decreased(0.533965--->0.532031) \t Saving The Model\n",
            "Epoch 81 \t\t Training Accuracy: 0.9981192427873612 \t\t Validation Accuracy: 0.9946989005804062\n",
            "Validation Loss Decreased(0.532031--->0.530110) \t Saving The Model\n",
            "Epoch 82 \t\t Training Accuracy: 0.998128616809845 \t\t Validation Accuracy: 0.9947179979085923\n",
            "Validation Loss Decreased(0.530110--->0.528200) \t Saving The Model\n",
            "Epoch 83 \t\t Training Accuracy: 0.998137806802988 \t\t Validation Accuracy: 0.9947366505861283\n",
            "Validation Loss Decreased(0.528200--->0.526335) \t Saving The Model\n",
            "Epoch 84 \t\t Training Accuracy: 0.9981467382609844 \t\t Validation Accuracy: 0.9947551959753036\n",
            "Validation Loss Decreased(0.526335--->0.524480) \t Saving The Model\n",
            "Epoch 85 \t\t Training Accuracy: 0.9981555883586407 \t\t Validation Accuracy: 0.9947736328840255\n",
            "Validation Loss Decreased(0.524480--->0.522637) \t Saving The Model\n",
            "Epoch 86 \t\t Training Accuracy: 0.998164358139038 \t\t Validation Accuracy: 0.9947919636964798\n",
            "Validation Loss Decreased(0.522637--->0.520804) \t Saving The Model\n",
            "Epoch 87 \t\t Training Accuracy: 0.9981730480492115 \t\t Validation Accuracy: 0.9948101860284805\n",
            "Validation Loss Decreased(0.520804--->0.518981) \t Saving The Model\n",
            "Epoch 88 \t\t Training Accuracy: 0.9981816597282887 \t\t Validation Accuracy: 0.9948283022642136\n",
            "Validation Loss Decreased(0.518981--->0.517170) \t Saving The Model\n",
            "Epoch 89 \t\t Training Accuracy: 0.998190193772316 \t\t Validation Accuracy: 0.9948463106155395\n",
            "Validation Loss Decreased(0.517170--->0.515369) \t Saving The Model\n",
            "Epoch 90 \t\t Training Accuracy: 0.998198651522398 \t\t Validation Accuracy: 0.9948642122745514\n",
            "Validation Loss Decreased(0.515369--->0.513579) \t Saving The Model\n",
            "Epoch 91 \t\t Training Accuracy: 0.9982070334255695 \t\t Validation Accuracy: 0.9948820066452027\n",
            "Validation Loss Decreased(0.513579--->0.511799) \t Saving The Model\n",
            "Epoch 92 \t\t Training Accuracy: 0.9982153403759003 \t\t Validation Accuracy: 0.9948996949195862\n",
            "Validation Loss Decreased(0.511799--->0.510031) \t Saving The Model\n",
            "Epoch 93 \t\t Training Accuracy: 0.9982235741615295 \t\t Validation Accuracy: 0.9949172759056091\n",
            "Validation Loss Decreased(0.510031--->0.508272) \t Saving The Model\n",
            "Epoch 94 \t\t Training Accuracy: 0.9982317347824573 \t\t Validation Accuracy: 0.9949347513914109\n",
            "Validation Loss Decreased(0.508272--->0.506525) \t Saving The Model\n",
            "Epoch 95 \t\t Training Accuracy: 0.9982398240268231 \t\t Validation Accuracy: 0.9949521201848984\n",
            "Validation Loss Decreased(0.506525--->0.504788) \t Saving The Model\n",
            "Epoch 96 \t\t Training Accuracy: 0.998247841745615 \t\t Validation Accuracy: 0.9949693834781647\n",
            "Validation Loss Decreased(0.504788--->0.503062) \t Saving The Model\n",
            "Epoch 97 \t\t Training Accuracy: 0.9982557900249958 \t\t Validation Accuracy: 0.9949865418672562\n",
            "Validation Loss Decreased(0.503062--->0.501346) \t Saving The Model\n",
            "Epoch 98 \t\t Training Accuracy: 0.998263669013977 \t\t Validation Accuracy: 0.9950035953521729\n",
            "Validation Loss Decreased(0.501346--->0.499640) \t Saving The Model\n",
            "Epoch 99 \t\t Training Accuracy: 0.9982714796066284 \t\t Validation Accuracy: 0.995020544230938\n",
            "Validation Loss Decreased(0.499640--->0.497946) \t Saving The Model\n",
            "Epoch 100 \t\t Training Accuracy: 0.9982792234420776 \t\t Validation Accuracy: 0.9950373885035515\n",
            "Validation Loss Decreased(0.497946--->0.496261) \t Saving The Model\n",
            "Epoch 101 \t\t Training Accuracy: 0.9982869000732899 \t\t Validation Accuracy: 0.9950541290640831\n",
            "Validation Loss Decreased(0.496261--->0.494587) \t Saving The Model\n",
            "Epoch 102 \t\t Training Accuracy: 0.9982945120334625 \t\t Validation Accuracy: 0.9950707659125329\n",
            "Validation Loss Decreased(0.494587--->0.492923) \t Saving The Model\n",
            "Epoch 103 \t\t Training Accuracy: 0.9983020587265492 \t\t Validation Accuracy: 0.9950873002409935\n",
            "Validation Loss Decreased(0.492923--->0.491270) \t Saving The Model\n",
            "Epoch 104 \t\t Training Accuracy: 0.9983095417916775 \t\t Validation Accuracy: 0.9951037320494652\n",
            "Validation Loss Decreased(0.491270--->0.489627) \t Saving The Model\n",
            "Epoch 105 \t\t Training Accuracy: 0.9983169619739055 \t\t Validation Accuracy: 0.9951200616359711\n",
            "Validation Loss Decreased(0.489627--->0.487994) \t Saving The Model\n",
            "Epoch 106 \t\t Training Accuracy: 0.9983243198692798 \t\t Validation Accuracy: 0.9951362895965576\n",
            "Validation Loss Decreased(0.487994--->0.486371) \t Saving The Model\n",
            "Epoch 107 \t\t Training Accuracy: 0.9983316160738468 \t\t Validation Accuracy: 0.9951524171233177\n",
            "Validation Loss Decreased(0.486371--->0.484758) \t Saving The Model\n",
            "Epoch 108 \t\t Training Accuracy: 0.9983388522267341 \t\t Validation Accuracy: 0.995168443620205\n",
            "Validation Loss Decreased(0.484758--->0.483156) \t Saving The Model\n",
            "Epoch 109 \t\t Training Accuracy: 0.9983460284769535 \t\t Validation Accuracy: 0.9951843699812889\n",
            "Validation Loss Decreased(0.483156--->0.481563) \t Saving The Model\n",
            "Epoch 110 \t\t Training Accuracy: 0.9983531455695629 \t\t Validation Accuracy: 0.9952001973986626\n",
            "Validation Loss Decreased(0.481563--->0.479980) \t Saving The Model\n",
            "Epoch 111 \t\t Training Accuracy: 0.9983602045476436 \t\t Validation Accuracy: 0.9952159252762794\n",
            "Validation Loss Decreased(0.479980--->0.478407) \t Saving The Model\n",
            "Epoch 112 \t\t Training Accuracy: 0.9983672060072422 \t\t Validation Accuracy: 0.9952315557003021\n",
            "Validation Loss Decreased(0.478407--->0.476844) \t Saving The Model\n",
            "Epoch 113 \t\t Training Accuracy: 0.9983741508424282 \t\t Validation Accuracy: 0.9952470874786377\n",
            "Validation Loss Decreased(0.476844--->0.475291) \t Saving The Model\n",
            "Epoch 114 \t\t Training Accuracy: 0.9983810396492481 \t\t Validation Accuracy: 0.995262521803379\n",
            "Validation Loss Decreased(0.475291--->0.473748) \t Saving The Model\n",
            "Epoch 115 \t\t Training Accuracy: 0.9983878730237484 \t\t Validation Accuracy: 0.9952778598666191\n",
            "Validation Loss Decreased(0.473748--->0.472214) \t Saving The Model\n",
            "Epoch 116 \t\t Training Accuracy: 0.9983946518599986 \t\t Validation Accuracy: 0.9952931013703347\n",
            "Validation Loss Decreased(0.472214--->0.470690) \t Saving The Model\n",
            "Epoch 117 \t\t Training Accuracy: 0.9984013767540455 \t\t Validation Accuracy: 0.9953082478046418\n",
            "Validation Loss Decreased(0.470690--->0.469175) \t Saving The Model\n",
            "Epoch 118 \t\t Training Accuracy: 0.9984080485999585 \t\t Validation Accuracy: 0.9953232994675636\n",
            "Validation Loss Decreased(0.469175--->0.467670) \t Saving The Model\n",
            "Epoch 119 \t\t Training Accuracy: 0.9984146678447723 \t\t Validation Accuracy: 0.9953382560610771\n",
            "Validation Loss Decreased(0.467670--->0.466174) \t Saving The Model\n",
            "Epoch 120 \t\t Training Accuracy: 0.9984212352335453 \t\t Validation Accuracy: 0.9953531193733215\n",
            "Validation Loss Decreased(0.466174--->0.464688) \t Saving The Model\n",
            "Epoch 121 \t\t Training Accuracy: 0.9984277515113353 \t\t Validation Accuracy: 0.9953678897023202\n",
            "Validation Loss Decreased(0.464688--->0.463211) \t Saving The Model\n",
            "Epoch 122 \t\t Training Accuracy: 0.9984342171251774 \t\t Validation Accuracy: 0.9953825667500495\n",
            "Validation Loss Decreased(0.463211--->0.461743) \t Saving The Model\n",
            "Epoch 123 \t\t Training Accuracy: 0.998440632969141 \t\t Validation Accuracy: 0.9953971529006957\n",
            "Validation Loss Decreased(0.461743--->0.460285) \t Saving The Model\n",
            "Epoch 124 \t\t Training Accuracy: 0.9984469994902611 \t\t Validation Accuracy: 0.995411647260189\n",
            "Validation Loss Decreased(0.460285--->0.458835) \t Saving The Model\n",
            "Epoch 125 \t\t Training Accuracy: 0.9984533171355724 \t\t Validation Accuracy: 0.9954260501265526\n",
            "Validation Loss Decreased(0.458835--->0.457395) \t Saving The Model\n",
            "Epoch 126 \t\t Training Accuracy: 0.998459587097168 \t\t Validation Accuracy: 0.9954403641819954\n",
            "Validation Loss Decreased(0.457395--->0.455964) \t Saving The Model\n",
            "Epoch 127 \t\t Training Accuracy: 0.9984658092260361 \t\t Validation Accuracy: 0.9954545879364014\n",
            "Validation Loss Decreased(0.455964--->0.454541) \t Saving The Model\n",
            "Epoch 128 \t\t Training Accuracy: 0.9984719844162464 \t\t Validation Accuracy: 0.9954687225818634\n",
            "Validation Loss Decreased(0.454541--->0.453128) \t Saving The Model\n",
            "Epoch 129 \t\t Training Accuracy: 0.9984781134128571 \t\t Validation Accuracy: 0.9954827699065208\n",
            "Validation Loss Decreased(0.453128--->0.451723) \t Saving The Model\n",
            "Epoch 130 \t\t Training Accuracy: 0.9984841963648796 \t\t Validation Accuracy: 0.9954967281222343\n",
            "Validation Loss Decreased(0.451723--->0.450327) \t Saving The Model\n",
            "Epoch 131 \t\t Training Accuracy: 0.998490234464407 \t\t Validation Accuracy: 0.9955106005072594\n",
            "Validation Loss Decreased(0.450327--->0.448940) \t Saving The Model\n",
            "Epoch 132 \t\t Training Accuracy: 0.9984962275624275 \t\t Validation Accuracy: 0.995524385869503\n",
            "Validation Loss Decreased(0.448940--->0.447561) \t Saving The Model\n",
            "Epoch 133 \t\t Training Accuracy: 0.998502176553011 \t\t Validation Accuracy: 0.9955380854010581\n",
            "Validation Loss Decreased(0.447561--->0.446191) \t Saving The Model\n",
            "Epoch 134 \t\t Training Accuracy: 0.9985080820322036 \t\t Validation Accuracy: 0.9955516993999481\n",
            "Validation Loss Decreased(0.446191--->0.444830) \t Saving The Model\n",
            "Epoch 135 \t\t Training Accuracy: 0.9985139444470406 \t\t Validation Accuracy: 0.9955652293562889\n",
            "Validation Loss Decreased(0.444830--->0.443477) \t Saving The Model\n",
            "Epoch 136 \t\t Training Accuracy: 0.9985197640955448 \t\t Validation Accuracy: 0.9955786752700806\n",
            "Validation Loss Decreased(0.443477--->0.442132) \t Saving The Model\n",
            "Epoch 137 \t\t Training Accuracy: 0.9985255415737629 \t\t Validation Accuracy: 0.995592037141323\n",
            "Validation Loss Decreased(0.442132--->0.440796) \t Saving The Model\n",
            "Epoch 138 \t\t Training Accuracy: 0.9985312776267529 \t\t Validation Accuracy: 0.995605317056179\n",
            "Validation Loss Decreased(0.440796--->0.439468) \t Saving The Model\n",
            "Epoch 139 \t\t Training Accuracy: 0.9985369725525379 \t\t Validation Accuracy: 0.9956185141205788\n",
            "Validation Loss Decreased(0.439468--->0.438149) \t Saving The Model\n",
            "Epoch 140 \t\t Training Accuracy: 0.9985426266491413 \t\t Validation Accuracy: 0.9956316295266151\n",
            "Validation Loss Decreased(0.438149--->0.436837) \t Saving The Model\n",
            "Epoch 141 \t\t Training Accuracy: 0.9985482408106328 \t\t Validation Accuracy: 0.9956446644663811\n",
            "Validation Loss Decreased(0.436837--->0.435534) \t Saving The Model\n",
            "Epoch 142 \t\t Training Accuracy: 0.9985538150370121 \t\t Validation Accuracy: 0.9956576183438302\n",
            "Validation Loss Decreased(0.435534--->0.434238) \t Saving The Model\n",
            "Epoch 143 \t\t Training Accuracy: 0.9985593499243259 \t\t Validation Accuracy: 0.9956704929471016\n",
            "Validation Loss Decreased(0.434238--->0.432951) \t Saving The Model\n",
            "Epoch 144 \t\t Training Accuracy: 0.998564845919609 \t\t Validation Accuracy: 0.9956832882761956\n",
            "Validation Loss Decreased(0.432951--->0.431671) \t Saving The Model\n",
            "Epoch 145 \t\t Training Accuracy: 0.9985703037679196 \t\t Validation Accuracy: 0.9956960043311119\n",
            "Validation Loss Decreased(0.431671--->0.430400) \t Saving The Model\n",
            "Epoch 146 \t\t Training Accuracy: 0.9985757234692574 \t\t Validation Accuracy: 0.9957086431980133\n",
            "Validation Loss Decreased(0.430400--->0.429136) \t Saving The Model\n",
            "Epoch 147 \t\t Training Accuracy: 0.998581105619669 \t\t Validation Accuracy: 0.99572120398283\n",
            "Validation Loss Decreased(0.429136--->0.427880) \t Saving The Model\n",
            "Epoch 148 \t\t Training Accuracy: 0.9985864506661892 \t\t Validation Accuracy: 0.9957336881756782\n",
            "Validation Loss Decreased(0.427880--->0.426631) \t Saving The Model\n",
            "Epoch 149 \t\t Training Accuracy: 0.9985917590558528 \t\t Validation Accuracy: 0.9957460954785347\n",
            "Validation Loss Decreased(0.426631--->0.425390) \t Saving The Model\n",
            "Epoch 150 \t\t Training Accuracy: 0.9985970306396484 \t\t Validation Accuracy: 0.9957584276795387\n",
            "Validation Loss Decreased(0.425390--->0.424157) \t Saving The Model\n",
            "Epoch 151 \t\t Training Accuracy: 0.9986022667586804 \t\t Validation Accuracy: 0.9957706847786904\n",
            "Validation Loss Decreased(0.424157--->0.422932) \t Saving The Model\n",
            "Epoch 152 \t\t Training Accuracy: 0.9986074669659137 \t\t Validation Accuracy: 0.9957828667759895\n",
            "Validation Loss Decreased(0.422932--->0.421713) \t Saving The Model\n",
            "Epoch 153 \t\t Training Accuracy: 0.9986126326024533 \t\t Validation Accuracy: 0.9957949754595756\n",
            "Validation Loss Decreased(0.421713--->0.420502) \t Saving The Model\n",
            "Epoch 154 \t\t Training Accuracy: 0.9986177627742291 \t\t Validation Accuracy: 0.9958070099353791\n",
            "Validation Loss Decreased(0.420502--->0.419299) \t Saving The Model\n",
            "Epoch 155 \t\t Training Accuracy: 0.9986228589713574 \t\t Validation Accuracy: 0.9958189716935157\n",
            "Validation Loss Decreased(0.419299--->0.418103) \t Saving The Model\n",
            "Epoch 156 \t\t Training Accuracy: 0.9986279207468033 \t\t Validation Accuracy: 0.9958308610320091\n",
            "Validation Loss Decreased(0.418103--->0.416914) \t Saving The Model\n",
            "Epoch 157 \t\t Training Accuracy: 0.9986329491436482 \t\t Validation Accuracy: 0.9958426782488823\n",
            "Validation Loss Decreased(0.416914--->0.415732) \t Saving The Model\n",
            "Epoch 158 \t\t Training Accuracy: 0.9986379440128803 \t\t Validation Accuracy: 0.9958544242382049\n",
            "Validation Loss Decreased(0.415732--->0.414558) \t Saving The Model\n",
            "Epoch 159 \t\t Training Accuracy: 0.9986429059505463 \t\t Validation Accuracy: 0.9958661004900933\n",
            "Validation Loss Decreased(0.414558--->0.413390) \t Saving The Model\n",
            "Epoch 160 \t\t Training Accuracy: 0.9986478351056576 \t\t Validation Accuracy: 0.9958777049183846\n",
            "Validation Loss Decreased(0.413390--->0.412230) \t Saving The Model\n",
            "Epoch 161 \t\t Training Accuracy: 0.9986527319252491 \t\t Validation Accuracy: 0.9958892408013343\n",
            "Validation Loss Decreased(0.412230--->0.411076) \t Saving The Model\n",
            "Epoch 162 \t\t Training Accuracy: 0.9986575968563557 \t\t Validation Accuracy: 0.9959007069468498\n",
            "Validation Loss Decreased(0.411076--->0.409929) \t Saving The Model\n",
            "Epoch 163 \t\t Training Accuracy: 0.9986624300479889 \t\t Validation Accuracy: 0.9959121045470237\n",
            "Validation Loss Decreased(0.409929--->0.408790) \t Saving The Model\n",
            "Epoch 164 \t\t Training Accuracy: 0.9986672322452068 \t\t Validation Accuracy: 0.9959234347939492\n",
            "Validation Loss Decreased(0.408790--->0.407657) \t Saving The Model\n",
            "Epoch 165 \t\t Training Accuracy: 0.9986720030009747 \t\t Validation Accuracy: 0.995934696495533\n",
            "Validation Loss Decreased(0.407657--->0.406530) \t Saving The Model\n",
            "Epoch 166 \t\t Training Accuracy: 0.9986767430603504 \t\t Validation Accuracy: 0.9959458920359612\n",
            "Validation Loss Decreased(0.406530--->0.405411) \t Saving The Model\n",
            "Epoch 167 \t\t Training Accuracy: 0.9986814528703689 \t\t Validation Accuracy: 0.9959570202231407\n",
            "Validation Loss Decreased(0.405411--->0.404298) \t Saving The Model\n",
            "Epoch 168 \t\t Training Accuracy: 0.9986861325800419 \t\t Validation Accuracy: 0.9959680822491646\n",
            "Validation Loss Decreased(0.404298--->0.403192) \t Saving The Model\n",
            "Epoch 169 \t\t Training Accuracy: 0.9986907823383808 \t\t Validation Accuracy: 0.9959790793061256\n",
            "Validation Loss Decreased(0.403192--->0.402092) \t Saving The Model\n",
            "Epoch 170 \t\t Training Accuracy: 0.9986954027414322 \t\t Validation Accuracy: 0.9959900107979774\n",
            "Validation Loss Decreased(0.402092--->0.400999) \t Saving The Model\n",
            "Epoch 171 \t\t Training Accuracy: 0.9986999936401844 \t\t Validation Accuracy: 0.9960008788108826\n",
            "Validation Loss Decreased(0.400999--->0.399912) \t Saving The Model\n",
            "Epoch 172 \t\t Training Accuracy: 0.9987045557796955 \t\t Validation Accuracy: 0.9960116818547249\n",
            "Validation Loss Decreased(0.399912--->0.398832) \t Saving The Model\n",
            "Epoch 173 \t\t Training Accuracy: 0.9987090891599655 \t\t Validation Accuracy: 0.9960224211215973\n",
            "Validation Loss Decreased(0.398832--->0.397758) \t Saving The Model\n",
            "Epoch 174 \t\t Training Accuracy: 0.9987135942280293 \t\t Validation Accuracy: 0.9960330983996392\n",
            "Validation Loss Decreased(0.397758--->0.396690) \t Saving The Model\n",
            "Epoch 175 \t\t Training Accuracy: 0.9987180711328983 \t\t Validation Accuracy: 0.9960437121987343\n",
            "Validation Loss Decreased(0.396690--->0.395629) \t Saving The Model\n",
            "Epoch 176 \t\t Training Accuracy: 0.9987225203216076 \t\t Validation Accuracy: 0.9960542640089989\n",
            "Validation Loss Decreased(0.395629--->0.394574) \t Saving The Model\n",
            "Epoch 177 \t\t Training Accuracy: 0.998726941794157 \t\t Validation Accuracy: 0.9960647535324096\n",
            "Validation Loss Decreased(0.394574--->0.393525) \t Saving The Model\n",
            "Epoch 178 \t\t Training Accuracy: 0.9987313358485699 \t\t Validation Accuracy: 0.9960751828551292\n",
            "Validation Loss Decreased(0.393525--->0.392482) \t Saving The Model\n",
            "Epoch 179 \t\t Training Accuracy: 0.9987357030808925 \t\t Validation Accuracy: 0.9960855507850647\n",
            "Validation Loss Decreased(0.392482--->0.391445) \t Saving The Model\n",
            "Epoch 180 \t\t Training Accuracy: 0.9987400431931018 \t\t Validation Accuracy: 0.9960958588123322\n",
            "Validation Loss Decreased(0.391445--->0.390414) \t Saving The Model\n",
            "Epoch 181 \t\t Training Accuracy: 0.9987443570792675 \t\t Validation Accuracy: 0.9961061069369316\n",
            "Validation Loss Decreased(0.390414--->0.389389) \t Saving The Model\n",
            "Epoch 182 \t\t Training Accuracy: 0.9987486444413662 \t\t Validation Accuracy: 0.9961162957549096\n",
            "Validation Loss Decreased(0.389389--->0.388370) \t Saving The Model\n",
            "Epoch 183 \t\t Training Accuracy: 0.9987529057264328 \t\t Validation Accuracy: 0.9961264255642891\n",
            "Validation Loss Decreased(0.388370--->0.387357) \t Saving The Model\n",
            "Epoch 184 \t\t Training Accuracy: 0.9987571412324905 \t\t Validation Accuracy: 0.9961364969611168\n",
            "Validation Loss Decreased(0.387357--->0.386350) \t Saving The Model\n",
            "Epoch 185 \t\t Training Accuracy: 0.998761351108551 \t\t Validation Accuracy: 0.9961465096473694\n",
            "Validation Loss Decreased(0.386350--->0.385349) \t Saving The Model\n",
            "Epoch 186 \t\t Training Accuracy: 0.99876553542912 \t\t Validation Accuracy: 0.996156465113163\n",
            "Validation Loss Decreased(0.385349--->0.384353) \t Saving The Model\n",
            "Epoch 187 \t\t Training Accuracy: 0.9987696949392557 \t\t Validation Accuracy: 0.996166363954544\n",
            "Validation Loss Decreased(0.384353--->0.383364) \t Saving The Model\n",
            "Epoch 188 \t\t Training Accuracy: 0.9987738294154406 \t\t Validation Accuracy: 0.9961762061715126\n",
            "Validation Loss Decreased(0.383364--->0.382379) \t Saving The Model\n",
            "Epoch 189 \t\t Training Accuracy: 0.9987779393047095 \t\t Validation Accuracy: 0.9961859917640686\n",
            "Validation Loss Decreased(0.382379--->0.381401) \t Saving The Model\n",
            "Epoch 190 \t\t Training Accuracy: 0.9987820247560739 \t\t Validation Accuracy: 0.9961957216262818\n",
            "Validation Loss Decreased(0.381401--->0.380428) \t Saving The Model\n",
            "Epoch 191 \t\t Training Accuracy: 0.9987860857695341 \t\t Validation Accuracy: 0.996205395758152\n",
            "Validation Loss Decreased(0.380428--->0.379460) \t Saving The Model\n",
            "Epoch 192 \t\t Training Accuracy: 0.9987901226431132 \t\t Validation Accuracy: 0.9962150147557258\n",
            "Validation Loss Decreased(0.379460--->0.378499) \t Saving The Model\n",
            "Epoch 193 \t\t Training Accuracy: 0.9987941360473633 \t\t Validation Accuracy: 0.9962245798110962\n",
            "Validation Loss Decreased(0.378499--->0.377542) \t Saving The Model\n",
            "Epoch 194 \t\t Training Accuracy: 0.9987981257587671 \t\t Validation Accuracy: 0.9962340900301934\n",
            "Validation Loss Decreased(0.377542--->0.376591) \t Saving The Model\n",
            "Epoch 195 \t\t Training Accuracy: 0.9988020918518304 \t\t Validation Accuracy: 0.9962435469031334\n",
            "Validation Loss Decreased(0.376591--->0.375645) \t Saving The Model\n",
            "Epoch 196 \t\t Training Accuracy: 0.9988060349971056 \t\t Validation Accuracy: 0.9962529495358468\n",
            "Validation Loss Decreased(0.375645--->0.374705) \t Saving The Model\n",
            "Epoch 197 \t\t Training Accuracy: 0.9988099551200866 \t\t Validation Accuracy: 0.9962623000144959\n",
            "Validation Loss Decreased(0.374705--->0.373770) \t Saving The Model\n",
            "Epoch 198 \t\t Training Accuracy: 0.9988138523697853 \t\t Validation Accuracy: 0.9962715974450111\n",
            "Validation Loss Decreased(0.373770--->0.372840) \t Saving The Model\n",
            "Epoch 199 \t\t Training Accuracy: 0.9988177269697189 \t\t Validation Accuracy: 0.9962808430194855\n",
            "Validation Loss Decreased(0.372840--->0.371916) \t Saving The Model\n",
            "Epoch 200 \t\t Training Accuracy: 0.9988215792924166 \t\t Validation Accuracy: 0.9962900373339653\n",
            "Validation Loss Decreased(0.371916--->0.370996) \t Saving The Model\n",
            "Epoch 201 \t\t Training Accuracy: 0.9988254094868898 \t\t Validation Accuracy: 0.996299179494381\n",
            "Validation Loss Decreased(0.370996--->0.370082) \t Saving The Model\n",
            "Epoch 202 \t\t Training Accuracy: 0.998829217478633 \t\t Validation Accuracy: 0.9963082703948021\n",
            "Validation Loss Decreased(0.370082--->0.369173) \t Saving The Model\n",
            "Epoch 203 \t\t Training Accuracy: 0.9988330034166575 \t\t Validation Accuracy: 0.9963173112273216\n",
            "Validation Loss Decreased(0.369173--->0.368269) \t Saving The Model\n",
            "Epoch 204 \t\t Training Accuracy: 0.9988367682695389 \t\t Validation Accuracy: 0.9963263007998466\n",
            "Validation Loss Decreased(0.368269--->0.367370) \t Saving The Model\n",
            "Epoch 205 \t\t Training Accuracy: 0.9988405112177133 \t\t Validation Accuracy: 0.9963352409005165\n",
            "Validation Loss Decreased(0.367370--->0.366476) \t Saving The Model\n",
            "Epoch 206 \t\t Training Accuracy: 0.9988442331552505 \t\t Validation Accuracy: 0.9963441321253776\n",
            "Validation Loss Decreased(0.366476--->0.365587) \t Saving The Model\n",
            "Epoch 207 \t\t Training Accuracy: 0.9988479337096214 \t\t Validation Accuracy: 0.9963529729843139\n",
            "Validation Loss Decreased(0.365587--->0.364703) \t Saving The Model\n",
            "Epoch 208 \t\t Training Accuracy: 0.9988516133278609 \t\t Validation Accuracy: 0.9963617661595344\n",
            "Validation Loss Decreased(0.364703--->0.363823) \t Saving The Model\n",
            "Epoch 209 \t\t Training Accuracy: 0.9988552721589804 \t\t Validation Accuracy: 0.996370510160923\n",
            "Validation Loss Decreased(0.363823--->0.362949) \t Saving The Model\n",
            "Epoch 210 \t\t Training Accuracy: 0.998858910575509 \t\t Validation Accuracy: 0.9963792061805725\n",
            "Validation Loss Decreased(0.362949--->0.362079) \t Saving The Model\n",
            "Epoch 211 \t\t Training Accuracy: 0.9988625288009644 \t\t Validation Accuracy: 0.9963878545165062\n",
            "Validation Loss Decreased(0.362079--->0.361215) \t Saving The Model\n",
            "Epoch 212 \t\t Training Accuracy: 0.9988661264628171 \t\t Validation Accuracy: 0.9963964554667473\n",
            "Validation Loss Decreased(0.361215--->0.360354) \t Saving The Model\n",
            "Epoch 213 \t\t Training Accuracy: 0.9988697040081024 \t\t Validation Accuracy: 0.9964050099253654\n",
            "Validation Loss Decreased(0.360354--->0.359499) \t Saving The Model\n",
            "Epoch 214 \t\t Training Accuracy: 0.9988732615858317 \t\t Validation Accuracy: 0.9964135169982911\n",
            "Validation Loss Decreased(0.359499--->0.358648) \t Saving The Model\n",
            "Epoch 215 \t\t Training Accuracy: 0.9988767995685339 \t\t Validation Accuracy: 0.9964219772815704\n",
            "Validation Loss Decreased(0.358648--->0.357802) \t Saving The Model\n",
            "Epoch 216 \t\t Training Accuracy: 0.998880318030715 \t\t Validation Accuracy: 0.9964303922653198\n",
            "Validation Loss Decreased(0.357802--->0.356961) \t Saving The Model\n",
            "Epoch 217 \t\t Training Accuracy: 0.9988838168978691 \t\t Validation Accuracy: 0.996438761651516\n",
            "Validation Loss Decreased(0.356961--->0.356124) \t Saving The Model\n",
            "Epoch 218 \t\t Training Accuracy: 0.9988872963935137 \t\t Validation Accuracy: 0.9964470848441124\n",
            "Validation Loss Decreased(0.356124--->0.355292) \t Saving The Model\n",
            "Epoch 219 \t\t Training Accuracy: 0.9988907567411661 \t\t Validation Accuracy: 0.9964553633332253\n",
            "Validation Loss Decreased(0.355292--->0.354464) \t Saving The Model\n",
            "Epoch 220 \t\t Training Accuracy: 0.9988941983133555 \t\t Validation Accuracy: 0.9964635971188546\n",
            "Validation Loss Decreased(0.354464--->0.353640) \t Saving The Model\n",
            "Epoch 221 \t\t Training Accuracy: 0.99889762096107 \t\t Validation Accuracy: 0.9964717867970466\n",
            "Validation Loss Decreased(0.353640--->0.352821) \t Saving The Model\n",
            "Epoch 222 \t\t Training Accuracy: 0.9989010248333215 \t\t Validation Accuracy: 0.9964799323678016\n",
            "Validation Loss Decreased(0.352821--->0.352007) \t Saving The Model\n",
            "Epoch 223 \t\t Training Accuracy: 0.9989044102281333 \t\t Validation Accuracy: 0.9964880338311195\n",
            "Validation Loss Decreased(0.352007--->0.351197) \t Saving The Model\n",
            "Epoch 224 \t\t Training Accuracy: 0.9989077773690224 \t\t Validation Accuracy: 0.9964960923790932\n",
            "Validation Loss Decreased(0.351197--->0.350391) \t Saving The Model\n",
            "Epoch 225 \t\t Training Accuracy: 0.9989111260324717 \t\t Validation Accuracy: 0.9965041080117225\n",
            "Validation Loss Decreased(0.350391--->0.349589) \t Saving The Model\n",
            "Epoch 226 \t\t Training Accuracy: 0.9989144569635391 \t\t Validation Accuracy: 0.9965120804309845\n",
            "Validation Loss Decreased(0.349589--->0.348792) \t Saving The Model\n",
            "Epoch 227 \t\t Training Accuracy: 0.9989177697896957 \t\t Validation Accuracy: 0.9965200102329255\n",
            "Validation Loss Decreased(0.348792--->0.347999) \t Saving The Model\n",
            "Epoch 228 \t\t Training Accuracy: 0.998921064734459 \t\t Validation Accuracy: 0.9965278980135918\n",
            "Validation Loss Decreased(0.347999--->0.347210) \t Saving The Model\n",
            "Epoch 229 \t\t Training Accuracy: 0.9989243417978286 \t\t Validation Accuracy: 0.9965357434749603\n",
            "Validation Loss Decreased(0.347210--->0.346426) \t Saving The Model\n",
            "Epoch 230 \t\t Training Accuracy: 0.9989276016503572 \t\t Validation Accuracy: 0.9965435481071472\n",
            "Validation Loss Decreased(0.346426--->0.345645) \t Saving The Model\n",
            "Epoch 231 \t\t Training Accuracy: 0.9989308438450099 \t\t Validation Accuracy: 0.9965513110160827\n",
            "Validation Loss Decreased(0.345645--->0.344869) \t Saving The Model\n",
            "Epoch 232 \t\t Training Accuracy: 0.9989340687543153 \t\t Validation Accuracy: 0.996559033691883\n",
            "Validation Loss Decreased(0.344869--->0.344097) \t Saving The Model\n",
            "Epoch 233 \t\t Training Accuracy: 0.9989372766017914 \t\t Validation Accuracy: 0.9965667143464089\n",
            "Validation Loss Decreased(0.344097--->0.343329) \t Saving The Model\n",
            "Epoch 234 \t\t Training Accuracy: 0.9989404674619436 \t\t Validation Accuracy: 0.9965743553638459\n",
            "Validation Loss Decreased(0.343329--->0.342564) \t Saving The Model\n",
            "Epoch 235 \t\t Training Accuracy: 0.9989436415582895 \t\t Validation Accuracy: 0.9965819561481476\n",
            "Validation Loss Decreased(0.342564--->0.341804) \t Saving The Model\n",
            "Epoch 236 \t\t Training Accuracy: 0.9989467986673116 \t\t Validation Accuracy: 0.9965895172953606\n",
            "Validation Loss Decreased(0.341804--->0.341048) \t Saving The Model\n",
            "Epoch 237 \t\t Training Accuracy: 0.9989499393105506 \t\t Validation Accuracy: 0.9965970382094383\n",
            "Validation Loss Decreased(0.341048--->0.340296) \t Saving The Model\n",
            "Epoch 238 \t\t Training Accuracy: 0.9989530634880066 \t\t Validation Accuracy: 0.996604520380497\n",
            "Validation Loss Decreased(0.340296--->0.339548) \t Saving The Model\n",
            "Epoch 239 \t\t Training Accuracy: 0.9989561709016561 \t\t Validation Accuracy: 0.9966119635105133\n",
            "Validation Loss Decreased(0.339548--->0.338804) \t Saving The Model\n",
            "Epoch 240 \t\t Training Accuracy: 0.9989592625945807 \t\t Validation Accuracy: 0.9966193684935569\n",
            "Validation Loss Decreased(0.338804--->0.338063) \t Saving The Model\n",
            "Epoch 241 \t\t Training Accuracy: 0.9989623379707336 \t\t Validation Accuracy: 0.9966267347335815\n",
            "Validation Loss Decreased(0.338063--->0.337327) \t Saving The Model\n",
            "Epoch 242 \t\t Training Accuracy: 0.9989653971791267 \t\t Validation Accuracy: 0.996634062230587\n",
            "Validation Loss Decreased(0.337327--->0.336594) \t Saving The Model\n",
            "Epoch 243 \t\t Training Accuracy: 0.998968440592289 \t\t Validation Accuracy: 0.9966413521766663\n",
            "Validation Loss Decreased(0.336594--->0.335865) \t Saving The Model\n",
            "Epoch 244 \t\t Training Accuracy: 0.9989714682102203 \t\t Validation Accuracy: 0.9966486048698425\n",
            "Validation Loss Decreased(0.335865--->0.335140) \t Saving The Model\n",
            "Epoch 245 \t\t Training Accuracy: 0.9989744800329209 \t\t Validation Accuracy: 0.9966558191180229\n",
            "Validation Loss Decreased(0.335140--->0.334418) \t Saving The Model\n",
            "Epoch 246 \t\t Training Accuracy: 0.9989774767309427 \t\t Validation Accuracy: 0.9966629973053932\n",
            "Validation Loss Decreased(0.334418--->0.333700) \t Saving The Model\n",
            "Epoch 247 \t\t Training Accuracy: 0.9989804578572512 \t\t Validation Accuracy: 0.9966701379418373\n",
            "Validation Loss Decreased(0.333700--->0.332986) \t Saving The Model\n",
            "Epoch 248 \t\t Training Accuracy: 0.9989834233373404 \t\t Validation Accuracy: 0.9966772422194481\n",
            "Validation Loss Decreased(0.332986--->0.332276) \t Saving The Model\n",
            "Epoch 249 \t\t Training Accuracy: 0.9989863736182452 \t\t Validation Accuracy: 0.9966843098402023\n",
            "Validation Loss Decreased(0.332276--->0.331569) \t Saving The Model\n",
            "Epoch 250 \t\t Training Accuracy: 0.9989893092960119 \t\t Validation Accuracy: 0.9966913416981698\n",
            "Validation Loss Decreased(0.331569--->0.330866) \t Saving The Model\n",
            "Epoch 251 \t\t Training Accuracy: 0.9989922294020652 \t\t Validation Accuracy: 0.9966983380913734\n",
            "Validation Loss Decreased(0.330866--->0.330166) \t Saving The Model\n",
            "Epoch 252 \t\t Training Accuracy: 0.998995135128498 \t\t Validation Accuracy: 0.9967052981257438\n",
            "Validation Loss Decreased(0.330166--->0.329470) \t Saving The Model\n",
            "Epoch 253 \t\t Training Accuracy: 0.9989980258792639 \t\t Validation Accuracy: 0.9967122235894204\n",
            "Validation Loss Decreased(0.329470--->0.328778) \t Saving The Model\n",
            "Epoch 254 \t\t Training Accuracy: 0.9990009019523859 \t\t Validation Accuracy: 0.9967191132903099\n",
            "Validation Loss Decreased(0.328778--->0.328089) \t Saving The Model\n",
            "Epoch 255 \t\t Training Accuracy: 0.9990037634968758 \t\t Validation Accuracy: 0.9967259684205055\n",
            "Validation Loss Decreased(0.328089--->0.327403) \t Saving The Model\n",
            "Epoch 256 \t\t Training Accuracy: 0.9990066105127334 \t\t Validation Accuracy: 0.9967327889800072\n",
            "Validation Loss Decreased(0.327403--->0.326721) \t Saving The Model\n",
            "Epoch 257 \t\t Training Accuracy: 0.9990094431489706 \t\t Validation Accuracy: 0.9967395749688148\n",
            "Validation Loss Decreased(0.326721--->0.326043) \t Saving The Model\n",
            "Epoch 258 \t\t Training Accuracy: 0.9990122617036105 \t\t Validation Accuracy: 0.9967463266849518\n",
            "Validation Loss Decreased(0.326043--->0.325367) \t Saving The Model\n",
            "Epoch 259 \t\t Training Accuracy: 0.9990150662511588 \t\t Validation Accuracy: 0.9967530450224876\n",
            "Validation Loss Decreased(0.325367--->0.324695) \t Saving The Model\n",
            "Epoch 260 \t\t Training Accuracy: 0.9990178564935923 \t\t Validation Accuracy: 0.9967597293853759\n",
            "Validation Loss Decreased(0.324695--->0.324027) \t Saving The Model\n",
            "Epoch 261 \t\t Training Accuracy: 0.9990206327289343 \t\t Validation Accuracy: 0.9967663803696633\n",
            "Validation Loss Decreased(0.324027--->0.323362) \t Saving The Model\n",
            "Epoch 262 \t\t Training Accuracy: 0.9990233954042196 \t\t Validation Accuracy: 0.9967729982733726\n",
            "Validation Loss Decreased(0.323362--->0.322700) \t Saving The Model\n",
            "Epoch 263 \t\t Training Accuracy: 0.9990261442214251 \t\t Validation Accuracy: 0.9967795825004577\n",
            "Validation Loss Decreased(0.322700--->0.322042) \t Saving The Model\n",
            "Epoch 264 \t\t Training Accuracy: 0.9990288790315389 \t\t Validation Accuracy: 0.9967861342430114\n",
            "Validation Loss Decreased(0.322042--->0.321387) \t Saving The Model\n",
            "Epoch 265 \t\t Training Accuracy: 0.9990316007286311 \t\t Validation Accuracy: 0.9967926540970802\n",
            "Validation Loss Decreased(0.321387--->0.320735) \t Saving The Model\n",
            "Epoch 266 \t\t Training Accuracy: 0.9990343087911606 \t\t Validation Accuracy: 0.9967991408705711\n",
            "Validation Loss Decreased(0.320735--->0.320086) \t Saving The Model\n",
            "Epoch 267 \t\t Training Accuracy: 0.9990370035171509 \t\t Validation Accuracy: 0.9968055954575539\n",
            "Validation Loss Decreased(0.320086--->0.319440) \t Saving The Model\n",
            "Epoch 268 \t\t Training Accuracy: 0.9990396850556135 \t\t Validation Accuracy: 0.9968120190501213\n",
            "Validation Loss Decreased(0.319440--->0.318798) \t Saving The Model\n",
            "Epoch 269 \t\t Training Accuracy: 0.9990423532575369 \t\t Validation Accuracy: 0.9968184104561806\n",
            "Validation Loss Decreased(0.318798--->0.318159) \t Saving The Model\n",
            "Epoch 270 \t\t Training Accuracy: 0.9990450084209442 \t\t Validation Accuracy: 0.9968247699737549\n",
            "Validation Loss Decreased(0.318159--->0.317523) \t Saving The Model\n",
            "Epoch 271 \t\t Training Accuracy: 0.9990476506948471 \t\t Validation Accuracy: 0.9968310987949371\n",
            "Validation Loss Decreased(0.317523--->0.316890) \t Saving The Model\n",
            "Epoch 272 \t\t Training Accuracy: 0.9990502800792456 \t\t Validation Accuracy: 0.9968373957276344\n",
            "Validation Loss Decreased(0.316890--->0.316260) \t Saving The Model\n",
            "Epoch 273 \t\t Training Accuracy: 0.9990528962761164 \t\t Validation Accuracy: 0.9968436631560326\n",
            "Validation Loss Decreased(0.316260--->0.315634) \t Saving The Model\n",
            "Epoch 274 \t\t Training Accuracy: 0.9990555000305176 \t\t Validation Accuracy: 0.9968498989939689\n",
            "Validation Loss Decreased(0.315634--->0.315010) \t Saving The Model\n",
            "Epoch 275 \t\t Training Accuracy: 0.999058091044426 \t\t Validation Accuracy: 0.9968561053276062\n",
            "Validation Loss Decreased(0.315010--->0.314389) \t Saving The Model\n",
            "Epoch 276 \t\t Training Accuracy: 0.999060669541359 \t\t Validation Accuracy: 0.996862280368805\n",
            "Validation Loss Decreased(0.314389--->0.313772) \t Saving The Model\n",
            "Epoch 277 \t\t Training Accuracy: 0.9990632354468107 \t\t Validation Accuracy: 0.9968684256076813\n",
            "Validation Loss Decreased(0.313772--->0.313157) \t Saving The Model\n",
            "Epoch 278 \t\t Training Accuracy: 0.9990657889842987 \t\t Validation Accuracy: 0.9968745410442352\n",
            "Validation Loss Decreased(0.313157--->0.312546) \t Saving The Model\n",
            "Epoch 279 \t\t Training Accuracy: 0.9990683305263519 \t\t Validation Accuracy: 0.99688062697649\n",
            "Validation Loss Decreased(0.312546--->0.311937) \t Saving The Model\n",
            "Epoch 280 \t\t Training Accuracy: 0.999070859476924 \t\t Validation Accuracy: 0.9968866840004921\n",
            "Validation Loss Decreased(0.311937--->0.311332) \t Saving The Model\n",
            "Epoch 281 \t\t Training Accuracy: 0.9990733764320612 \t\t Validation Accuracy: 0.9968927112221718\n",
            "Validation Loss Decreased(0.311332--->0.310729) \t Saving The Model\n",
            "Epoch 282 \t\t Training Accuracy: 0.9990758814662695 \t\t Validation Accuracy: 0.9968987098336219\n",
            "Validation Loss Decreased(0.310729--->0.310129) \t Saving The Model\n",
            "Epoch 283 \t\t Training Accuracy: 0.999078374505043 \t\t Validation Accuracy: 0.9969046789407731\n",
            "Validation Loss Decreased(0.310129--->0.309532) \t Saving The Model\n",
            "Epoch 284 \t\t Training Accuracy: 0.999080855473876 \t\t Validation Accuracy: 0.996910620033741\n",
            "Validation Loss Decreased(0.309532--->0.308938) \t Saving The Model\n",
            "Epoch 285 \t\t Training Accuracy: 0.9990833247452975 \t\t Validation Accuracy: 0.9969165322184562\n",
            "Validation Loss Decreased(0.308938--->0.308347) \t Saving The Model\n",
            "Epoch 286 \t\t Training Accuracy: 0.9990857822448015 \t\t Validation Accuracy: 0.996922415792942\n",
            "Validation Loss Decreased(0.308347--->0.307758) \t Saving The Model\n",
            "Epoch 287 \t\t Training Accuracy: 0.9990882279723883 \t\t Validation Accuracy: 0.9969282722473145\n",
            "Validation Loss Decreased(0.307758--->0.307173) \t Saving The Model\n",
            "Epoch 288 \t\t Training Accuracy: 0.9990906623750925 \t\t Validation Accuracy: 0.9969341000914573\n",
            "Validation Loss Decreased(0.307173--->0.306590) \t Saving The Model\n",
            "Epoch 289 \t\t Training Accuracy: 0.999093085154891 \t\t Validation Accuracy: 0.9969399008154869\n",
            "Validation Loss Decreased(0.306590--->0.306010) \t Saving The Model\n",
            "Epoch 290 \t\t Training Accuracy: 0.9990954963117837 \t\t Validation Accuracy: 0.9969456738233566\n",
            "Validation Loss Decreased(0.306010--->0.305433) \t Saving The Model\n",
            "Epoch 291 \t\t Training Accuracy: 0.9990978964418172 \t\t Validation Accuracy: 0.9969514188170433\n",
            "Validation Loss Decreased(0.305433--->0.304858) \t Saving The Model\n",
            "Epoch 292 \t\t Training Accuracy: 0.9991002847999334 \t\t Validation Accuracy: 0.9969571375846863\n",
            "Validation Loss Decreased(0.304858--->0.304286) \t Saving The Model\n",
            "Epoch 293 \t\t Training Accuracy: 0.9991026624292135 \t\t Validation Accuracy: 0.9969628286361695\n",
            "Validation Loss Decreased(0.304286--->0.303717) \t Saving The Model\n",
            "Epoch 294 \t\t Training Accuracy: 0.9991050288826228 \t\t Validation Accuracy: 0.9969684931635857\n",
            "Validation Loss Decreased(0.303717--->0.303151) \t Saving The Model\n",
            "Epoch 295 \t\t Training Accuracy: 0.9991073843836784 \t\t Validation Accuracy: 0.9969741305708886\n",
            "Validation Loss Decreased(0.303151--->0.302587) \t Saving The Model\n",
            "Epoch 296 \t\t Training Accuracy: 0.9991097285598517 \t\t Validation Accuracy: 0.9969797417521477\n",
            "Validation Loss Decreased(0.302587--->0.302026) \t Saving The Model\n",
            "Epoch 297 \t\t Training Accuracy: 0.9991120620816946 \t\t Validation Accuracy: 0.9969853270053863\n",
            "Validation Loss Decreased(0.302026--->0.301467) \t Saving The Model\n",
            "Epoch 298 \t\t Training Accuracy: 0.9991143844276666 \t\t Validation Accuracy: 0.9969908854365349\n",
            "Validation Loss Decreased(0.301467--->0.300911) \t Saving The Model\n",
            "Epoch 299 \t\t Training Accuracy: 0.9991166963428258 \t\t Validation Accuracy: 0.9969964182376861\n",
            "Validation Loss Decreased(0.300911--->0.300358) \t Saving The Model\n",
            "Epoch 300 \t\t Training Accuracy: 0.9991189973801374 \t\t Validation Accuracy: 0.9970019257068634\n",
            "Validation Loss Decreased(0.300358--->0.299807) \t Saving The Model\n",
            "Epoch 301 \t\t Training Accuracy: 0.9991212878376245 \t\t Validation Accuracy: 0.9970074066519737\n",
            "Validation Loss Decreased(0.299807--->0.299259) \t Saving The Model\n",
            "Epoch 302 \t\t Training Accuracy: 0.9991235677152872 \t\t Validation Accuracy: 0.9970128625631333\n",
            "Validation Loss Decreased(0.299259--->0.298714) \t Saving The Model\n",
            "Epoch 303 \t\t Training Accuracy: 0.9991258373111487 \t\t Validation Accuracy: 0.9970182931423187\n",
            "Validation Loss Decreased(0.298714--->0.298171) \t Saving The Model\n",
            "Epoch 304 \t\t Training Accuracy: 0.9991280963271856 \t\t Validation Accuracy: 0.9970236980915069\n",
            "Validation Loss Decreased(0.298171--->0.297630) \t Saving The Model\n",
            "Epoch 305 \t\t Training Accuracy: 0.9991303446888924 \t\t Validation Accuracy: 0.997029078900814\n",
            "Validation Loss Decreased(0.297630--->0.297092) \t Saving The Model\n",
            "Epoch 306 \t\t Training Accuracy: 0.9991325833648443 \t\t Validation Accuracy: 0.9970344349741935\n",
            "Validation Loss Decreased(0.297092--->0.296557) \t Saving The Model\n",
            "Epoch 307 \t\t Training Accuracy: 0.999134811386466 \t\t Validation Accuracy: 0.9970397654175759\n",
            "Validation Loss Decreased(0.296557--->0.296023) \t Saving The Model\n",
            "Epoch 308 \t\t Training Accuracy: 0.9991370294243097 \t\t Validation Accuracy: 0.9970450714230538\n",
            "Validation Loss Decreased(0.296023--->0.295493) \t Saving The Model\n",
            "Epoch 309 \t\t Training Accuracy: 0.9991392371058464 \t\t Validation Accuracy: 0.9970503535866737\n",
            "Validation Loss Decreased(0.295493--->0.294965) \t Saving The Model\n",
            "Epoch 310 \t\t Training Accuracy: 0.9991414351016283 \t\t Validation Accuracy: 0.997055612206459\n",
            "Validation Loss Decreased(0.294965--->0.294439) \t Saving The Model\n",
            "Epoch 311 \t\t Training Accuracy: 0.9991436228901148 \t\t Validation Accuracy: 0.997060845196247\n",
            "Validation Loss Decreased(0.294439--->0.293915) \t Saving The Model\n",
            "Epoch 312 \t\t Training Accuracy: 0.9991458006948233 \t\t Validation Accuracy: 0.9970660552382469\n",
            "Validation Loss Decreased(0.293915--->0.293394) \t Saving The Model\n",
            "Epoch 313 \t\t Training Accuracy: 0.9991479689627886 \t\t Validation Accuracy: 0.9970712411403656\n",
            "Validation Loss Decreased(0.293394--->0.292876) \t Saving The Model\n",
            "Epoch 314 \t\t Training Accuracy: 0.9991501273959875 \t\t Validation Accuracy: 0.9970764037966728\n",
            "Validation Loss Decreased(0.292876--->0.292360) \t Saving The Model\n",
            "Epoch 315 \t\t Training Accuracy: 0.9991522759199143 \t\t Validation Accuracy: 0.9970815432071686\n",
            "Validation Loss Decreased(0.292360--->0.291846) \t Saving The Model\n",
            "Epoch 316 \t\t Training Accuracy: 0.9991544150561095 \t\t Validation Accuracy: 0.9970866587758064\n",
            "Validation Loss Decreased(0.291846--->0.291334) \t Saving The Model\n",
            "Epoch 317 \t\t Training Accuracy: 0.9991565442830325 \t\t Validation Accuracy: 0.9970917513966561\n",
            "Validation Loss Decreased(0.291334--->0.290825) \t Saving The Model\n",
            "Epoch 318 \t\t Training Accuracy: 0.9991586642712355 \t\t Validation Accuracy: 0.9970968210697174\n",
            "Validation Loss Decreased(0.290825--->0.290318) \t Saving The Model\n",
            "Epoch 319 \t\t Training Accuracy: 0.9991607745736837 \t\t Validation Accuracy: 0.9971018674969673\n",
            "Validation Loss Decreased(0.290318--->0.289813) \t Saving The Model\n",
            "Epoch 320 \t\t Training Accuracy: 0.9991628754138947 \t\t Validation Accuracy: 0.9971068918704986\n",
            "Validation Loss Decreased(0.289813--->0.289311) \t Saving The Model\n",
            "Epoch 321 \t\t Training Accuracy: 0.9991649671643973 \t\t Validation Accuracy: 0.9971118929982186\n",
            "Validation Loss Decreased(0.289311--->0.288811) \t Saving The Model\n",
            "Epoch 322 \t\t Training Accuracy: 0.9991670494526624 \t\t Validation Accuracy: 0.9971168720722199\n",
            "Validation Loss Decreased(0.288811--->0.288313) \t Saving The Model\n",
            "Epoch 323 \t\t Training Accuracy: 0.9991691225022078 \t\t Validation Accuracy: 0.9971218284964561\n",
            "Validation Loss Decreased(0.288313--->0.287817) \t Saving The Model\n",
            "Epoch 324 \t\t Training Accuracy: 0.9991711862385273 \t\t Validation Accuracy: 0.9971267628669739\n",
            "Validation Loss Decreased(0.287817--->0.287324) \t Saving The Model\n",
            "Epoch 325 \t\t Training Accuracy: 0.9991732411831618 \t\t Validation Accuracy: 0.9971316757798195\n",
            "Validation Loss Decreased(0.287324--->0.286832) \t Saving The Model\n",
            "Epoch 326 \t\t Training Accuracy: 0.9991752866655588 \t\t Validation Accuracy: 0.9971365663409233\n",
            "Validation Loss Decreased(0.286832--->0.286343) \t Saving The Model\n",
            "Epoch 327 \t\t Training Accuracy: 0.9991773232072592 \t\t Validation Accuracy: 0.9971414351463318\n",
            "Validation Loss Decreased(0.286343--->0.285856) \t Saving The Model\n",
            "Epoch 328 \t\t Training Accuracy: 0.9991793511062861 \t\t Validation Accuracy: 0.9971462813019752\n",
            "Validation Loss Decreased(0.285856--->0.285372) \t Saving The Model\n",
            "Epoch 329 \t\t Training Accuracy: 0.999181369766593 \t\t Validation Accuracy: 0.9971511074900627\n",
            "Validation Loss Decreased(0.285372--->0.284889) \t Saving The Model\n",
            "Epoch 330 \t\t Training Accuracy: 0.9991833797097206 \t\t Validation Accuracy: 0.9971559119224548\n",
            "Validation Loss Decreased(0.284889--->0.284409) \t Saving The Model\n",
            "Epoch 331 \t\t Training Accuracy: 0.9991853808611632 \t\t Validation Accuracy: 0.9971606948971748\n",
            "Validation Loss Decreased(0.284409--->0.283931) \t Saving The Model\n",
            "Epoch 332 \t\t Training Accuracy: 0.9991873735189438 \t\t Validation Accuracy: 0.9971654573082924\n",
            "Validation Loss Decreased(0.283931--->0.283454) \t Saving The Model\n",
            "Epoch 333 \t\t Training Accuracy: 0.9991893570870161 \t\t Validation Accuracy: 0.9971701979637146\n",
            "Validation Loss Decreased(0.283454--->0.282980) \t Saving The Model\n",
            "Epoch 334 \t\t Training Accuracy: 0.9991913320869208 \t\t Validation Accuracy: 0.9971749183535575\n",
            "Validation Loss Decreased(0.282980--->0.282508) \t Saving The Model\n",
            "Epoch 335 \t\t Training Accuracy: 0.9991932984441518 \t\t Validation Accuracy: 0.9971796175837517\n",
            "Validation Loss Decreased(0.282508--->0.282038) \t Saving The Model\n",
            "Epoch 336 \t\t Training Accuracy: 0.9991952565312385 \t\t Validation Accuracy: 0.9971842965483666\n",
            "Validation Loss Decreased(0.282038--->0.281570) \t Saving The Model\n",
            "Epoch 337 \t\t Training Accuracy: 0.9991972061246633 \t\t Validation Accuracy: 0.9971889546513557\n",
            "Validation Loss Decreased(0.281570--->0.281105) \t Saving The Model\n",
            "Epoch 338 \t\t Training Accuracy: 0.9991991472244263 \t\t Validation Accuracy: 0.9971935930848121\n",
            "Validation Loss Decreased(0.281105--->0.280641) \t Saving The Model\n",
            "Epoch 339 \t\t Training Accuracy: 0.9992010796815157 \t\t Validation Accuracy: 0.9971982103586197\n",
            "Validation Loss Decreased(0.280641--->0.280179) \t Saving The Model\n",
            "Epoch 340 \t\t Training Accuracy: 0.9992030041664839 \t\t Validation Accuracy: 0.9972028079628944\n",
            "Validation Loss Decreased(0.280179--->0.279719) \t Saving The Model\n",
            "Epoch 341 \t\t Training Accuracy: 0.9992049204558134 \t\t Validation Accuracy: 0.9972073855996132\n",
            "Validation Loss Decreased(0.279719--->0.279261) \t Saving The Model\n",
            "Epoch 342 \t\t Training Accuracy: 0.9992068282514811 \t\t Validation Accuracy: 0.9972119438648224\n",
            "Validation Loss Decreased(0.279261--->0.278806) \t Saving The Model\n",
            "Epoch 343 \t\t Training Accuracy: 0.9992087280005216 \t\t Validation Accuracy: 0.9972164821624756\n",
            "Validation Loss Decreased(0.278806--->0.278352) \t Saving The Model\n",
            "Epoch 344 \t\t Training Accuracy: 0.9992106193304062 \t\t Validation Accuracy: 0.9972210004925728\n",
            "Validation Loss Decreased(0.278352--->0.277900) \t Saving The Model\n",
            "Epoch 345 \t\t Training Accuracy: 0.9992125026881695 \t\t Validation Accuracy: 0.9972254994511605\n",
            "Validation Loss Decreased(0.277900--->0.277450) \t Saving The Model\n",
            "Epoch 346 \t\t Training Accuracy: 0.9992143783718348 \t\t Validation Accuracy: 0.9972299790382385\n",
            "Validation Loss Decreased(0.277450--->0.277002) \t Saving The Model\n",
            "Epoch 347 \t\t Training Accuracy: 0.9992162457853556 \t\t Validation Accuracy: 0.9972344398498535\n",
            "Validation Loss Decreased(0.277002--->0.276556) \t Saving The Model\n",
            "Epoch 348 \t\t Training Accuracy: 0.9992181051522493 \t\t Validation Accuracy: 0.9972388806939125\n",
            "Validation Loss Decreased(0.276556--->0.276112) \t Saving The Model\n",
            "Epoch 349 \t\t Training Accuracy: 0.9992199569195509 \t\t Validation Accuracy: 0.9972433033585548\n",
            "Validation Loss Decreased(0.276112--->0.275670) \t Saving The Model\n",
            "Epoch 350 \t\t Training Accuracy: 0.9992218007892371 \t\t Validation Accuracy: 0.9972477063536644\n",
            "Validation Loss Decreased(0.275670--->0.275229) \t Saving The Model\n",
            "Epoch 351 \t\t Training Accuracy: 0.9992236369103193 \t\t Validation Accuracy: 0.997252090871334\n",
            "Validation Loss Decreased(0.275229--->0.274791) \t Saving The Model\n",
            "Epoch 352 \t\t Training Accuracy: 0.9992254649847746 \t\t Validation Accuracy: 0.9972564569115638\n",
            "Validation Loss Decreased(0.274791--->0.274354) \t Saving The Model\n",
            "Epoch 353 \t\t Training Accuracy: 0.9992272856086493 \t\t Validation Accuracy: 0.9972608035802841\n",
            "Validation Loss Decreased(0.274354--->0.273920) \t Saving The Model\n",
            "Epoch 354 \t\t Training Accuracy: 0.9992290982604026 \t\t Validation Accuracy: 0.9972651323676109\n",
            "Validation Loss Decreased(0.273920--->0.273487) \t Saving The Model\n",
            "Epoch 355 \t\t Training Accuracy: 0.9992309036105871 \t\t Validation Accuracy: 0.9972694423794747\n",
            "Validation Loss Decreased(0.273487--->0.273056) \t Saving The Model\n",
            "Epoch 356 \t\t Training Accuracy: 0.9992327013611794 \t\t Validation Accuracy: 0.9972737351059914\n",
            "Validation Loss Decreased(0.273056--->0.272626) \t Saving The Model\n",
            "Epoch 357 \t\t Training Accuracy: 0.9992344915121794 \t\t Validation Accuracy: 0.9972780084609986\n",
            "Validation Loss Decreased(0.272626--->0.272199) \t Saving The Model\n",
            "Epoch 358 \t\t Training Accuracy: 0.999236274138093 \t\t Validation Accuracy: 0.9972822645306587\n",
            "Validation Loss Decreased(0.272199--->0.271774) \t Saving The Model\n",
            "Epoch 359 \t\t Training Accuracy: 0.9992380494624377 \t\t Validation Accuracy: 0.9972865024209022\n",
            "Validation Loss Decreased(0.271774--->0.271350) \t Saving The Model\n",
            "Epoch 360 \t\t Training Accuracy: 0.9992398174107074 \t\t Validation Accuracy: 0.9972907230257988\n",
            "Validation Loss Decreased(0.271350--->0.270928) \t Saving The Model\n",
            "Epoch 361 \t\t Training Accuracy: 0.9992415779829025 \t\t Validation Accuracy: 0.9972949251532555\n",
            "Validation Loss Decreased(0.270928--->0.270507) \t Saving The Model\n",
            "Epoch 362 \t\t Training Accuracy: 0.9992433312535286 \t\t Validation Accuracy: 0.9972991099953652\n",
            "Validation Loss Decreased(0.270507--->0.270089) \t Saving The Model\n",
            "Epoch 363 \t\t Training Accuracy: 0.9992450771480799 \t\t Validation Accuracy: 0.9973032772541046\n",
            "Validation Loss Decreased(0.270089--->0.269672) \t Saving The Model\n",
            "Epoch 364 \t\t Training Accuracy: 0.9992468160390854 \t\t Validation Accuracy: 0.9973074272274971\n",
            "Validation Loss Decreased(0.269672--->0.269257) \t Saving The Model\n",
            "Epoch 365 \t\t Training Accuracy: 0.9992485476285219 \t\t Validation Accuracy: 0.9973115599155427\n",
            "Validation Loss Decreased(0.269257--->0.268844) \t Saving The Model\n",
            "Epoch 366 \t\t Training Accuracy: 0.9992502718418836 \t\t Validation Accuracy: 0.9973156747221946\n",
            "Validation Loss Decreased(0.268844--->0.268433) \t Saving The Model\n",
            "Epoch 367 \t\t Training Accuracy: 0.9992519893497228 \t\t Validation Accuracy: 0.9973197734355926\n",
            "Validation Loss Decreased(0.268433--->0.268023) \t Saving The Model\n",
            "Epoch 368 \t\t Training Accuracy: 0.9992536994814872 \t\t Validation Accuracy: 0.9973238545656204\n",
            "Validation Loss Decreased(0.268023--->0.267615) \t Saving The Model\n",
            "Epoch 369 \t\t Training Accuracy: 0.9992554027587175 \t\t Validation Accuracy: 0.9973279187083244\n",
            "Validation Loss Decreased(0.267615--->0.267208) \t Saving The Model\n",
            "Epoch 370 \t\t Training Accuracy: 0.9992570991069079 \t\t Validation Accuracy: 0.9973319652676582\n",
            "Validation Loss Decreased(0.267208--->0.266803) \t Saving The Model\n",
            "Epoch 371 \t\t Training Accuracy: 0.9992587883025408 \t\t Validation Accuracy: 0.9973359963297844\n",
            "Validation Loss Decreased(0.266803--->0.266400) \t Saving The Model\n",
            "Epoch 372 \t\t Training Accuracy: 0.9992604707926511 \t\t Validation Accuracy: 0.9973400104045868\n",
            "Validation Loss Decreased(0.266400--->0.265999) \t Saving The Model\n",
            "Epoch 373 \t\t Training Accuracy: 0.99926214620471 \t\t Validation Accuracy: 0.9973440083861351\n",
            "Validation Loss Decreased(0.265999--->0.265599) \t Saving The Model\n",
            "Epoch 374 \t\t Training Accuracy: 0.9992638152092695 \t\t Validation Accuracy: 0.9973479893803596\n",
            "Validation Loss Decreased(0.265599--->0.265201) \t Saving The Model\n",
            "Epoch 375 \t\t Training Accuracy: 0.9992654773592949 \t\t Validation Accuracy: 0.99735195428133\n",
            "Validation Loss Decreased(0.265201--->0.264805) \t Saving The Model\n",
            "Epoch 376 \t\t Training Accuracy: 0.999267132282257 \t\t Validation Accuracy: 0.9973559027910233\n",
            "Validation Loss Decreased(0.264805--->0.264410) \t Saving The Model\n",
            "Epoch 377 \t\t Training Accuracy: 0.9992687809467315 \t\t Validation Accuracy: 0.9973598358035087\n",
            "Validation Loss Decreased(0.264410--->0.264016) \t Saving The Model\n",
            "Epoch 378 \t\t Training Accuracy: 0.9992704229801893 \t\t Validation Accuracy: 0.9973637515306473\n",
            "Validation Loss Decreased(0.264016--->0.263625) \t Saving The Model\n",
            "Epoch 379 \t\t Training Accuracy: 0.9992720582336188 \t\t Validation Accuracy: 0.9973676520586013\n",
            "Validation Loss Decreased(0.263625--->0.263235) \t Saving The Model\n",
            "Epoch 380 \t\t Training Accuracy: 0.9992736869305372 \t\t Validation Accuracy: 0.9973715355992318\n",
            "Validation Loss Decreased(0.263235--->0.262846) \t Saving The Model\n",
            "Epoch 381 \t\t Training Accuracy: 0.9992753087729216 \t\t Validation Accuracy: 0.9973754048347473\n",
            "Validation Loss Decreased(0.262846--->0.262460) \t Saving The Model\n",
            "Epoch 382 \t\t Training Accuracy: 0.999276924431324 \t\t Validation Accuracy: 0.9973792579770088\n",
            "Validation Loss Decreased(0.262460--->0.262074) \t Saving The Model\n",
            "Epoch 383 \t\t Training Accuracy: 0.9992785335332155 \t\t Validation Accuracy: 0.9973830953240395\n",
            "Validation Loss Decreased(0.262074--->0.261690) \t Saving The Model\n",
            "Epoch 384 \t\t Training Accuracy: 0.9992801362276077 \t\t Validation Accuracy: 0.997386917769909\n",
            "Validation Loss Decreased(0.261690--->0.261308) \t Saving The Model\n",
            "Epoch 385 \t\t Training Accuracy: 0.9992817326635123 \t\t Validation Accuracy: 0.9973907241225243\n",
            "Validation Loss Decreased(0.261308--->0.260928) \t Saving The Model\n",
            "Epoch 386 \t\t Training Accuracy: 0.9992833225429059 \t\t Validation Accuracy: 0.9973945152759552\n",
            "Validation Loss Decreased(0.260928--->0.260548) \t Saving The Model\n",
            "Epoch 387 \t\t Training Accuracy: 0.9992849060148 \t\t Validation Accuracy: 0.9973982912302017\n",
            "Validation Loss Decreased(0.260548--->0.260171) \t Saving The Model\n",
            "Epoch 388 \t\t Training Accuracy: 0.9992864833027124 \t\t Validation Accuracy: 0.9974020525813103\n",
            "Validation Loss Decreased(0.260171--->0.259795) \t Saving The Model\n",
            "Epoch 389 \t\t Training Accuracy: 0.9992880544066429 \t\t Validation Accuracy: 0.997405798137188\n",
            "Validation Loss Decreased(0.259795--->0.259420) \t Saving The Model\n",
            "Epoch 390 \t\t Training Accuracy: 0.9992896191775799 \t\t Validation Accuracy: 0.9974095287919045\n",
            "Validation Loss Decreased(0.259420--->0.259047) \t Saving The Model\n",
            "Epoch 391 \t\t Training Accuracy: 0.9992911778390408 \t\t Validation Accuracy: 0.9974132445454598\n",
            "Validation Loss Decreased(0.259047--->0.258676) \t Saving The Model\n",
            "Epoch 392 \t\t Training Accuracy: 0.9992927301675081 \t\t Validation Accuracy: 0.9974169448018074\n",
            "Validation Loss Decreased(0.258676--->0.258306) \t Saving The Model\n",
            "Epoch 393 \t\t Training Accuracy: 0.9992942766100168 \t\t Validation Accuracy: 0.9974206307530403\n",
            "Validation Loss Decreased(0.258306--->0.257937) \t Saving The Model\n",
            "Epoch 394 \t\t Training Accuracy: 0.9992958166450262 \t\t Validation Accuracy: 0.9974243029952049\n",
            "Validation Loss Decreased(0.257937--->0.257570) \t Saving The Model\n",
            "Epoch 395 \t\t Training Accuracy: 0.9992973507195712 \t\t Validation Accuracy: 0.9974279597401619\n",
            "Validation Loss Decreased(0.257570--->0.257204) \t Saving The Model\n",
            "Epoch 396 \t\t Training Accuracy: 0.9992988788336515 \t\t Validation Accuracy: 0.9974316015839577\n",
            "Validation Loss Decreased(0.257204--->0.256840) \t Saving The Model\n",
            "Epoch 397 \t\t Training Accuracy: 0.9993004011362792 \t\t Validation Accuracy: 0.9974352297186851\n",
            "Validation Loss Decreased(0.256840--->0.256477) \t Saving The Model\n",
            "Epoch 398 \t\t Training Accuracy: 0.9993019174784422 \t\t Validation Accuracy: 0.9974388429522514\n",
            "Validation Loss Decreased(0.256477--->0.256116) \t Saving The Model\n",
            "Epoch 399 \t\t Training Accuracy: 0.9993034276366234 \t\t Validation Accuracy: 0.9974424427747727\n",
            "Validation Loss Decreased(0.256116--->0.255756) \t Saving The Model\n",
            "Epoch 400 \t\t Training Accuracy: 0.9993049316853285 \t\t Validation Accuracy: 0.9974460279941559\n",
            "Validation Loss Decreased(0.255756--->0.255397) \t Saving The Model\n",
            "Epoch 401 \t\t Training Accuracy: 0.999306430220604 \t\t Validation Accuracy: 0.9974495989084243\n",
            "Validation Loss Decreased(0.255397--->0.255040) \t Saving The Model\n",
            "Epoch 402 \t\t Training Accuracy: 0.9993079230189323 \t\t Validation Accuracy: 0.997453156709671\n",
            "Validation Loss Decreased(0.255040--->0.254684) \t Saving The Model\n",
            "Epoch 403 \t\t Training Accuracy: 0.9993094098567963 \t\t Validation Accuracy: 0.9974566999077797\n",
            "Validation Loss Decreased(0.254684--->0.254330) \t Saving The Model\n",
            "Epoch 404 \t\t Training Accuracy: 0.9993108908832073 \t\t Validation Accuracy: 0.9974602285027504\n",
            "Validation Loss Decreased(0.254330--->0.253977) \t Saving The Model\n",
            "Epoch 405 \t\t Training Accuracy: 0.9993123663961887 \t\t Validation Accuracy: 0.9974637439846993\n",
            "Validation Loss Decreased(0.253977--->0.253626) \t Saving The Model\n",
            "Epoch 406 \t\t Training Accuracy: 0.9993138360232114 \t\t Validation Accuracy: 0.997467246055603\n",
            "Validation Loss Decreased(0.253626--->0.253275) \t Saving The Model\n",
            "Epoch 407 \t\t Training Accuracy: 0.9993153000622987 \t\t Validation Accuracy: 0.9974707344174385\n",
            "Validation Loss Decreased(0.253275--->0.252927) \t Saving The Model\n",
            "Epoch 408 \t\t Training Accuracy: 0.999316758364439 \t\t Validation Accuracy: 0.9974742087721825\n",
            "Validation Loss Decreased(0.252927--->0.252579) \t Saving The Model\n",
            "Epoch 409 \t\t Training Accuracy: 0.9993182108551264 \t\t Validation Accuracy: 0.9974776694178581\n",
            "Validation Loss Decreased(0.252579--->0.252233) \t Saving The Model\n",
            "Epoch 410 \t\t Training Accuracy: 0.9993196580559015 \t\t Validation Accuracy: 0.9974811175465583\n",
            "Validation Loss Decreased(0.252233--->0.251888) \t Saving The Model\n",
            "Epoch 411 \t\t Training Accuracy: 0.999321099743247 \t\t Validation Accuracy: 0.9974845516681671\n",
            "Validation Loss Decreased(0.251888--->0.251545) \t Saving The Model\n",
            "Epoch 412 \t\t Training Accuracy: 0.9993225357681513 \t\t Validation Accuracy: 0.997487972676754\n",
            "Validation Loss Decreased(0.251545--->0.251203) \t Saving The Model\n",
            "Epoch 413 \t\t Training Accuracy: 0.9993239662796259 \t\t Validation Accuracy: 0.9974913802742958\n",
            "Validation Loss Decreased(0.251203--->0.250862) \t Saving The Model\n",
            "Epoch 414 \t\t Training Accuracy: 0.9993253915011883 \t\t Validation Accuracy: 0.9974947747588158\n",
            "Validation Loss Decreased(0.250862--->0.250523) \t Saving The Model\n",
            "Epoch 415 \t\t Training Accuracy: 0.9993268111348153 \t\t Validation Accuracy: 0.9974981564283371\n",
            "Validation Loss Decreased(0.250523--->0.250184) \t Saving The Model\n",
            "Epoch 416 \t\t Training Accuracy: 0.9993282255530357 \t\t Validation Accuracy: 0.9975015245378017\n",
            "Validation Loss Decreased(0.250184--->0.249848) \t Saving The Model\n",
            "Epoch 417 \t\t Training Accuracy: 0.9993296343833208 \t\t Validation Accuracy: 0.9975048798322678\n",
            "Validation Loss Decreased(0.249848--->0.249512) \t Saving The Model\n",
            "Epoch 418 \t\t Training Accuracy: 0.9993310379236937 \t\t Validation Accuracy: 0.9975082229077816\n",
            "Validation Loss Decreased(0.249512--->0.249178) \t Saving The Model\n",
            "Epoch 419 \t\t Training Accuracy: 0.9993324360996485 \t\t Validation Accuracy: 0.997511552721262\n",
            "Validation Loss Decreased(0.249178--->0.248845) \t Saving The Model\n",
            "Epoch 420 \t\t Training Accuracy: 0.9993338289111853 \t\t Validation Accuracy: 0.9975148701667785\n",
            "Validation Loss Decreased(0.248845--->0.248513) \t Saving The Model\n",
            "Epoch 421 \t\t Training Accuracy: 0.9993352168053389 \t\t Validation Accuracy: 0.9975181747972965\n",
            "Validation Loss Decreased(0.248513--->0.248183) \t Saving The Model\n",
            "Epoch 422 \t\t Training Accuracy: 0.9993365990370512 \t\t Validation Accuracy: 0.9975214666128158\n",
            "Validation Loss Decreased(0.248183--->0.247853) \t Saving The Model\n",
            "Epoch 423 \t\t Training Accuracy: 0.9993379762768746 \t\t Validation Accuracy: 0.9975247460603714\n",
            "Validation Loss Decreased(0.247853--->0.247525) \t Saving The Model\n",
            "Epoch 424 \t\t Training Accuracy: 0.9993393485993147 \t\t Validation Accuracy: 0.9975280129909515\n",
            "Validation Loss Decreased(0.247525--->0.247199) \t Saving The Model\n",
            "Epoch 425 \t\t Training Accuracy: 0.999340715482831 \t\t Validation Accuracy: 0.9975312680006028\n",
            "Validation Loss Decreased(0.247199--->0.246873) \t Saving The Model\n",
            "Epoch 426 \t\t Training Accuracy: 0.9993420771509409 \t\t Validation Accuracy: 0.9975345104932785\n",
            "Validation Loss Decreased(0.246873--->0.246549) \t Saving The Model\n",
            "Epoch 427 \t\t Training Accuracy: 0.9993434339016676 \t\t Validation Accuracy: 0.9975377409160138\n",
            "Validation Loss Decreased(0.246549--->0.246226) \t Saving The Model\n",
            "Epoch 428 \t\t Training Accuracy: 0.9993447854369879 \t\t Validation Accuracy: 0.9975409585237504\n",
            "Validation Loss Decreased(0.246226--->0.245904) \t Saving The Model\n",
            "Epoch 429 \t\t Training Accuracy: 0.999346132054925 \t\t Validation Accuracy: 0.9975441648066043\n",
            "Validation Loss Decreased(0.245904--->0.245584) \t Saving The Model\n",
            "Epoch 430 \t\t Training Accuracy: 0.9993474734574557 \t\t Validation Accuracy: 0.9975473582744598\n",
            "Validation Loss Decreased(0.245584--->0.245264) \t Saving The Model\n",
            "Epoch 431 \t\t Training Accuracy: 0.9993488100916147 \t\t Validation Accuracy: 0.9975505401194096\n",
            "Validation Loss Decreased(0.245264--->0.244946) \t Saving The Model\n",
            "Epoch 432 \t\t Training Accuracy: 0.9993501418083907 \t\t Validation Accuracy: 0.9975537104904652\n",
            "Validation Loss Decreased(0.244946--->0.244629) \t Saving The Model\n",
            "Epoch 433 \t\t Training Accuracy: 0.9993514684587717 \t\t Validation Accuracy: 0.9975568683445454\n",
            "Validation Loss Decreased(0.244629--->0.244313) \t Saving The Model\n",
            "Epoch 434 \t\t Training Accuracy: 0.9993527901917696 \t\t Validation Accuracy: 0.9975600145757199\n",
            "Validation Loss Decreased(0.244313--->0.243999) \t Saving The Model\n",
            "Epoch 435 \t\t Training Accuracy: 0.9993541072309017 \t\t Validation Accuracy: 0.9975631487369537\n",
            "Validation Loss Decreased(0.243999--->0.243685) \t Saving The Model\n",
            "Epoch 436 \t\t Training Accuracy: 0.99935541883111 \t\t Validation Accuracy: 0.9975662718713284\n",
            "Validation Loss Decreased(0.243685--->0.243373) \t Saving The Model\n",
            "Epoch 437 \t\t Training Accuracy: 0.9993567261844873 \t\t Validation Accuracy: 0.9975693838298321\n",
            "Validation Loss Decreased(0.243373--->0.243062) \t Saving The Model\n",
            "Epoch 438 \t\t Training Accuracy: 0.9993580285459757 \t\t Validation Accuracy: 0.997572483420372\n",
            "Validation Loss Decreased(0.243062--->0.242752) \t Saving The Model\n",
            "Epoch 439 \t\t Training Accuracy: 0.9993593262881041 \t\t Validation Accuracy: 0.997575571089983\n",
            "Validation Loss Decreased(0.242752--->0.242443) \t Saving The Model\n",
            "Epoch 440 \t\t Training Accuracy: 0.9993606189638377 \t\t Validation Accuracy: 0.9975786477327346\n",
            "Validation Loss Decreased(0.242443--->0.242135) \t Saving The Model\n",
            "Epoch 441 \t\t Training Accuracy: 0.9993619070202112 \t\t Validation Accuracy: 0.9975817131996155\n",
            "Validation Loss Decreased(0.242135--->0.241829) \t Saving The Model\n",
            "Epoch 442 \t\t Training Accuracy: 0.9993631903082132 \t\t Validation Accuracy: 0.9975847677886486\n",
            "Validation Loss Decreased(0.241829--->0.241523) \t Saving The Model\n",
            "Epoch 443 \t\t Training Accuracy: 0.9993644690513611 \t\t Validation Accuracy: 0.9975878101587295\n",
            "Validation Loss Decreased(0.241523--->0.241219) \t Saving The Model\n",
            "Epoch 444 \t\t Training Accuracy: 0.9993657430261373 \t\t Validation Accuracy: 0.9975908415019512\n",
            "Validation Loss Decreased(0.241219--->0.240916) \t Saving The Model\n",
            "Epoch 445 \t\t Training Accuracy: 0.9993670126795768 \t\t Validation Accuracy: 0.9975938622653484\n",
            "Validation Loss Decreased(0.240916--->0.240614) \t Saving The Model\n",
            "Epoch 446 \t\t Training Accuracy: 0.9993682769685983 \t\t Validation Accuracy: 0.9975968720018864\n",
            "Validation Loss Decreased(0.240614--->0.240313) \t Saving The Model\n",
            "Epoch 447 \t\t Training Accuracy: 0.9993695373088122 \t\t Validation Accuracy: 0.9975998695194721\n",
            "Validation Loss Decreased(0.240313--->0.240013) \t Saving The Model\n",
            "Epoch 448 \t\t Training Accuracy: 0.9993707928061485 \t\t Validation Accuracy: 0.9976028572022915\n",
            "Validation Loss Decreased(0.240013--->0.239714) \t Saving The Model\n",
            "Epoch 449 \t\t Training Accuracy: 0.9993720438331366 \t\t Validation Accuracy: 0.9976058340072632\n",
            "Validation Loss Decreased(0.239714--->0.239417) \t Saving The Model\n",
            "Epoch 450 \t\t Training Accuracy: 0.9993732902407646 \t\t Validation Accuracy: 0.9976087994873524\n",
            "Validation Loss Decreased(0.239417--->0.239120) \t Saving The Model\n",
            "Epoch 451 \t\t Training Accuracy: 0.9993745324760676 \t\t Validation Accuracy: 0.9976117543876171\n",
            "Validation Loss Decreased(0.239120--->0.238825) \t Saving The Model\n",
            "Epoch 452 \t\t Training Accuracy: 0.9993757699802518 \t\t Validation Accuracy: 0.9976146984100341\n",
            "Validation Loss Decreased(0.238825--->0.238530) \t Saving The Model\n",
            "Epoch 453 \t\t Training Accuracy: 0.9993770031258464 \t\t Validation Accuracy: 0.99761763215065\n",
            "Validation Loss Decreased(0.238530--->0.238237) \t Saving The Model\n",
            "Epoch 454 \t\t Training Accuracy: 0.9993782318010926 \t\t Validation Accuracy: 0.9976205556094646\n",
            "Validation Loss Decreased(0.238237--->0.237944) \t Saving The Model\n",
            "Epoch 455 \t\t Training Accuracy: 0.9993794556334614 \t\t Validation Accuracy: 0.9976234667003154\n",
            "Validation Loss Decreased(0.237944--->0.237653) \t Saving The Model\n",
            "Epoch 456 \t\t Training Accuracy: 0.9993806755542756 \t\t Validation Accuracy: 0.9976263691484928\n",
            "Validation Loss Decreased(0.237653--->0.237363) \t Saving The Model\n",
            "Epoch 457 \t\t Training Accuracy: 0.9993818913027644 \t\t Validation Accuracy: 0.9976292608678341\n",
            "Validation Loss Decreased(0.237363--->0.237074) \t Saving The Model\n",
            "Epoch 458 \t\t Training Accuracy: 0.999383102171123 \t\t Validation Accuracy: 0.9976321415603161\n",
            "Validation Loss Decreased(0.237074--->0.236786) \t Saving The Model\n",
            "Epoch 459 \t\t Training Accuracy: 0.999384309053421 \t\t Validation Accuracy: 0.9976350118219852\n",
            "Validation Loss Decreased(0.236786--->0.236499) \t Saving The Model\n",
            "Epoch 460 \t\t Training Accuracy: 0.9993855114653707 \t\t Validation Accuracy: 0.9976378719508648\n",
            "Validation Loss Decreased(0.236499--->0.236213) \t Saving The Model\n",
            "Epoch 461 \t\t Training Accuracy: 0.9993867097422481 \t\t Validation Accuracy: 0.9976407216489315\n",
            "Validation Loss Decreased(0.236213--->0.235928) \t Saving The Model\n",
            "Epoch 462 \t\t Training Accuracy: 0.9993879039213062 \t\t Validation Accuracy: 0.997643561810255\n",
            "Validation Loss Decreased(0.235928--->0.235644) \t Saving The Model\n",
            "Epoch 463 \t\t Training Accuracy: 0.9993890933319927 \t\t Validation Accuracy: 0.9976463906466961\n",
            "Validation Loss Decreased(0.235644--->0.235361) \t Saving The Model\n",
            "Epoch 464 \t\t Training Accuracy: 0.9993902788683772 \t\t Validation Accuracy: 0.9976492100954055\n",
            "Validation Loss Decreased(0.235361--->0.235079) \t Saving The Model\n",
            "Epoch 465 \t\t Training Accuracy: 0.9993914602324366 \t\t Validation Accuracy: 0.9976520200073719\n",
            "Validation Loss Decreased(0.235079--->0.234798) \t Saving The Model\n",
            "Epoch 466 \t\t Training Accuracy: 0.9993926375731825 \t\t Validation Accuracy: 0.9976548193395138\n",
            "Validation Loss Decreased(0.234798--->0.234518) \t Saving The Model\n",
            "Epoch 467 \t\t Training Accuracy: 0.9993938103318214 \t\t Validation Accuracy: 0.9976576088368893\n",
            "Validation Loss Decreased(0.234518--->0.234239) \t Saving The Model\n",
            "Epoch 468 \t\t Training Accuracy: 0.9993949792906642 \t\t Validation Accuracy: 0.9976603889465332\n",
            "Validation Loss Decreased(0.234239--->0.233961) \t Saving The Model\n",
            "Epoch 469 \t\t Training Accuracy: 0.999396144002676 \t\t Validation Accuracy: 0.9976631586253643\n",
            "Validation Loss Decreased(0.233961--->0.233684) \t Saving The Model\n",
            "Epoch 470 \t\t Training Accuracy: 0.9993973045051098 \t\t Validation Accuracy: 0.9976659183204174\n",
            "Validation Loss Decreased(0.233684--->0.233408) \t Saving The Model\n",
            "Epoch 471 \t\t Training Accuracy: 0.9993984611704946 \t\t Validation Accuracy: 0.9976686687767505\n",
            "Validation Loss Decreased(0.233408--->0.233133) \t Saving The Model\n",
            "Epoch 472 \t\t Training Accuracy: 0.9993996139988304 \t\t Validation Accuracy: 0.9976714098453522\n",
            "Validation Loss Decreased(0.233133--->0.232859) \t Saving The Model\n",
            "Epoch 473 \t\t Training Accuracy: 0.9994007623568177 \t\t Validation Accuracy: 0.9976741397380828\n",
            "Validation Loss Decreased(0.232859--->0.232586) \t Saving The Model\n",
            "Epoch 474 \t\t Training Accuracy: 0.9994019067287445 \t\t Validation Accuracy: 0.997676861435175\n",
            "Validation Loss Decreased(0.232586--->0.232314) \t Saving The Model\n",
            "Epoch 475 \t\t Training Accuracy: 0.999403047375381 \t\t Validation Accuracy: 0.9976795732975006\n",
            "Validation Loss Decreased(0.232314--->0.232043) \t Saving The Model\n",
            "Epoch 476 \t\t Training Accuracy: 0.9994041837751866 \t\t Validation Accuracy: 0.9976822757720947\n",
            "Validation Loss Decreased(0.232043--->0.231772) \t Saving The Model\n",
            "Epoch 477 \t\t Training Accuracy: 0.9994053164497018 \t\t Validation Accuracy: 0.9976849687099457\n",
            "Validation Loss Decreased(0.231772--->0.231503) \t Saving The Model\n",
            "Epoch 478 \t\t Training Accuracy: 0.9994064450636506 \t\t Validation Accuracy: 0.997687651515007\n",
            "Validation Loss Decreased(0.231503--->0.231235) \t Saving The Model\n",
            "Epoch 479 \t\t Training Accuracy: 0.9994075699523092 \t\t Validation Accuracy: 0.9976903259754181\n",
            "Validation Loss Decreased(0.231235--->0.230967) \t Saving The Model\n",
            "Epoch 480 \t\t Training Accuracy: 0.9994086906313896 \t\t Validation Accuracy: 0.9976929903030396\n",
            "Validation Loss Decreased(0.230967--->0.230701) \t Saving The Model\n",
            "Epoch 481 \t\t Training Accuracy: 0.999409807510674 \t\t Validation Accuracy: 0.9976956458389759\n",
            "Validation Loss Decreased(0.230701--->0.230435) \t Saving The Model\n",
            "Epoch 482 \t\t Training Accuracy: 0.9994109207391739 \t\t Validation Accuracy: 0.9976982919871807\n",
            "Validation Loss Decreased(0.230435--->0.230171) \t Saving The Model\n",
            "Epoch 483 \t\t Training Accuracy: 0.9994120299816132 \t\t Validation Accuracy: 0.9977009285986423\n",
            "Validation Loss Decreased(0.230171--->0.229907) \t Saving The Model\n",
            "Epoch 484 \t\t Training Accuracy: 0.9994131355360151 \t\t Validation Accuracy: 0.9977035564184189\n",
            "Validation Loss Decreased(0.229907--->0.229644) \t Saving The Model\n",
            "Epoch 485 \t\t Training Accuracy: 0.9994142371416092 \t\t Validation Accuracy: 0.9977061755955219\n",
            "Validation Loss Decreased(0.229644--->0.229382) \t Saving The Model\n",
            "Epoch 486 \t\t Training Accuracy: 0.9994153349474072 \t\t Validation Accuracy: 0.9977087853848934\n",
            "Validation Loss Decreased(0.229382--->0.229121) \t Saving The Model\n",
            "Epoch 487 \t\t Training Accuracy: 0.9994164291024208 \t\t Validation Accuracy: 0.997711385935545\n",
            "Validation Loss Decreased(0.229121--->0.228861) \t Saving The Model\n",
            "Epoch 488 \t\t Training Accuracy: 0.9994175194203854 \t\t Validation Accuracy: 0.9977139776945114\n",
            "Validation Loss Decreased(0.228861--->0.228602) \t Saving The Model\n",
            "Epoch 489 \t\t Training Accuracy: 0.9994186061620712 \t\t Validation Accuracy: 0.9977165605127811\n",
            "Validation Loss Decreased(0.228602--->0.228344) \t Saving The Model\n",
            "Epoch 490 \t\t Training Accuracy: 0.9994196889922022 \t\t Validation Accuracy: 0.9977191346883774\n",
            "Validation Loss Decreased(0.228344--->0.228087) \t Saving The Model\n",
            "Epoch 491 \t\t Training Accuracy: 0.9994207685068249 \t\t Validation Accuracy: 0.9977217000722886\n",
            "Validation Loss Decreased(0.228087--->0.227830) \t Saving The Model\n",
            "Epoch 492 \t\t Training Accuracy: 0.9994218439981342 \t\t Validation Accuracy: 0.997724256068468\n",
            "Validation Loss Decreased(0.227830--->0.227574) \t Saving The Model\n",
            "Epoch 493 \t\t Training Accuracy: 0.9994229159876704 \t\t Validation Accuracy: 0.997726803869009\n",
            "Validation Loss Decreased(0.227574--->0.227320) \t Saving The Model\n",
            "Epoch 494 \t\t Training Accuracy: 0.9994239841774106 \t\t Validation Accuracy: 0.997729343175888\n",
            "Validation Loss Decreased(0.227320--->0.227066) \t Saving The Model\n",
            "Epoch 495 \t\t Training Accuracy: 0.999425048828125 \t\t Validation Accuracy: 0.997731872946024\n",
            "Validation Loss Decreased(0.227066--->0.226813) \t Saving The Model\n",
            "Epoch 496 \t\t Training Accuracy: 0.9994261098280549 \t\t Validation Accuracy: 0.9977343946695327\n",
            "Validation Loss Decreased(0.226813--->0.226561) \t Saving The Model\n",
            "Epoch 497 \t\t Training Accuracy: 0.9994271673634648 \t\t Validation Accuracy: 0.9977369080483913\n",
            "Validation Loss Decreased(0.226561--->0.226309) \t Saving The Model\n",
            "Epoch 498 \t\t Training Accuracy: 0.9994282211735844 \t\t Validation Accuracy: 0.9977394126355648\n",
            "Validation Loss Decreased(0.226309--->0.226059) \t Saving The Model\n",
            "Epoch 499 \t\t Training Accuracy: 0.9994292716681957 \t\t Validation Accuracy: 0.9977419085800647\n",
            "Validation Loss Decreased(0.226059--->0.225809) \t Saving The Model\n",
            "Epoch 500 \t\t Training Accuracy: 0.9994303185865283 \t\t Validation Accuracy: 0.9977443964779377\n",
            "Validation Loss Decreased(0.225809--->0.225560) \t Saving The Model\n",
            "Epoch 501 \t\t Training Accuracy: 0.9994313617423177 \t\t Validation Accuracy: 0.9977468751370907\n",
            "Validation Loss Decreased(0.225560--->0.225312) \t Saving The Model\n",
            "Epoch 502 \t\t Training Accuracy: 0.9994324016943574 \t\t Validation Accuracy: 0.9977493460476399\n",
            "Validation Loss Decreased(0.225312--->0.225065) \t Saving The Model\n",
            "Epoch 503 \t\t Training Accuracy: 0.9994334376975894 \t\t Validation Accuracy: 0.9977518086135387\n",
            "Validation Loss Decreased(0.225065--->0.224819) \t Saving The Model\n",
            "Epoch 504 \t\t Training Accuracy: 0.9994344706833362 \t\t Validation Accuracy: 0.9977542626857757\n",
            "Validation Loss Decreased(0.224819--->0.224574) \t Saving The Model\n",
            "Epoch 505 \t\t Training Accuracy: 0.9994355000928045 \t\t Validation Accuracy: 0.9977567088603974\n",
            "Validation Loss Decreased(0.224574--->0.224329) \t Saving The Model\n",
            "Epoch 506 \t\t Training Accuracy: 0.9994365261495113 \t\t Validation Accuracy: 0.9977591460943223\n",
            "Validation Loss Decreased(0.224329--->0.224085) \t Saving The Model\n",
            "Epoch 507 \t\t Training Accuracy: 0.9994375487789512 \t\t Validation Accuracy: 0.997761576026678\n",
            "Validation Loss Decreased(0.224085--->0.223842) \t Saving The Model\n",
            "Epoch 508 \t\t Training Accuracy: 0.9994385676458478 \t\t Validation Accuracy: 0.997763997465372\n",
            "Validation Loss Decreased(0.223842--->0.223600) \t Saving The Model\n",
            "Epoch 509 \t\t Training Accuracy: 0.9994395835325122 \t\t Validation Accuracy: 0.9977664107084274\n",
            "Validation Loss Decreased(0.223600--->0.223359) \t Saving The Model\n",
            "Epoch 510 \t\t Training Accuracy: 0.9994405958428979 \t\t Validation Accuracy: 0.9977688167989254\n",
            "Validation Loss Decreased(0.223359--->0.223118) \t Saving The Model\n",
            "Epoch 511 \t\t Training Accuracy: 0.9994416050240398 \t\t Validation Accuracy: 0.997771213799715\n",
            "Validation Loss Decreased(0.223118--->0.222879) \t Saving The Model\n",
            "Epoch 512 \t\t Training Accuracy: 0.9994426107406617 \t\t Validation Accuracy: 0.9977736036479473\n",
            "Validation Loss Decreased(0.222879--->0.222640) \t Saving The Model\n",
            "Epoch 513 \t\t Training Accuracy: 0.9994436128810048 \t\t Validation Accuracy: 0.9977759845554829\n",
            "Validation Loss Decreased(0.222640--->0.222402) \t Saving The Model\n",
            "Epoch 514 \t\t Training Accuracy: 0.9994446119666099 \t\t Validation Accuracy: 0.9977783578634262\n",
            "Validation Loss Decreased(0.222402--->0.222164) \t Saving The Model\n",
            "Epoch 515 \t\t Training Accuracy: 0.9994456076994538 \t\t Validation Accuracy: 0.9977807235717774\n",
            "Validation Loss Decreased(0.222164--->0.221928) \t Saving The Model\n",
            "Epoch 516 \t\t Training Accuracy: 0.9994466000795365 \t\t Validation Accuracy: 0.9977830816805363\n",
            "Validation Loss Decreased(0.221928--->0.221692) \t Saving The Model\n",
            "Epoch 517 \t\t Training Accuracy: 0.9994475893303751 \t\t Validation Accuracy: 0.9977854318916798\n",
            "Validation Loss Decreased(0.221692--->0.221457) \t Saving The Model\n",
            "Epoch 518 \t\t Training Accuracy: 0.9994485754147172 \t\t Validation Accuracy: 0.9977877740561962\n",
            "Validation Loss Decreased(0.221457--->0.221223) \t Saving The Model\n",
            "Epoch 519 \t\t Training Accuracy: 0.9994495579600334 \t\t Validation Accuracy: 0.9977901087701321\n",
            "Validation Loss Decreased(0.221223--->0.220989) \t Saving The Model\n",
            "Epoch 520 \t\t Training Accuracy: 0.9994505373761058 \t\t Validation Accuracy: 0.9977924357354641\n",
            "Validation Loss Decreased(0.220989--->0.220756) \t Saving The Model\n",
            "Epoch 521 \t\t Training Accuracy: 0.999451513774693 \t\t Validation Accuracy: 0.9977947545051574\n",
            "Validation Loss Decreased(0.220756--->0.220525) \t Saving The Model\n",
            "Epoch 522 \t\t Training Accuracy: 0.9994524870440364 \t\t Validation Accuracy: 0.9977970664203167\n",
            "Validation Loss Decreased(0.220525--->0.220293) \t Saving The Model\n",
            "Epoch 523 \t\t Training Accuracy: 0.9994534567371011 \t\t Validation Accuracy: 0.9977993702888489\n",
            "Validation Loss Decreased(0.220293--->0.220063) \t Saving The Model\n",
            "Epoch 524 \t\t Training Accuracy: 0.9994544237852097 \t\t Validation Accuracy: 0.9978016670048236\n",
            "Validation Loss Decreased(0.220063--->0.219833) \t Saving The Model\n",
            "Epoch 525 \t\t Training Accuracy: 0.9994553872942924 \t\t Validation Accuracy: 0.9978039562702179\n",
            "Validation Loss Decreased(0.219833--->0.219604) \t Saving The Model\n",
            "Epoch 526 \t\t Training Accuracy: 0.9994563478603959 \t\t Validation Accuracy: 0.9978062379360199\n",
            "Validation Loss Decreased(0.219604--->0.219376) \t Saving The Model\n",
            "Epoch 527 \t\t Training Accuracy: 0.9994573051854968 \t\t Validation Accuracy: 0.9978085117042065\n",
            "Validation Loss Decreased(0.219376--->0.219149) \t Saving The Model\n",
            "Epoch 528 \t\t Training Accuracy: 0.9994582596793771 \t\t Validation Accuracy: 0.9978107787668705\n",
            "Validation Loss Decreased(0.219149--->0.218922) \t Saving The Model\n",
            "Epoch 529 \t\t Training Accuracy: 0.9994592109695077 \t\t Validation Accuracy: 0.9978130388259888\n",
            "Validation Loss Decreased(0.218922--->0.218696) \t Saving The Model\n",
            "Epoch 530 \t\t Training Accuracy: 0.9994601590931416 \t\t Validation Accuracy: 0.997815290093422\n",
            "Validation Loss Decreased(0.218696--->0.218471) \t Saving The Model\n",
            "Epoch 531 \t\t Training Accuracy: 0.9994611040130258 \t\t Validation Accuracy: 0.9978175352513791\n",
            "Validation Loss Decreased(0.218471--->0.218246) \t Saving The Model\n",
            "Epoch 532 \t\t Training Accuracy: 0.9994620459154249 \t\t Validation Accuracy: 0.9978197725117206\n",
            "Validation Loss Decreased(0.218246--->0.218023) \t Saving The Model\n",
            "Epoch 533 \t\t Training Accuracy: 0.9994629849493504 \t\t Validation Accuracy: 0.9978220029175282\n",
            "Validation Loss Decreased(0.218023--->0.217800) \t Saving The Model\n",
            "Epoch 534 \t\t Training Accuracy: 0.9994639210030436 \t\t Validation Accuracy: 0.9978242260217667\n",
            "Validation Loss Decreased(0.217800--->0.217577) \t Saving The Model\n",
            "Epoch 535 \t\t Training Accuracy: 0.9994648541137576 \t\t Validation Accuracy: 0.9978264421224594\n",
            "Validation Loss Decreased(0.217577--->0.217356) \t Saving The Model\n",
            "Epoch 536 \t\t Training Accuracy: 0.9994657840952277 \t\t Validation Accuracy: 0.99782865062356\n",
            "Validation Loss Decreased(0.217356--->0.217135) \t Saving The Model\n",
            "Epoch 537 \t\t Training Accuracy: 0.9994667111709714 \t\t Validation Accuracy: 0.9978308528661728\n",
            "Validation Loss Decreased(0.217135--->0.216915) \t Saving The Model\n",
            "Epoch 538 \t\t Training Accuracy: 0.9994676353409886 \t\t Validation Accuracy: 0.9978330467641353\n",
            "Validation Loss Decreased(0.216915--->0.216695) \t Saving The Model\n",
            "Epoch 539 \t\t Training Accuracy: 0.999468556381762 \t\t Validation Accuracy: 0.9978352354466915\n",
            "Validation Loss Decreased(0.216695--->0.216476) \t Saving The Model\n",
            "Epoch 540 \t\t Training Accuracy: 0.9994694747030735 \t\t Validation Accuracy: 0.9978374160826207\n",
            "Validation Loss Decreased(0.216476--->0.216258) \t Saving The Model\n",
            "Epoch 541 \t\t Training Accuracy: 0.9994703897833824 \t\t Validation Accuracy: 0.9978395886719227\n",
            "Validation Loss Decreased(0.216258--->0.216041) \t Saving The Model\n",
            "Epoch 542 \t\t Training Accuracy: 0.9994713021814823 \t\t Validation Accuracy: 0.9978417558968067\n",
            "Validation Loss Decreased(0.216041--->0.215824) \t Saving The Model\n",
            "Epoch 543 \t\t Training Accuracy: 0.9994722117111087 \t\t Validation Accuracy: 0.9978439162671566\n",
            "Validation Loss Decreased(0.215824--->0.215608) \t Saving The Model\n",
            "Epoch 544 \t\t Training Accuracy: 0.9994731183350086 \t\t Validation Accuracy: 0.997846068739891\n",
            "Validation Loss Decreased(0.215608--->0.215393) \t Saving The Model\n",
            "Epoch 545 \t\t Training Accuracy: 0.9994740219041706 \t\t Validation Accuracy: 0.9978482146561146\n",
            "Validation Loss Decreased(0.215393--->0.215179) \t Saving The Model\n",
            "Epoch 546 \t\t Training Accuracy: 0.9994749228283762 \t\t Validation Accuracy: 0.997850354462862\n",
            "Validation Loss Decreased(0.215179--->0.214965) \t Saving The Model\n",
            "Epoch 547 \t\t Training Accuracy: 0.9994758207723499 \t\t Validation Accuracy: 0.9978524874150753\n",
            "Validation Loss Decreased(0.214965--->0.214751) \t Saving The Model\n",
            "Epoch 548 \t\t Training Accuracy: 0.9994767160713672 \t\t Validation Accuracy: 0.997854612916708\n",
            "Validation Loss Decreased(0.214751--->0.214539) \t Saving The Model\n",
            "Epoch 549 \t\t Training Accuracy: 0.9994776083901524 \t\t Validation Accuracy: 0.9978567318618298\n",
            "Validation Loss Decreased(0.214539--->0.214327) \t Saving The Model\n",
            "Epoch 550 \t\t Training Accuracy: 0.9994784977659583 \t\t Validation Accuracy: 0.9978588438034057\n",
            "Validation Loss Decreased(0.214327--->0.214116) \t Saving The Model\n",
            "Epoch 551 \t\t Training Accuracy: 0.9994793847203255 \t\t Validation Accuracy: 0.9978609497845173\n",
            "Validation Loss Decreased(0.214116--->0.213905) \t Saving The Model\n",
            "Epoch 552 \t\t Training Accuracy: 0.9994802685454488 \t\t Validation Accuracy: 0.9978630495071411\n",
            "Validation Loss Decreased(0.213905--->0.213695) \t Saving The Model\n",
            "Epoch 553 \t\t Training Accuracy: 0.9994811499118805 \t\t Validation Accuracy: 0.9978651417791844\n",
            "Validation Loss Decreased(0.213695--->0.213486) \t Saving The Model\n",
            "Epoch 554 \t\t Training Accuracy: 0.9994820281863213 \t\t Validation Accuracy: 0.997867227345705\n",
            "Validation Loss Decreased(0.213486--->0.213277) \t Saving The Model\n",
            "Epoch 555 \t\t Training Accuracy: 0.9994829040765763 \t\t Validation Accuracy: 0.9978693073987961\n",
            "Validation Loss Decreased(0.213277--->0.213069) \t Saving The Model\n",
            "Epoch 556 \t\t Training Accuracy: 0.9994837770238518 \t\t Validation Accuracy: 0.9978713808953762\n",
            "Validation Loss Decreased(0.213069--->0.212862) \t Saving The Model\n",
            "Epoch 557 \t\t Training Accuracy: 0.9994846474751831 \t\t Validation Accuracy: 0.9978734473884106\n",
            "Validation Loss Decreased(0.212862--->0.212655) \t Saving The Model\n",
            "Epoch 558 \t\t Training Accuracy: 0.9994855149462819 \t\t Validation Accuracy: 0.997875507324934\n",
            "Validation Loss Decreased(0.212655--->0.212449) \t Saving The Model\n",
            "Epoch 559 \t\t Training Accuracy: 0.9994863795116544 \t\t Validation Accuracy: 0.9978775607049465\n",
            "Validation Loss Decreased(0.212449--->0.212244) \t Saving The Model\n",
            "Epoch 560 \t\t Training Accuracy: 0.9994872419163585 \t\t Validation Accuracy: 0.9978796075284481\n",
            "Validation Loss Decreased(0.212244--->0.212039) \t Saving The Model\n",
            "Epoch 561 \t\t Training Accuracy: 0.9994881014153362 \t\t Validation Accuracy: 0.9978816485404969\n",
            "Validation Loss Decreased(0.212039--->0.211835) \t Saving The Model\n",
            "Epoch 562 \t\t Training Accuracy: 0.9994889583066106 \t\t Validation Accuracy: 0.9978836829960346\n",
            "Validation Loss Decreased(0.211835--->0.211632) \t Saving The Model\n",
            "Epoch 563 \t\t Training Accuracy: 0.9994898124411702 \t\t Validation Accuracy: 0.9978857113420964\n",
            "Validation Loss Decreased(0.211632--->0.211429) \t Saving The Model\n",
            "Epoch 564 \t\t Training Accuracy: 0.9994906640052795 \t\t Validation Accuracy: 0.9978877337276936\n",
            "Validation Loss Decreased(0.211429--->0.211227) \t Saving The Model\n",
            "Epoch 565 \t\t Training Accuracy: 0.9994915128871799 \t\t Validation Accuracy: 0.9978897492587566\n",
            "Validation Loss Decreased(0.211227--->0.211025) \t Saving The Model\n",
            "Epoch 566 \t\t Training Accuracy: 0.999492359124124 \t\t Validation Accuracy: 0.9978917586803436\n",
            "Validation Loss Decreased(0.211025--->0.210824) \t Saving The Model\n",
            "Epoch 567 \t\t Training Accuracy: 0.9994932029768825 \t\t Validation Accuracy: 0.9978937621414662\n",
            "Validation Loss Decreased(0.210824--->0.210624) \t Saving The Model\n",
            "Epoch 568 \t\t Training Accuracy: 0.9994940440729261 \t\t Validation Accuracy: 0.9978957597911358\n",
            "Validation Loss Decreased(0.210624--->0.210424) \t Saving The Model\n",
            "Epoch 569 \t\t Training Accuracy: 0.9994948826357722 \t\t Validation Accuracy: 0.9978977505862713\n",
            "Validation Loss Decreased(0.210424--->0.210225) \t Saving The Model\n",
            "Epoch 570 \t\t Training Accuracy: 0.9994957185909152 \t\t Validation Accuracy: 0.9978997355699539\n",
            "Validation Loss Decreased(0.210225--->0.210026) \t Saving The Model\n",
            "Epoch 571 \t\t Training Accuracy: 0.9994965521618724 \t\t Validation Accuracy: 0.9979017142951488\n",
            "Validation Loss Decreased(0.210026--->0.209829) \t Saving The Model\n",
            "Epoch 572 \t\t Training Accuracy: 0.9994973829016089 \t\t Validation Accuracy: 0.9979036869108677\n",
            "Validation Loss Decreased(0.209829--->0.209631) \t Saving The Model\n",
            "Epoch 573 \t\t Training Accuracy: 0.9994982110336423 \t\t Validation Accuracy: 0.997905653566122\n",
            "Validation Loss Decreased(0.209631--->0.209435) \t Saving The Model\n",
            "Epoch 574 \t\t Training Accuracy: 0.9994990370050073 \t\t Validation Accuracy: 0.9979076141119003\n",
            "Validation Loss Decreased(0.209435--->0.209239) \t Saving The Model\n",
            "Epoch 575 \t\t Training Accuracy: 0.999499860368669 \t\t Validation Accuracy: 0.9979095685482026\n",
            "Validation Loss Decreased(0.209239--->0.209043) \t Saving The Model\n",
            "Epoch 576 \t\t Training Accuracy: 0.9995006811618805 \t\t Validation Accuracy: 0.9979115168750287\n",
            "Validation Loss Decreased(0.209043--->0.208848) \t Saving The Model\n",
            "Epoch 577 \t\t Training Accuracy: 0.9995014994964003 \t\t Validation Accuracy: 0.9979134601354599\n",
            "Validation Loss Decreased(0.208848--->0.208654) \t Saving The Model\n",
            "Epoch 578 \t\t Training Accuracy: 0.999502315223217 \t\t Validation Accuracy: 0.9979153974354267\n",
            "Validation Loss Decreased(0.208654--->0.208460) \t Saving The Model\n",
            "Epoch 579 \t\t Training Accuracy: 0.9995031287148595 \t\t Validation Accuracy: 0.9979173278808594\n",
            "Validation Loss Decreased(0.208460--->0.208267) \t Saving The Model\n",
            "Epoch 580 \t\t Training Accuracy: 0.9995039394497871 \t\t Validation Accuracy: 0.997919253706932\n",
            "Validation Loss Decreased(0.208267--->0.208075) \t Saving The Model\n",
            "Epoch 581 \t\t Training Accuracy: 0.9995047480985523 \t\t Validation Accuracy: 0.9979211734235287\n",
            "Validation Loss Decreased(0.208075--->0.207883) \t Saving The Model\n",
            "Epoch 582 \t\t Training Accuracy: 0.9995055539533496 \t\t Validation Accuracy: 0.9979230862855911\n",
            "Validation Loss Decreased(0.207883--->0.207691) \t Saving The Model\n",
            "Epoch 583 \t\t Training Accuracy: 0.9995063575729728 \t\t Validation Accuracy: 0.9979249940812588\n",
            "Validation Loss Decreased(0.207691--->0.207501) \t Saving The Model\n",
            "Epoch 584 \t\t Training Accuracy: 0.9995071585476398 \t\t Validation Accuracy: 0.99792689666152\n",
            "Validation Loss Decreased(0.207501--->0.207310) \t Saving The Model\n",
            "Epoch 585 \t\t Training Accuracy: 0.9995079572871327 \t\t Validation Accuracy: 0.9979287926852703\n",
            "Validation Loss Decreased(0.207310--->0.207121) \t Saving The Model\n",
            "Epoch 586 \t\t Training Accuracy: 0.9995087537541986 \t\t Validation Accuracy: 0.9979306836426258\n",
            "Validation Loss Decreased(0.207121--->0.206932) \t Saving The Model\n",
            "Epoch 587 \t\t Training Accuracy: 0.9995095478370786 \t\t Validation Accuracy: 0.9979325692355633\n",
            "Validation Loss Decreased(0.206932--->0.206743) \t Saving The Model\n",
            "Epoch 588 \t\t Training Accuracy: 0.999510339461267 \t\t Validation Accuracy: 0.9979344479739666\n",
            "Validation Loss Decreased(0.206743--->0.206555) \t Saving The Model\n",
            "Epoch 589 \t\t Training Accuracy: 0.9995111286267638 \t\t Validation Accuracy: 0.9979363216459751\n",
            "Validation Loss Decreased(0.206555--->0.206368) \t Saving The Model\n",
            "Epoch 590 \t\t Training Accuracy: 0.9995119153335691 \t\t Validation Accuracy: 0.9979381895065308\n",
            "Validation Loss Decreased(0.206368--->0.206181) \t Saving The Model\n",
            "Epoch 591 \t\t Training Accuracy: 0.9995127001404762 \t\t Validation Accuracy: 0.9979400527477265\n",
            "Validation Loss Decreased(0.206181--->0.205995) \t Saving The Model\n",
            "Epoch 592 \t\t Training Accuracy: 0.9995134823396802 \t\t Validation Accuracy: 0.9979419092833995\n",
            "Validation Loss Decreased(0.205995--->0.205809) \t Saving The Model\n",
            "Epoch 593 \t\t Training Accuracy: 0.9995142620801926 \t\t Validation Accuracy: 0.9979437606036663\n",
            "Validation Loss Decreased(0.205809--->0.205624) \t Saving The Model\n",
            "Epoch 594 \t\t Training Accuracy: 0.9995150397345424 \t\t Validation Accuracy: 0.997945606559515\n",
            "Validation Loss Decreased(0.205624--->0.205439) \t Saving The Model\n",
            "Epoch 595 \t\t Training Accuracy: 0.999515814743936 \t\t Validation Accuracy: 0.9979474467039108\n",
            "Validation Loss Decreased(0.205439--->0.205255) \t Saving The Model\n",
            "Epoch 596 \t\t Training Accuracy: 0.9995165878534317 \t\t Validation Accuracy: 0.997949282079935\n",
            "Validation Loss Decreased(0.205255--->0.205072) \t Saving The Model\n",
            "Epoch 597 \t\t Training Accuracy: 0.9995173585042357 \t\t Validation Accuracy: 0.9979511113464832\n",
            "Validation Loss Decreased(0.205072--->0.204889) \t Saving The Model\n",
            "Epoch 598 \t\t Training Accuracy: 0.9995181268453598 \t\t Validation Accuracy: 0.9979529355466366\n",
            "Validation Loss Decreased(0.204889--->0.204706) \t Saving The Model\n",
            "Epoch 599 \t\t Training Accuracy: 0.9995188929513097 \t\t Validation Accuracy: 0.9979547537863255\n",
            "Validation Loss Decreased(0.204706--->0.204525) \t Saving The Model\n",
            "Epoch 600 \t\t Training Accuracy: 0.9995196567475796 \t\t Validation Accuracy: 0.997956567555666\n",
            "Validation Loss Decreased(0.204525--->0.204343) \t Saving The Model\n",
            "Epoch 601 \t\t Training Accuracy: 0.9995204182341695 \t\t Validation Accuracy: 0.9979583759605885\n",
            "Validation Loss Decreased(0.204343--->0.204162) \t Saving The Model\n",
            "Epoch 602 \t\t Training Accuracy: 0.9995211777091026 \t\t Validation Accuracy: 0.9979601787030696\n",
            "Validation Loss Decreased(0.204162--->0.203982) \t Saving The Model\n",
            "Epoch 603 \t\t Training Accuracy: 0.9995219349861145 \t\t Validation Accuracy: 0.9979619757831096\n",
            "Validation Loss Decreased(0.203982--->0.203802) \t Saving The Model\n",
            "Epoch 604 \t\t Training Accuracy: 0.9995226896181703 \t\t Validation Accuracy: 0.9979637682437896\n",
            "Validation Loss Decreased(0.203802--->0.203623) \t Saving The Model\n",
            "Epoch 605 \t\t Training Accuracy: 0.9995234423875808 \t\t Validation Accuracy: 0.9979655547440052\n",
            "Validation Loss Decreased(0.203623--->0.203445) \t Saving The Model\n",
            "Epoch 606 \t\t Training Accuracy: 0.9995241929590702 \t\t Validation Accuracy: 0.9979673366248608\n",
            "Validation Loss Decreased(0.203445--->0.203266) \t Saving The Model\n",
            "Epoch 607 \t\t Training Accuracy: 0.9995249410346151 \t\t Validation Accuracy: 0.9979691131412983\n",
            "Validation Loss Decreased(0.203266--->0.203089) \t Saving The Model\n",
            "Epoch 608 \t\t Training Accuracy: 0.9995256872847676 \t\t Validation Accuracy: 0.9979708848893643\n",
            "Validation Loss Decreased(0.203089--->0.202912) \t Saving The Model\n",
            "Epoch 609 \t\t Training Accuracy: 0.9995264313742518 \t\t Validation Accuracy: 0.9979726512730122\n",
            "Validation Loss Decreased(0.202912--->0.202735) \t Saving The Model\n",
            "Epoch 610 \t\t Training Accuracy: 0.9995271730050445 \t\t Validation Accuracy: 0.9979744116961956\n",
            "Validation Loss Decreased(0.202735--->0.202559) \t Saving The Model\n",
            "Epoch 611 \t\t Training Accuracy: 0.9995279128476977 \t\t Validation Accuracy: 0.9979761676490306\n",
            "Validation Loss Decreased(0.202559--->0.202383) \t Saving The Model\n",
            "Epoch 612 \t\t Training Accuracy: 0.9995286501571536 \t\t Validation Accuracy: 0.9979779177904129\n",
            "Validation Loss Decreased(0.202383--->0.202208) \t Saving The Model\n",
            "Epoch 613 \t\t Training Accuracy: 0.9995293856039643 \t\t Validation Accuracy: 0.9979796640574932\n",
            "Validation Loss Decreased(0.202208--->0.202034) \t Saving The Model\n",
            "Epoch 614 \t\t Training Accuracy: 0.9995301187038421 \t\t Validation Accuracy: 0.9979814045131207\n",
            "Validation Loss Decreased(0.202034--->0.201860) \t Saving The Model\n",
            "Epoch 615 \t\t Training Accuracy: 0.9995308497548103 \t\t Validation Accuracy: 0.9979831402003765\n",
            "Validation Loss Decreased(0.201860--->0.201686) \t Saving The Model\n",
            "Epoch 616 \t\t Training Accuracy: 0.999531578719616 \t\t Validation Accuracy: 0.9979848705232144\n",
            "Validation Loss Decreased(0.201686--->0.201513) \t Saving The Model\n",
            "Epoch 617 \t\t Training Accuracy: 0.9995323056355119 \t\t Validation Accuracy: 0.9979865957796573\n",
            "Validation Loss Decreased(0.201513--->0.201340) \t Saving The Model\n",
            "Epoch 618 \t\t Training Accuracy: 0.9995330304279924 \t\t Validation Accuracy: 0.9979883170127869\n",
            "Validation Loss Decreased(0.201340--->0.201168) \t Saving The Model\n",
            "Epoch 619 \t\t Training Accuracy: 0.9995337530970574 \t\t Validation Accuracy: 0.9979900325834751\n",
            "Validation Loss Decreased(0.201168--->0.200997) \t Saving The Model\n",
            "Epoch 620 \t\t Training Accuracy: 0.9995344737544656 \t\t Validation Accuracy: 0.9979917435348034\n",
            "Validation Loss Decreased(0.200997--->0.200826) \t Saving The Model\n",
            "Epoch 621 \t\t Training Accuracy: 0.9995351923629642 \t\t Validation Accuracy: 0.9979934494197369\n",
            "Validation Loss Decreased(0.200826--->0.200655) \t Saving The Model\n",
            "Epoch 622 \t\t Training Accuracy: 0.9995359086990356 \t\t Validation Accuracy: 0.9979951505362987\n",
            "Validation Loss Decreased(0.200655--->0.200485) \t Saving The Model\n",
            "Epoch 623 \t\t Training Accuracy: 0.999536623172462 \t\t Validation Accuracy: 0.9979968464374542\n",
            "Validation Loss Decreased(0.200485--->0.200315) \t Saving The Model\n",
            "Epoch 624 \t\t Training Accuracy: 0.9995373355224728 \t\t Validation Accuracy: 0.9979985380172729\n",
            "Validation Loss Decreased(0.200315--->0.200146) \t Saving The Model\n",
            "Epoch 625 \t\t Training Accuracy: 0.9995380456745625 \t\t Validation Accuracy: 0.9980002237856388\n",
            "Validation Loss Decreased(0.200146--->0.199978) \t Saving The Model\n",
            "Epoch 626 \t\t Training Accuracy: 0.9995387542247772 \t\t Validation Accuracy: 0.9980019056797027\n",
            "Validation Loss Decreased(0.199978--->0.199809) \t Saving The Model\n",
            "Epoch 627 \t\t Training Accuracy: 0.9995394605398178 \t\t Validation Accuracy: 0.9980035823583603\n",
            "Validation Loss Decreased(0.199809--->0.199642) \t Saving The Model\n",
            "Epoch 628 \t\t Training Accuracy: 0.9995401645824313 \t\t Validation Accuracy: 0.9980052542686463\n",
            "Validation Loss Decreased(0.199642--->0.199475) \t Saving The Model\n",
            "Epoch 629 \t\t Training Accuracy: 0.9995408670976758 \t\t Validation Accuracy: 0.9980069218575954\n",
            "Validation Loss Decreased(0.199475--->0.199308) \t Saving The Model\n",
            "Epoch 630 \t\t Training Accuracy: 0.999541567414999 \t\t Validation Accuracy: 0.9980085845291614\n",
            "Validation Loss Decreased(0.199308--->0.199142) \t Saving The Model\n",
            "Epoch 631 \t\t Training Accuracy: 0.9995422656461597 \t\t Validation Accuracy: 0.9980102424323559\n",
            "Validation Loss Decreased(0.199142--->0.198976) \t Saving The Model\n",
            "Epoch 632 \t\t Training Accuracy: 0.9995429618284106 \t\t Validation Accuracy: 0.9980118952691555\n",
            "Validation Loss Decreased(0.198976--->0.198810) \t Saving The Model\n",
            "Epoch 633 \t\t Training Accuracy: 0.9995436562597751 \t\t Validation Accuracy: 0.9980135436356068\n",
            "Validation Loss Decreased(0.198810--->0.198646) \t Saving The Model\n",
            "Epoch 634 \t\t Training Accuracy: 0.9995443485304714 \t\t Validation Accuracy: 0.998015187382698\n",
            "Validation Loss Decreased(0.198646--->0.198481) \t Saving The Model\n",
            "Epoch 635 \t\t Training Accuracy: 0.9995450392365456 \t\t Validation Accuracy: 0.998016826659441\n",
            "Validation Loss Decreased(0.198481--->0.198317) \t Saving The Model\n",
            "Epoch 636 \t\t Training Accuracy: 0.9995457276701927 \t\t Validation Accuracy: 0.9980184610188008\n",
            "Validation Loss Decreased(0.198317--->0.198154) \t Saving The Model\n",
            "Epoch 637 \t\t Training Accuracy: 0.9995464142039419 \t\t Validation Accuracy: 0.9980200906097889\n",
            "Validation Loss Decreased(0.198154--->0.197991) \t Saving The Model\n",
            "Epoch 638 \t\t Training Accuracy: 0.9995470987632871 \t\t Validation Accuracy: 0.9980217158794403\n",
            "Validation Loss Decreased(0.197991--->0.197828) \t Saving The Model\n",
            "Epoch 639 \t\t Training Accuracy: 0.9995477813482284 \t\t Validation Accuracy: 0.9980233369767666\n",
            "Validation Loss Decreased(0.197828--->0.197666) \t Saving The Model\n",
            "Epoch 640 \t\t Training Accuracy: 0.9995484620705247 \t\t Validation Accuracy: 0.9980249534547329\n",
            "Validation Loss Decreased(0.197666--->0.197505) \t Saving The Model\n",
            "Epoch 641 \t\t Training Accuracy: 0.9995491410419345 \t\t Validation Accuracy: 0.9980265651643276\n",
            "Validation Loss Decreased(0.197505--->0.197343) \t Saving The Model\n",
            "Epoch 642 \t\t Training Accuracy: 0.9995498180016875 \t\t Validation Accuracy: 0.9980281722545624\n",
            "Validation Loss Decreased(0.197343--->0.197183) \t Saving The Model\n",
            "Epoch 643 \t\t Training Accuracy: 0.9995504931733012 \t\t Validation Accuracy: 0.9980297748744488\n",
            "Validation Loss Decreased(0.197183--->0.197023) \t Saving The Model\n",
            "Epoch 644 \t\t Training Accuracy: 0.999551166407764 \t\t Validation Accuracy: 0.9980313728749752\n",
            "Validation Loss Decreased(0.197023--->0.196863) \t Saving The Model\n",
            "Epoch 645 \t\t Training Accuracy: 0.9995518377050757 \t\t Validation Accuracy: 0.9980329670011997\n",
            "Validation Loss Decreased(0.196863--->0.196703) \t Saving The Model\n",
            "Epoch 646 \t\t Training Accuracy: 0.9995525071024894 \t\t Validation Accuracy: 0.9980345566570759\n",
            "Validation Loss Decreased(0.196703--->0.196544) \t Saving The Model\n",
            "Epoch 647 \t\t Training Accuracy: 0.999553174674511 \t\t Validation Accuracy: 0.9980361415445804\n",
            "Validation Loss Decreased(0.196544--->0.196386) \t Saving The Model\n",
            "Epoch 648 \t\t Training Accuracy: 0.9995538402721286 \t\t Validation Accuracy: 0.9980377215147018\n",
            "Validation Loss Decreased(0.196386--->0.196228) \t Saving The Model\n",
            "Epoch 649 \t\t Training Accuracy: 0.9995545041561127 \t\t Validation Accuracy: 0.9980392976105213\n",
            "Validation Loss Decreased(0.196228--->0.196070) \t Saving The Model\n",
            "Epoch 650 \t\t Training Accuracy: 0.9995551662519574 \t\t Validation Accuracy: 0.9980408696830273\n",
            "Validation Loss Decreased(0.196070--->0.195913) \t Saving The Model\n",
            "Epoch 651 \t\t Training Accuracy: 0.9995558262988925 \t\t Validation Accuracy: 0.9980424369871617\n",
            "Validation Loss Decreased(0.195913--->0.195756) \t Saving The Model\n",
            "Epoch 652 \t\t Training Accuracy: 0.9995564847066999 \t\t Validation Accuracy: 0.9980440002679825\n",
            "Validation Loss Decreased(0.195756--->0.195600) \t Saving The Model\n",
            "Epoch 653 \t\t Training Accuracy: 0.999557141251862 \t\t Validation Accuracy: 0.9980455593764782\n",
            "Validation Loss Decreased(0.195600--->0.195444) \t Saving The Model\n",
            "Epoch 654 \t\t Training Accuracy: 0.9995577959343791 \t\t Validation Accuracy: 0.9980471135675907\n",
            "Validation Loss Decreased(0.195444--->0.195289) \t Saving The Model\n",
            "Epoch 655 \t\t Training Accuracy: 0.9995584487915039 \t\t Validation Accuracy: 0.9980486643314361\n",
            "Validation Loss Decreased(0.195289--->0.195134) \t Saving The Model\n",
            "Epoch 656 \t\t Training Accuracy: 0.9995590999722481 \t\t Validation Accuracy: 0.9980502098798751\n",
            "Validation Loss Decreased(0.195134--->0.194979) \t Saving The Model\n",
            "Epoch 657 \t\t Training Accuracy: 0.9995597493648529 \t\t Validation Accuracy: 0.9980517518520355\n",
            "Validation Loss Decreased(0.194979--->0.194825) \t Saving The Model\n",
            "Epoch 658 \t\t Training Accuracy: 0.9995603971555829 \t\t Validation Accuracy: 0.9980532898008824\n",
            "Validation Loss Decreased(0.194825--->0.194671) \t Saving The Model\n",
            "Epoch 659 \t\t Training Accuracy: 0.9995610426738858 \t\t Validation Accuracy: 0.9980548231303692\n",
            "Validation Loss Decreased(0.194671--->0.194518) \t Saving The Model\n",
            "Epoch 660 \t\t Training Accuracy: 0.9995616868138313 \t\t Validation Accuracy: 0.9980563521385193\n",
            "Validation Loss Decreased(0.194518--->0.194365) \t Saving The Model\n",
            "Epoch 661 \t\t Training Accuracy: 0.9995623289048672 \t\t Validation Accuracy: 0.9980578774213791\n",
            "Validation Loss Decreased(0.194365--->0.194212) \t Saving The Model\n",
            "Epoch 662 \t\t Training Accuracy: 0.9995629695057869 \t\t Validation Accuracy: 0.9980593980848789\n",
            "Validation Loss Decreased(0.194212--->0.194060) \t Saving The Model\n",
            "Epoch 663 \t\t Training Accuracy: 0.9995636083185673 \t\t Validation Accuracy: 0.9980609136819839\n",
            "Validation Loss Decreased(0.194060--->0.193909) \t Saving The Model\n",
            "Epoch 664 \t\t Training Accuracy: 0.999564245454967 \t\t Validation Accuracy: 0.9980624265968799\n",
            "Validation Loss Decreased(0.193909--->0.193757) \t Saving The Model\n",
            "Epoch 665 \t\t Training Accuracy: 0.9995648803934455 \t\t Validation Accuracy: 0.9980639350414277\n",
            "Validation Loss Decreased(0.193757--->0.193606) \t Saving The Model\n",
            "Epoch 666 \t\t Training Accuracy: 0.9995655142143369 \t\t Validation Accuracy: 0.9980654393136501\n",
            "Validation Loss Decreased(0.193606--->0.193456) \t Saving The Model\n",
            "Epoch 667 \t\t Training Accuracy: 0.9995661460235715 \t\t Validation Accuracy: 0.9980669395625591\n",
            "Validation Loss Decreased(0.193456--->0.193306) \t Saving The Model\n",
            "Epoch 668 \t\t Training Accuracy: 0.9995667758584023 \t\t Validation Accuracy: 0.9980684359371662\n",
            "Validation Loss Decreased(0.193306--->0.193156) \t Saving The Model\n",
            "Epoch 669 \t\t Training Accuracy: 0.999567404538393 \t\t Validation Accuracy: 0.9980699281394482\n",
            "Validation Loss Decreased(0.193156--->0.193007) \t Saving The Model\n",
            "Epoch 670 \t\t Training Accuracy: 0.99956803124398 \t\t Validation Accuracy: 0.9980714166164398\n",
            "Validation Loss Decreased(0.193007--->0.192858) \t Saving The Model\n",
            "Epoch 671 \t\t Training Accuracy: 0.9995686560869217 \t\t Validation Accuracy: 0.9980729009211063\n",
            "Validation Loss Decreased(0.192858--->0.192710) \t Saving The Model\n",
            "Epoch 672 \t\t Training Accuracy: 0.9995692794397474 \t\t Validation Accuracy: 0.9980743806064128\n",
            "Validation Loss Decreased(0.192710--->0.192562) \t Saving The Model\n",
            "Epoch 673 \t\t Training Accuracy: 0.9995699011161924 \t\t Validation Accuracy: 0.9980758564174175\n",
            "Validation Loss Decreased(0.192562--->0.192414) \t Saving The Model\n",
            "Epoch 674 \t\t Training Accuracy: 0.9995705211535096 \t\t Validation Accuracy: 0.9980773292481899\n",
            "Validation Loss Decreased(0.192414--->0.192267) \t Saving The Model\n",
            "Epoch 675 \t\t Training Accuracy: 0.999571139216423 \t\t Validation Accuracy: 0.9980787968635559\n",
            "Validation Loss Decreased(0.192267--->0.192120) \t Saving The Model\n",
            "Epoch 676 \t\t Training Accuracy: 0.999571755938232 \t\t Validation Accuracy: 0.9980802613496781\n",
            "Validation Loss Decreased(0.192120--->0.191974) \t Saving The Model\n",
            "Epoch 677 \t\t Training Accuracy: 0.9995723707973957 \t\t Validation Accuracy: 0.9980817218124867\n",
            "Validation Loss Decreased(0.191974--->0.191828) \t Saving The Model\n",
            "Epoch 678 \t\t Training Accuracy: 0.9995729843899608 \t\t Validation Accuracy: 0.9980831779539585\n",
            "Validation Loss Decreased(0.191828--->0.191682) \t Saving The Model\n",
            "Epoch 679 \t\t Training Accuracy: 0.9995735959336162 \t\t Validation Accuracy: 0.9980846309661865\n",
            "Validation Loss Decreased(0.191682--->0.191537) \t Saving The Model\n",
            "Epoch 680 \t\t Training Accuracy: 0.9995742059871554 \t\t Validation Accuracy: 0.9980860801041126\n",
            "Validation Loss Decreased(0.191537--->0.191392) \t Saving The Model\n",
            "Epoch 681 \t\t Training Accuracy: 0.9995748143270612 \t\t Validation Accuracy: 0.9980875246226788\n",
            "Validation Loss Decreased(0.191392--->0.191248) \t Saving The Model\n",
            "Epoch 682 \t\t Training Accuracy: 0.9995754212513566 \t\t Validation Accuracy: 0.998088965266943\n",
            "Validation Loss Decreased(0.191248--->0.191103) \t Saving The Model\n",
            "Epoch 683 \t\t Training Accuracy: 0.9995760263130069 \t\t Validation Accuracy: 0.9980904023349285\n",
            "Validation Loss Decreased(0.191103--->0.190960) \t Saving The Model\n",
            "Epoch 684 \t\t Training Accuracy: 0.9995766296982765 \t\t Validation Accuracy: 0.9980918361246586\n",
            "Validation Loss Decreased(0.190960--->0.190816) \t Saving The Model\n",
            "Epoch 685 \t\t Training Accuracy: 0.9995772317424416 \t\t Validation Accuracy: 0.9980932657420635\n",
            "Validation Loss Decreased(0.190816--->0.190673) \t Saving The Model\n",
            "Epoch 686 \t\t Training Accuracy: 0.9995778322219848 \t\t Validation Accuracy: 0.9980946910381318\n",
            "Validation Loss Decreased(0.190673--->0.190531) \t Saving The Model\n",
            "Epoch 687 \t\t Training Accuracy: 0.9995784309133887 \t\t Validation Accuracy: 0.9980961133539676\n",
            "Validation Loss Decreased(0.190531--->0.190389) \t Saving The Model\n",
            "Epoch 688 \t\t Training Accuracy: 0.9995790281519293 \t\t Validation Accuracy: 0.9980975317955018\n",
            "Validation Loss Decreased(0.190389--->0.190247) \t Saving The Model\n",
            "Epoch 689 \t\t Training Accuracy: 0.9995796237513423 \t\t Validation Accuracy: 0.9980989460647106\n",
            "Validation Loss Decreased(0.190247--->0.190105) \t Saving The Model\n",
            "Epoch 690 \t\t Training Accuracy: 0.9995802177488804 \t\t Validation Accuracy: 0.998100357055664\n",
            "Validation Loss Decreased(0.190105--->0.189964) \t Saving The Model\n",
            "Epoch 691 \t\t Training Accuracy: 0.9995808101817966 \t\t Validation Accuracy: 0.9981017638742924\n",
            "Validation Loss Decreased(0.189964--->0.189824) \t Saving The Model\n",
            "Epoch 692 \t\t Training Accuracy: 0.9995814010500907 \t\t Validation Accuracy: 0.9981031669676304\n",
            "Validation Loss Decreased(0.189824--->0.189683) \t Saving The Model\n",
            "Epoch 693 \t\t Training Accuracy: 0.9995819904282689 \t\t Validation Accuracy: 0.9981045670807361\n",
            "Validation Loss Decreased(0.189683--->0.189543) \t Saving The Model\n",
            "Epoch 694 \t\t Training Accuracy: 0.9995825779438019 \t\t Validation Accuracy: 0.9981059628725052\n",
            "Validation Loss Decreased(0.189543--->0.189404) \t Saving The Model\n",
            "Epoch 695 \t\t Training Accuracy: 0.9995831641554832 \t\t Validation Accuracy: 0.9981073553860188\n",
            "Validation Loss Decreased(0.189404--->0.189264) \t Saving The Model\n",
            "Epoch 696 \t\t Training Accuracy: 0.999583749063313 \t\t Validation Accuracy: 0.9981087438762188\n",
            "Validation Loss Decreased(0.189264--->0.189126) \t Saving The Model\n",
            "Epoch 697 \t\t Training Accuracy: 0.9995843321084976 \t\t Validation Accuracy: 0.9981101290881633\n",
            "Validation Loss Decreased(0.189126--->0.188987) \t Saving The Model\n",
            "Epoch 698 \t\t Training Accuracy: 0.9995849136635662 \t\t Validation Accuracy: 0.9981115102767945\n",
            "Validation Loss Decreased(0.188987--->0.188849) \t Saving The Model\n",
            "Epoch 699 \t\t Training Accuracy: 0.9995854936912656 \t\t Validation Accuracy: 0.9981128887832165\n",
            "Validation Loss Decreased(0.188849--->0.188711) \t Saving The Model\n",
            "Epoch 700 \t\t Training Accuracy: 0.999586072191596 \t\t Validation Accuracy: 0.9981142634153366\n",
            "Validation Loss Decreased(0.188711--->0.188574) \t Saving The Model\n",
            "Epoch 701 \t\t Training Accuracy: 0.9995866493880748 \t\t Validation Accuracy: 0.9981156335771084\n",
            "Validation Loss Decreased(0.188574--->0.188437) \t Saving The Model\n",
            "Epoch 702 \t\t Training Accuracy: 0.9995872249081731 \t\t Validation Accuracy: 0.9981170006096363\n",
            "Validation Loss Decreased(0.188437--->0.188300) \t Saving The Model\n",
            "Epoch 703 \t\t Training Accuracy: 0.9995877990499139 \t\t Validation Accuracy: 0.9981183642148972\n",
            "Validation Loss Decreased(0.188300--->0.188164) \t Saving The Model\n",
            "Epoch 704 \t\t Training Accuracy: 0.999588371552527 \t\t Validation Accuracy: 0.998119724392891\n",
            "Validation Loss Decreased(0.188164--->0.188028) \t Saving The Model\n",
            "Epoch 705 \t\t Training Accuracy: 0.999588942565024 \t\t Validation Accuracy: 0.9981210811436176\n",
            "Validation Loss Decreased(0.188028--->0.187892) \t Saving The Model\n",
            "Epoch 706 \t\t Training Accuracy: 0.9995895122364163 \t\t Validation Accuracy: 0.9981224335730076\n",
            "Validation Loss Decreased(0.187892--->0.187757) \t Saving The Model\n",
            "Epoch 707 \t\t Training Accuracy: 0.9995900803804397 \t\t Validation Accuracy: 0.9981237833201885\n",
            "Validation Loss Decreased(0.187757--->0.187622) \t Saving The Model\n",
            "Epoch 708 \t\t Training Accuracy: 0.9995906470343471 \t\t Validation Accuracy: 0.9981251294910908\n",
            "Validation Loss Decreased(0.187622--->0.187487) \t Saving The Model\n",
            "Epoch 709 \t\t Training Accuracy: 0.9995912121608853 \t\t Validation Accuracy: 0.9981264720857144\n",
            "Validation Loss Decreased(0.187487--->0.187353) \t Saving The Model\n",
            "Epoch 710 \t\t Training Accuracy: 0.9995917759463191 \t\t Validation Accuracy: 0.9981278111040592\n",
            "Validation Loss Decreased(0.187353--->0.187219) \t Saving The Model\n",
            "Epoch 711 \t\t Training Accuracy: 0.9995923382416367 \t\t Validation Accuracy: 0.998129146695137\n",
            "Validation Loss Decreased(0.187219--->0.187085) \t Saving The Model\n",
            "Epoch 712 \t\t Training Accuracy: 0.9995928988233209 \t\t Validation Accuracy: 0.9981304793059826\n",
            "Validation Loss Decreased(0.187085--->0.186952) \t Saving The Model\n",
            "Epoch 713 \t\t Training Accuracy: 0.9995934584364295 \t\t Validation Accuracy: 0.9981318075954914\n",
            "Validation Loss Decreased(0.186952--->0.186819) \t Saving The Model\n",
            "Epoch 714 \t\t Training Accuracy: 0.9995940163731575 \t\t Validation Accuracy: 0.9981331330537796\n",
            "Validation Loss Decreased(0.186819--->0.186687) \t Saving The Model\n",
            "Epoch 715 \t\t Training Accuracy: 0.999594572596252 \t\t Validation Accuracy: 0.9981344550848007\n",
            "Validation Loss Decreased(0.186687--->0.186554) \t Saving The Model\n",
            "Epoch 716 \t\t Training Accuracy: 0.999595127850771 \t\t Validation Accuracy: 0.9981357742846012\n",
            "Validation Loss Decreased(0.186554--->0.186423) \t Saving The Model\n",
            "Epoch 717 \t\t Training Accuracy: 0.9995956814289093 \t\t Validation Accuracy: 0.9981370894610881\n",
            "Validation Loss Decreased(0.186423--->0.186291) \t Saving The Model\n",
            "Epoch 718 \t\t Training Accuracy: 0.9995962336286902 \t\t Validation Accuracy: 0.9981384010612965\n",
            "Validation Loss Decreased(0.186291--->0.186160) \t Saving The Model\n",
            "Epoch 719 \t\t Training Accuracy: 0.9995967843011022 \t\t Validation Accuracy: 0.9981397098302841\n",
            "Validation Loss Decreased(0.186160--->0.186029) \t Saving The Model\n",
            "Epoch 720 \t\t Training Accuracy: 0.9995973337069154 \t\t Validation Accuracy: 0.9981410150229931\n",
            "Validation Loss Decreased(0.186029--->0.185898) \t Saving The Model\n",
            "Epoch 721 \t\t Training Accuracy: 0.9995978816971183 \t\t Validation Accuracy: 0.9981423173844814\n",
            "Validation Loss Decreased(0.185898--->0.185768) \t Saving The Model\n",
            "Epoch 722 \t\t Training Accuracy: 0.9995984284207224 \t\t Validation Accuracy: 0.9981436157226562\n",
            "Validation Loss Decreased(0.185768--->0.185638) \t Saving The Model\n",
            "Epoch 723 \t\t Training Accuracy: 0.9995989733561873 \t\t Validation Accuracy: 0.998144911378622\n",
            "Validation Loss Decreased(0.185638--->0.185509) \t Saving The Model\n",
            "Epoch 724 \t\t Training Accuracy: 0.9995995173975826 \t\t Validation Accuracy: 0.9981462031602859\n",
            "Validation Loss Decreased(0.185509--->0.185380) \t Saving The Model\n",
            "Epoch 725 \t\t Training Accuracy: 0.9996000596508384 \t\t Validation Accuracy: 0.9981474919617176\n",
            "Validation Loss Decreased(0.185380--->0.185251) \t Saving The Model\n",
            "Epoch 726 \t\t Training Accuracy: 0.9996006006002426 \t\t Validation Accuracy: 0.9981487774848938\n",
            "Validation Loss Decreased(0.185251--->0.185122) \t Saving The Model\n",
            "Epoch 727 \t\t Training Accuracy: 0.9996011404320597 \t\t Validation Accuracy: 0.9981500597298145\n",
            "Validation Loss Decreased(0.185122--->0.184994) \t Saving The Model\n",
            "Epoch 728 \t\t Training Accuracy: 0.9996016786620021 \t\t Validation Accuracy: 0.9981513391435146\n",
            "Validation Loss Decreased(0.184994--->0.184866) \t Saving The Model\n",
            "Epoch 729 \t\t Training Accuracy: 0.9996022155880928 \t\t Validation Accuracy: 0.9981526152789593\n",
            "Validation Loss Decreased(0.184866--->0.184738) \t Saving The Model\n",
            "Epoch 730 \t\t Training Accuracy: 0.9996027513220906 \t\t Validation Accuracy: 0.998153887540102\n",
            "Validation Loss Decreased(0.184738--->0.184611) \t Saving The Model\n",
            "Epoch 731 \t\t Training Accuracy: 0.9996032851934433 \t\t Validation Accuracy: 0.9981551571190357\n",
            "Validation Loss Decreased(0.184611--->0.184484) \t Saving The Model\n",
            "Epoch 732 \t\t Training Accuracy: 0.9996038184314966 \t\t Validation Accuracy: 0.9981564228236676\n",
            "Validation Loss Decreased(0.184484--->0.184358) \t Saving The Model\n",
            "Epoch 733 \t\t Training Accuracy: 0.9996043499559164 \t\t Validation Accuracy: 0.9981576861441135\n",
            "Validation Loss Decreased(0.184358--->0.184231) \t Saving The Model\n",
            "Epoch 734 \t\t Training Accuracy: 0.9996048799529672 \t\t Validation Accuracy: 0.9981589460372925\n",
            "Validation Loss Decreased(0.184231--->0.184105) \t Saving The Model\n",
            "Epoch 735 \t\t Training Accuracy: 0.9996054087951779 \t\t Validation Accuracy: 0.9981602029502392\n",
            "Validation Loss Decreased(0.184105--->0.183980) \t Saving The Model\n",
            "Epoch 736 \t\t Training Accuracy: 0.9996059364452958 \t\t Validation Accuracy: 0.9981614565849304\n",
            "Validation Loss Decreased(0.183980--->0.183854) \t Saving The Model\n",
            "Epoch 737 \t\t Training Accuracy: 0.9996064627543092 \t\t Validation Accuracy: 0.9981627064943314\n",
            "Validation Loss Decreased(0.183854--->0.183729) \t Saving The Model\n",
            "Epoch 738 \t\t Training Accuracy: 0.9996069875359536 \t\t Validation Accuracy: 0.9981639541685581\n",
            "Validation Loss Decreased(0.183729--->0.183605) \t Saving The Model\n",
            "Epoch 739 \t\t Training Accuracy: 0.9996075109764934 \t\t Validation Accuracy: 0.9981651985645295\n",
            "Validation Loss Decreased(0.183605--->0.183480) \t Saving The Model\n",
            "Epoch 740 \t\t Training Accuracy: 0.9996080332621932 \t\t Validation Accuracy: 0.9981664396822453\n",
            "Validation Loss Decreased(0.183480--->0.183356) \t Saving The Model\n",
            "Epoch 741 \t\t Training Accuracy: 0.9996085543558002 \t\t Validation Accuracy: 0.9981676781177521\n",
            "Validation Loss Decreased(0.183356--->0.183232) \t Saving The Model\n",
            "Epoch 742 \t\t Training Accuracy: 0.9996090738847851 \t\t Validation Accuracy: 0.998168912678957\n",
            "Validation Loss Decreased(0.183232--->0.183109) \t Saving The Model\n",
            "Epoch 743 \t\t Training Accuracy: 0.9996095922589302 \t\t Validation Accuracy: 0.9981701447069645\n",
            "Validation Loss Decreased(0.183109--->0.182986) \t Saving The Model\n",
            "Epoch 744 \t\t Training Accuracy: 0.9996101094409824 \t\t Validation Accuracy: 0.9981713736057282\n",
            "Validation Loss Decreased(0.182986--->0.182863) \t Saving The Model\n",
            "Epoch 745 \t\t Training Accuracy: 0.9996106250956655 \t\t Validation Accuracy: 0.9981725996732712\n",
            "Validation Loss Decreased(0.182863--->0.182740) \t Saving The Model\n",
            "Epoch 746 \t\t Training Accuracy: 0.9996111397817731 \t\t Validation Accuracy: 0.998173822015524\n",
            "Validation Loss Decreased(0.182740--->0.182618) \t Saving The Model\n",
            "Epoch 747 \t\t Training Accuracy: 0.9996116527542472 \t\t Validation Accuracy: 0.9981750421226024\n",
            "Validation Loss Decreased(0.182618--->0.182496) \t Saving The Model\n",
            "Epoch 748 \t\t Training Accuracy: 0.99961216468364 \t\t Validation Accuracy: 0.9981762592494487\n",
            "Validation Loss Decreased(0.182496--->0.182374) \t Saving The Model\n",
            "Epoch 749 \t\t Training Accuracy: 0.9996126754581929 \t\t Validation Accuracy: 0.9981774735450745\n",
            "Validation Loss Decreased(0.182374--->0.182253) \t Saving The Model\n",
            "Epoch 750 \t\t Training Accuracy: 0.9996131848916412 \t\t Validation Accuracy: 0.9981786842644215\n",
            "Validation Loss Decreased(0.182253--->0.182132) \t Saving The Model\n",
            "Epoch 751 \t\t Training Accuracy: 0.9996136931702495 \t\t Validation Accuracy: 0.9981798921525479\n",
            "Validation Loss Decreased(0.182132--->0.182011) \t Saving The Model\n",
            "Epoch 752 \t\t Training Accuracy: 0.9996142000332475 \t\t Validation Accuracy: 0.9981810973584652\n",
            "Validation Loss Decreased(0.182011--->0.181890) \t Saving The Model\n",
            "Epoch 753 \t\t Training Accuracy: 0.9996147057414055 \t\t Validation Accuracy: 0.9981822992861271\n",
            "Validation Loss Decreased(0.181890--->0.181770) \t Saving The Model\n",
            "Epoch 754 \t\t Training Accuracy: 0.9996152102574706 \t\t Validation Accuracy: 0.9981834983825684\n",
            "Validation Loss Decreased(0.181770--->0.181650) \t Saving The Model\n",
            "Epoch 755 \t\t Training Accuracy: 0.9996157132461667 \t\t Validation Accuracy: 0.9981846944987773\n",
            "Validation Loss Decreased(0.181650--->0.181531) \t Saving The Model\n",
            "Epoch 756 \t\t Training Accuracy: 0.9996162151917816 \t\t Validation Accuracy: 0.998185887336731\n",
            "Validation Loss Decreased(0.181531--->0.181411) \t Saving The Model\n",
            "Epoch 757 \t\t Training Accuracy: 0.9996167160198093 \t\t Validation Accuracy: 0.9981870776414872\n",
            "Validation Loss Decreased(0.181411--->0.181292) \t Saving The Model\n",
            "Epoch 758 \t\t Training Accuracy: 0.9996172154322267 \t\t Validation Accuracy: 0.9981882654130458\n",
            "Validation Loss Decreased(0.181292--->0.181173) \t Saving The Model\n",
            "Epoch 759 \t\t Training Accuracy: 0.9996177135407924 \t\t Validation Accuracy: 0.9981894500553607\n",
            "Validation Loss Decreased(0.181173--->0.181055) \t Saving The Model\n",
            "Epoch 760 \t\t Training Accuracy: 0.9996182107925415 \t\t Validation Accuracy: 0.9981906314194202\n",
            "Validation Loss Decreased(0.181055--->0.180937) \t Saving The Model\n",
            "Epoch 761 \t\t Training Accuracy: 0.9996187065169215 \t\t Validation Accuracy: 0.9981918103992939\n",
            "Validation Loss Decreased(0.180937--->0.180819) \t Saving The Model\n",
            "Epoch 762 \t\t Training Accuracy: 0.9996192010119558 \t\t Validation Accuracy: 0.9981929861009121\n",
            "Validation Loss Decreased(0.180819--->0.180701) \t Saving The Model\n",
            "Epoch 763 \t\t Training Accuracy: 0.9996196945011616 \t\t Validation Accuracy: 0.998194158822298\n",
            "Validation Loss Decreased(0.180701--->0.180584) \t Saving The Model\n",
            "Epoch 764 \t\t Training Accuracy: 0.9996201866865158 \t\t Validation Accuracy: 0.998195328861475\n",
            "Validation Loss Decreased(0.180584--->0.180467) \t Saving The Model\n",
            "Epoch 765 \t\t Training Accuracy: 0.9996206778287887 \t\t Validation Accuracy: 0.9981964965164661\n",
            "Validation Loss Decreased(0.180467--->0.180350) \t Saving The Model\n",
            "Epoch 766 \t\t Training Accuracy: 0.9996211677789688 \t\t Validation Accuracy: 0.9981976610422134\n",
            "Validation Loss Decreased(0.180350--->0.180234) \t Saving The Model\n",
            "Epoch 767 \t\t Training Accuracy: 0.9996216562017799 \t\t Validation Accuracy: 0.9981988228857517\n",
            "Validation Loss Decreased(0.180234--->0.180118) \t Saving The Model\n",
            "Epoch 768 \t\t Training Accuracy: 0.9996221436187624 \t\t Validation Accuracy: 0.9981999821960926\n",
            "Validation Loss Decreased(0.180118--->0.180002) \t Saving The Model\n",
            "Epoch 769 \t\t Training Accuracy: 0.9996226298436522 \t\t Validation Accuracy: 0.998201138228178\n",
            "Validation Loss Decreased(0.180002--->0.179886) \t Saving The Model\n",
            "Epoch 770 \t\t Training Accuracy: 0.9996231150254608 \t\t Validation Accuracy: 0.9982022918760777\n",
            "Validation Loss Decreased(0.179886--->0.179771) \t Saving The Model\n",
            "Epoch 771 \t\t Training Accuracy: 0.9996235986426473 \t\t Validation Accuracy: 0.9982034422457218\n",
            "Validation Loss Decreased(0.179771--->0.179656) \t Saving The Model\n",
            "Epoch 772 \t\t Training Accuracy: 0.9996240815147758 \t\t Validation Accuracy: 0.9982045908272267\n",
            "Validation Loss Decreased(0.179656--->0.179541) \t Saving The Model\n",
            "Epoch 773 \t\t Training Accuracy: 0.9996245631203056 \t\t Validation Accuracy: 0.9982057356834412\n",
            "Validation Loss Decreased(0.179541--->0.179426) \t Saving The Model\n",
            "Epoch 774 \t\t Training Accuracy: 0.9996250435709954 \t\t Validation Accuracy: 0.9982068784534931\n",
            "Validation Loss Decreased(0.179426--->0.179312) \t Saving The Model\n",
            "Epoch 775 \t\t Training Accuracy: 0.9996255228295923 \t\t Validation Accuracy: 0.9982080173492431\n",
            "Validation Loss Decreased(0.179312--->0.179198) \t Saving The Model\n",
            "Epoch 776 \t\t Training Accuracy: 0.9996260009333491 \t\t Validation Accuracy: 0.9982091553509236\n",
            "Validation Loss Decreased(0.179198--->0.179084) \t Saving The Model\n",
            "Epoch 777 \t\t Training Accuracy: 0.999626477919519 \t\t Validation Accuracy: 0.9982102896273136\n",
            "Validation Loss Decreased(0.179084--->0.178971) \t Saving The Model\n",
            "Epoch 778 \t\t Training Accuracy: 0.99962695363909 \t\t Validation Accuracy: 0.9982114209234715\n",
            "Validation Loss Decreased(0.178971--->0.178858) \t Saving The Model\n",
            "Epoch 779 \t\t Training Accuracy: 0.9996274283155799 \t\t Validation Accuracy: 0.9982125501334668\n",
            "Validation Loss Decreased(0.178858--->0.178745) \t Saving The Model\n",
            "Epoch 780 \t\t Training Accuracy: 0.9996279017627239 \t\t Validation Accuracy: 0.9982136769592762\n",
            "Validation Loss Decreased(0.178745--->0.178632) \t Saving The Model\n",
            "Epoch 781 \t\t Training Accuracy: 0.999628374390304 \t\t Validation Accuracy: 0.9982147993147373\n",
            "Validation Loss Decreased(0.178632--->0.178520) \t Saving The Model\n",
            "Epoch 782 \t\t Training Accuracy: 0.9996288456395268 \t\t Validation Accuracy: 0.9982159203290939\n",
            "Validation Loss Decreased(0.178520--->0.178408) \t Saving The Model\n",
            "Epoch 783 \t\t Training Accuracy: 0.9996293155476451 \t\t Validation Accuracy: 0.9982170389592647\n",
            "Validation Loss Decreased(0.178408--->0.178296) \t Saving The Model\n",
            "Epoch 784 \t\t Training Accuracy: 0.9996297847852111 \t\t Validation Accuracy: 0.998218154758215\n",
            "Validation Loss Decreased(0.178296--->0.178185) \t Saving The Model\n",
            "Epoch 785 \t\t Training Accuracy: 0.9996302526816726 \t\t Validation Accuracy: 0.9982192671298981\n",
            "Validation Loss Decreased(0.178185--->0.178073) \t Saving The Model\n",
            "Epoch 786 \t\t Training Accuracy: 0.9996307193115354 \t\t Validation Accuracy: 0.998220375329256\n",
            "Validation Loss Decreased(0.178073--->0.177962) \t Saving The Model\n",
            "Epoch 787 \t\t Training Accuracy: 0.9996311851218342 \t\t Validation Accuracy: 0.9982214796543122\n",
            "Validation Loss Decreased(0.177962--->0.177852) \t Saving The Model\n",
            "Epoch 788 \t\t Training Accuracy: 0.9996316495910287 \t\t Validation Accuracy: 0.9982225804030895\n",
            "Validation Loss Decreased(0.177852--->0.177742) \t Saving The Model\n",
            "Epoch 789 \t\t Training Accuracy: 0.999632112979889 \t\t Validation Accuracy: 0.9982236798107624\n",
            "Validation Loss Decreased(0.177742--->0.177632) \t Saving The Model\n",
            "Epoch 790 \t\t Training Accuracy: 0.9996325754001737 \t\t Validation Accuracy: 0.9982247765362263\n",
            "Validation Loss Decreased(0.177632--->0.177522) \t Saving The Model\n",
            "Epoch 791 \t\t Training Accuracy: 0.9996330366283656 \t\t Validation Accuracy: 0.9982258696854115\n",
            "Validation Loss Decreased(0.177522--->0.177413) \t Saving The Model\n",
            "Epoch 792 \t\t Training Accuracy: 0.9996334969997406 \t\t Validation Accuracy: 0.9982269610464573\n",
            "Validation Loss Decreased(0.177413--->0.177304) \t Saving The Model\n",
            "Epoch 793 \t\t Training Accuracy: 0.9996339559927583 \t\t Validation Accuracy: 0.9982280497252941\n",
            "Validation Loss Decreased(0.177304--->0.177195) \t Saving The Model\n",
            "Epoch 794 \t\t Training Accuracy: 0.9996344139799476 \t\t Validation Accuracy: 0.9982291354238987\n",
            "Validation Loss Decreased(0.177195--->0.177086) \t Saving The Model\n",
            "Epoch 795 \t\t Training Accuracy: 0.9996348707377911 \t\t Validation Accuracy: 0.998230219334364\n",
            "Validation Loss Decreased(0.177086--->0.176978) \t Saving The Model\n",
            "Epoch 796 \t\t Training Accuracy: 0.9996353265270591 \t\t Validation Accuracy: 0.9982313002645969\n",
            "Validation Loss Decreased(0.176978--->0.176870) \t Saving The Model\n",
            "Epoch 797 \t\t Training Accuracy: 0.9996357814222574 \t\t Validation Accuracy: 0.9982323794066906\n",
            "Validation Loss Decreased(0.176870--->0.176762) \t Saving The Model\n",
            "Epoch 798 \t\t Training Accuracy: 0.9996362353488802 \t\t Validation Accuracy: 0.9982334554195404\n",
            "Validation Loss Decreased(0.176762--->0.176654) \t Saving The Model\n",
            "Epoch 799 \t\t Training Accuracy: 0.9996366876363755 \t\t Validation Accuracy: 0.9982345290482044\n",
            "Validation Loss Decreased(0.176654--->0.176547) \t Saving The Model\n",
            "Epoch 800 \t\t Training Accuracy: 0.9996371392905712 \t\t Validation Accuracy: 0.9982356010377407\n",
            "Validation Loss Decreased(0.176547--->0.176440) \t Saving The Model\n",
            "Epoch 801 \t\t Training Accuracy: 0.9996375899389386 \t\t Validation Accuracy: 0.998236669152975\n",
            "Validation Loss Decreased(0.176440--->0.176333) \t Saving The Model\n",
            "Epoch 802 \t\t Training Accuracy: 0.9996380392089486 \t\t Validation Accuracy: 0.9982377351820468\n",
            "Validation Loss Decreased(0.176333--->0.176226) \t Saving The Model\n",
            "Epoch 803 \t\t Training Accuracy: 0.9996384876593948 \t\t Validation Accuracy: 0.9982387994229793\n",
            "Validation Loss Decreased(0.176226--->0.176120) \t Saving The Model\n",
            "Epoch 804 \t\t Training Accuracy: 0.9996389348432422 \t\t Validation Accuracy: 0.9982398603856564\n",
            "Validation Loss Decreased(0.176120--->0.176014) \t Saving The Model\n",
            "Epoch 805 \t\t Training Accuracy: 0.9996393813937903 \t\t Validation Accuracy: 0.998240919560194\n",
            "Validation Loss Decreased(0.176014--->0.175908) \t Saving The Model\n",
            "Epoch 806 \t\t Training Accuracy: 0.9996398267522454 \t\t Validation Accuracy: 0.9982419757544995\n",
            "Validation Loss Decreased(0.175908--->0.175802) \t Saving The Model\n",
            "Epoch 807 \t\t Training Accuracy: 0.9996402711048722 \t\t Validation Accuracy: 0.9982430298626422\n",
            "Validation Loss Decreased(0.175802--->0.175697) \t Saving The Model\n",
            "Epoch 808 \t\t Training Accuracy: 0.9996407141909003 \t\t Validation Accuracy: 0.9982440815865994\n",
            "Validation Loss Decreased(0.175697--->0.175592) \t Saving The Model\n",
            "Epoch 809 \t\t Training Accuracy: 0.9996411563083529 \t\t Validation Accuracy: 0.998245130032301\n",
            "Validation Loss Decreased(0.175592--->0.175487) \t Saving The Model\n",
            "Epoch 810 \t\t Training Accuracy: 0.9996415975689888 \t\t Validation Accuracy: 0.9982461766898632\n",
            "Validation Loss Decreased(0.175487--->0.175382) \t Saving The Model\n",
            "Epoch 811 \t\t Training Accuracy: 0.9996420377492905 \t\t Validation Accuracy: 0.9982472212612629\n",
            "Validation Loss Decreased(0.175382--->0.175278) \t Saving The Model\n",
            "Epoch 812 \t\t Training Accuracy: 0.9996424769237637 \t\t Validation Accuracy: 0.998248263001442\n",
            "Validation Loss Decreased(0.175278--->0.175174) \t Saving The Model\n",
            "Epoch 813 \t\t Training Accuracy: 0.9996429151296615 \t\t Validation Accuracy: 0.9982493025064468\n",
            "Validation Loss Decreased(0.175174--->0.175070) \t Saving The Model\n",
            "Epoch 814 \t\t Training Accuracy: 0.9996433520689607 \t\t Validation Accuracy: 0.9982503400743008\n",
            "Validation Loss Decreased(0.175070--->0.174966) \t Saving The Model\n",
            "Epoch 815 \t\t Training Accuracy: 0.9996437883749604 \t\t Validation Accuracy: 0.9982513743638992\n",
            "Validation Loss Decreased(0.174966--->0.174863) \t Saving The Model\n",
            "Epoch 816 \t\t Training Accuracy: 0.9996442231163383 \t\t Validation Accuracy: 0.9982524068653583\n",
            "Validation Loss Decreased(0.174863--->0.174759) \t Saving The Model\n",
            "Epoch 817 \t\t Training Accuracy: 0.9996446573361755 \t\t Validation Accuracy: 0.9982534368336201\n",
            "Validation Loss Decreased(0.174759--->0.174656) \t Saving The Model\n",
            "Epoch 818 \t\t Training Accuracy: 0.9996450903639197 \t\t Validation Accuracy: 0.9982544648647308\n",
            "Validation Loss Decreased(0.174656--->0.174554) \t Saving The Model\n",
            "Epoch 819 \t\t Training Accuracy: 0.9996455225348473 \t\t Validation Accuracy: 0.9982554902136326\n",
            "Validation Loss Decreased(0.174554--->0.174451) \t Saving The Model\n",
            "Epoch 820 \t\t Training Accuracy: 0.9996459536626935 \t\t Validation Accuracy: 0.9982565133273602\n",
            "Validation Loss Decreased(0.174451--->0.174349) \t Saving The Model\n",
            "Epoch 821 \t\t Training Accuracy: 0.9996463837847114 \t\t Validation Accuracy: 0.9982575339078903\n",
            "Validation Loss Decreased(0.174349--->0.174247) \t Saving The Model\n",
            "Epoch 822 \t\t Training Accuracy: 0.999646812863648 \t\t Validation Accuracy: 0.9982585524022579\n",
            "Validation Loss Decreased(0.174247--->0.174145) \t Saving The Model\n",
            "Epoch 823 \t\t Training Accuracy: 0.9996472407504916 \t\t Validation Accuracy: 0.9982595686614514\n",
            "Validation Loss Decreased(0.174145--->0.174043) \t Saving The Model\n",
            "Epoch 824 \t\t Training Accuracy: 0.9996476681530475 \t\t Validation Accuracy: 0.9982605831325054\n",
            "Validation Loss Decreased(0.174043--->0.173942) \t Saving The Model\n",
            "Epoch 825 \t\t Training Accuracy: 0.9996480941399932 \t\t Validation Accuracy: 0.9982615946233273\n",
            "Validation Loss Decreased(0.173942--->0.173841) \t Saving The Model\n",
            "Epoch 826 \t\t Training Accuracy: 0.9996485192701221 \t\t Validation Accuracy: 0.9982626031339169\n",
            "Validation Loss Decreased(0.173841--->0.173740) \t Saving The Model\n",
            "Epoch 827 \t\t Training Accuracy: 0.999648943580687 \t\t Validation Accuracy: 0.9982636104524135\n",
            "Validation Loss Decreased(0.173740--->0.173639) \t Saving The Model\n",
            "Epoch 828 \t\t Training Accuracy: 0.9996493666246533 \t\t Validation Accuracy: 0.9982646153867245\n",
            "Validation Loss Decreased(0.173639--->0.173538) \t Saving The Model\n",
            "Epoch 829 \t\t Training Accuracy: 0.9996497892215848 \t\t Validation Accuracy: 0.9982656176388264\n",
            "Validation Loss Decreased(0.173538--->0.173438) \t Saving The Model\n",
            "Epoch 830 \t\t Training Accuracy: 0.9996502104401589 \t\t Validation Accuracy: 0.9982666181027889\n",
            "Validation Loss Decreased(0.173438--->0.173338) \t Saving The Model\n",
            "Epoch 831 \t\t Training Accuracy: 0.9996506307646632 \t\t Validation Accuracy: 0.9982676164805889\n",
            "Validation Loss Decreased(0.173338--->0.173238) \t Saving The Model\n",
            "Epoch 832 \t\t Training Accuracy: 0.9996510502323508 \t\t Validation Accuracy: 0.9982686118781566\n",
            "Validation Loss Decreased(0.173238--->0.173139) \t Saving The Model\n",
            "Epoch 833 \t\t Training Accuracy: 0.9996514685079455 \t\t Validation Accuracy: 0.9982696056365967\n",
            "Validation Loss Decreased(0.173139--->0.173039) \t Saving The Model\n",
            "Epoch 834 \t\t Training Accuracy: 0.9996518862619996 \t\t Validation Accuracy: 0.9982705976068974\n",
            "Validation Loss Decreased(0.173039--->0.172940) \t Saving The Model\n",
            "Epoch 835 \t\t Training Accuracy: 0.9996523026749492 \t\t Validation Accuracy: 0.9982715862989425\n",
            "Validation Loss Decreased(0.172940--->0.172841) \t Saving The Model\n",
            "Epoch 836 \t\t Training Accuracy: 0.9996527181938291 \t\t Validation Accuracy: 0.998272572606802\n",
            "Validation Loss Decreased(0.172841--->0.172743) \t Saving The Model\n",
            "Epoch 837 \t\t Training Accuracy: 0.9996531330421567 \t\t Validation Accuracy: 0.9982735575735568\n",
            "Validation Loss Decreased(0.172743--->0.172644) \t Saving The Model\n",
            "Epoch 838 \t\t Training Accuracy: 0.999653546847403 \t\t Validation Accuracy: 0.998274540156126\n",
            "Validation Loss Decreased(0.172644--->0.172546) \t Saving The Model\n",
            "Epoch 839 \t\t Training Accuracy: 0.9996539594605565 \t\t Validation Accuracy: 0.9982755206525326\n",
            "Validation Loss Decreased(0.172546--->0.172448) \t Saving The Model\n",
            "Epoch 840 \t\t Training Accuracy: 0.9996543714031577 \t\t Validation Accuracy: 0.9982764984667302\n",
            "Validation Loss Decreased(0.172448--->0.172350) \t Saving The Model\n",
            "Epoch 841 \t\t Training Accuracy: 0.9996547824516893 \t\t Validation Accuracy: 0.9982774740457535\n",
            "Validation Loss Decreased(0.172350--->0.172253) \t Saving The Model\n",
            "Epoch 842 \t\t Training Accuracy: 0.9996551923081278 \t\t Validation Accuracy: 0.9982784481346607\n",
            "Validation Loss Decreased(0.172253--->0.172155) \t Saving The Model\n",
            "Epoch 843 \t\t Training Accuracy: 0.9996556014940142 \t\t Validation Accuracy: 0.9982794199883938\n",
            "Validation Loss Decreased(0.172155--->0.172058) \t Saving The Model\n",
            "Epoch 844 \t\t Training Accuracy: 0.9996560095995665 \t\t Validation Accuracy: 0.9982803897559642\n",
            "Validation Loss Decreased(0.172058--->0.171961) \t Saving The Model\n",
            "Epoch 845 \t\t Training Accuracy: 0.9996564168855548 \t\t Validation Accuracy: 0.9982813566923141\n",
            "Validation Loss Decreased(0.171961--->0.171864) \t Saving The Model\n",
            "Epoch 846 \t\t Training Accuracy: 0.9996568233147264 \t\t Validation Accuracy: 0.9982823218405247\n",
            "Validation Loss Decreased(0.171864--->0.171768) \t Saving The Model\n",
            "Epoch 847 \t\t Training Accuracy: 0.9996572288870812 \t\t Validation Accuracy: 0.9982832850515843\n",
            "Validation Loss Decreased(0.171768--->0.171671) \t Saving The Model\n",
            "Epoch 848 \t\t Training Accuracy: 0.9996576331928373 \t\t Validation Accuracy: 0.998284245878458\n",
            "Validation Loss Decreased(0.171671--->0.171575) \t Saving The Model\n",
            "Epoch 849 \t\t Training Accuracy: 0.9996580370888114 \t\t Validation Accuracy: 0.9982852047681808\n",
            "Validation Loss Decreased(0.171575--->0.171480) \t Saving The Model\n",
            "Epoch 850 \t\t Training Accuracy: 0.9996584396809339 \t\t Validation Accuracy: 0.9982861615717411\n",
            "Validation Loss Decreased(0.171480--->0.171384) \t Saving The Model\n",
            "Epoch 851 \t\t Training Accuracy: 0.9996588416397572 \t\t Validation Accuracy: 0.998287115842104\n",
            "Validation Loss Decreased(0.171384--->0.171288) \t Saving The Model\n",
            "Epoch 852 \t\t Training Accuracy: 0.9996592427417635 \t\t Validation Accuracy: 0.9982880695164204\n",
            "Validation Loss Decreased(0.171288--->0.171193) \t Saving The Model\n",
            "Epoch 853 \t\t Training Accuracy: 0.9996596425771713 \t\t Validation Accuracy: 0.9982890199124813\n",
            "Validation Loss Decreased(0.171193--->0.171098) \t Saving The Model\n",
            "Epoch 854 \t\t Training Accuracy: 0.9996600419655443 \t\t Validation Accuracy: 0.9982899682223797\n",
            "Validation Loss Decreased(0.171098--->0.171003) \t Saving The Model\n",
            "Epoch 855 \t\t Training Accuracy: 0.9996604400873185 \t\t Validation Accuracy: 0.9982909148931504\n",
            "Validation Loss Decreased(0.171003--->0.170909) \t Saving The Model\n",
            "Epoch 856 \t\t Training Accuracy: 0.9996608375385404 \t\t Validation Accuracy: 0.9982918584346772\n",
            "Validation Loss Decreased(0.170909--->0.170814) \t Saving The Model\n",
            "Epoch 857 \t\t Training Accuracy: 0.9996612341701985 \t\t Validation Accuracy: 0.9982928012311458\n",
            "Validation Loss Decreased(0.170814--->0.170720) \t Saving The Model\n",
            "Epoch 858 \t\t Training Accuracy: 0.9996616299450397 \t\t Validation Accuracy: 0.9982937411963939\n",
            "Validation Loss Decreased(0.170720--->0.170626) \t Saving The Model\n",
            "Epoch 859 \t\t Training Accuracy: 0.9996620246395469 \t\t Validation Accuracy: 0.9982946795225144\n",
            "Validation Loss Decreased(0.170626--->0.170532) \t Saving The Model\n",
            "Epoch 860 \t\t Training Accuracy: 0.9996624186262488 \t\t Validation Accuracy: 0.9982956151664257\n",
            "Validation Loss Decreased(0.170532--->0.170438) \t Saving The Model\n",
            "Epoch 861 \t\t Training Accuracy: 0.9996628116071224 \t\t Validation Accuracy: 0.9982965491712094\n",
            "Validation Loss Decreased(0.170438--->0.170345) \t Saving The Model\n",
            "Epoch 862 \t\t Training Accuracy: 0.9996632036939264 \t\t Validation Accuracy: 0.9982974813878536\n",
            "Validation Loss Decreased(0.170345--->0.170252) \t Saving The Model\n",
            "Epoch 863 \t\t Training Accuracy: 0.9996635951474309 \t\t Validation Accuracy: 0.9982984113693237\n",
            "Validation Loss Decreased(0.170252--->0.170159) \t Saving The Model\n",
            "Epoch 864 \t\t Training Accuracy: 0.9996639855578542 \t\t Validation Accuracy: 0.9982993394136429\n",
            "Validation Loss Decreased(0.170159--->0.170066) \t Saving The Model\n",
            "Epoch 865 \t\t Training Accuracy: 0.9996643750742078 \t\t Validation Accuracy: 0.9983002655208111\n",
            "Validation Loss Decreased(0.170066--->0.169973) \t Saving The Model\n",
            "Epoch 866 \t\t Training Accuracy: 0.9996647637337446 \t\t Validation Accuracy: 0.9983011895418167\n",
            "Validation Loss Decreased(0.169973--->0.169881) \t Saving The Model\n",
            "Epoch 867 \t\t Training Accuracy: 0.9996651517227292 \t\t Validation Accuracy: 0.9983021111786365\n",
            "Validation Loss Decreased(0.169881--->0.169789) \t Saving The Model\n",
            "Epoch 868 \t\t Training Accuracy: 0.9996655386313796 \t\t Validation Accuracy: 0.9983030317723751\n",
            "Validation Loss Decreased(0.169789--->0.169697) \t Saving The Model\n",
            "Epoch 869 \t\t Training Accuracy: 0.9996659249067307 \t\t Validation Accuracy: 0.9983039498329163\n",
            "Validation Loss Decreased(0.169697--->0.169605) \t Saving The Model\n",
            "Epoch 870 \t\t Training Accuracy: 0.999666310288012 \t\t Validation Accuracy: 0.9983048652112484\n",
            "Validation Loss Decreased(0.169605--->0.169513) \t Saving The Model\n",
            "Epoch 871 \t\t Training Accuracy: 0.9996666946262122 \t\t Validation Accuracy: 0.9983057792484761\n",
            "Validation Loss Decreased(0.169513--->0.169422) \t Saving The Model\n",
            "Epoch 872 \t\t Training Accuracy: 0.9996670784428716 \t\t Validation Accuracy: 0.9983066914975643\n",
            "Validation Loss Decreased(0.169422--->0.169331) \t Saving The Model\n",
            "Epoch 873 \t\t Training Accuracy: 0.9996674614399672 \t\t Validation Accuracy: 0.99830760166049\n",
            "Validation Loss Decreased(0.169331--->0.169240) \t Saving The Model\n",
            "Epoch 874 \t\t Training Accuracy: 0.9996678433939814 \t\t Validation Accuracy: 0.9983085095882416\n",
            "Validation Loss Decreased(0.169240--->0.169149) \t Saving The Model\n",
            "Epoch 875 \t\t Training Accuracy: 0.9996682244166731 \t\t Validation Accuracy: 0.9983094163239002\n",
            "Validation Loss Decreased(0.169149--->0.169058) \t Saving The Model\n",
            "Epoch 876 \t\t Training Accuracy: 0.9996686047688127 \t\t Validation Accuracy: 0.9983103202283382\n",
            "Validation Loss Decreased(0.169058--->0.168968) \t Saving The Model\n",
            "Epoch 877 \t\t Training Accuracy: 0.9996689843013883 \t\t Validation Accuracy: 0.9983112223446369\n",
            "Validation Loss Decreased(0.168968--->0.168878) \t Saving The Model\n",
            "Epoch 878 \t\t Training Accuracy: 0.9996693629398942 \t\t Validation Accuracy: 0.9983121232688427\n",
            "Validation Loss Decreased(0.168878--->0.168788) \t Saving The Model\n",
            "Epoch 879 \t\t Training Accuracy: 0.9996697409078479 \t\t Validation Accuracy: 0.998313021659851\n",
            "Validation Loss Decreased(0.168788--->0.168698) \t Saving The Model\n",
            "Epoch 880 \t\t Training Accuracy: 0.9996701179817319 \t\t Validation Accuracy: 0.9983139179646969\n",
            "Validation Loss Decreased(0.168698--->0.168608) \t Saving The Model\n",
            "Epoch 881 \t\t Training Accuracy: 0.9996704941987992 \t\t Validation Accuracy: 0.9983148129284382\n",
            "Validation Loss Decreased(0.168608--->0.168519) \t Saving The Model\n",
            "Epoch 882 \t\t Training Accuracy: 0.999670869372785 \t\t Validation Accuracy: 0.9983157056570053\n",
            "Validation Loss Decreased(0.168519--->0.168429) \t Saving The Model\n",
            "Epoch 883 \t\t Training Accuracy: 0.9996712440624833 \t\t Validation Accuracy: 0.9983165961503982\n",
            "Validation Loss Decreased(0.168429--->0.168340) \t Saving The Model\n",
            "Epoch 884 \t\t Training Accuracy: 0.9996716180443763 \t\t Validation Accuracy: 0.9983174856007099\n",
            "Validation Loss Decreased(0.168340--->0.168251) \t Saving The Model\n",
            "Epoch 885 \t\t Training Accuracy: 0.9996719909086823 \t\t Validation Accuracy: 0.9983183725178242\n",
            "Validation Loss Decreased(0.168251--->0.168163) \t Saving The Model\n",
            "Epoch 886 \t\t Training Accuracy: 0.9996723632141947 \t\t Validation Accuracy: 0.9983192573487759\n",
            "Validation Loss Decreased(0.168163--->0.168074) \t Saving The Model\n",
            "Epoch 887 \t\t Training Accuracy: 0.9996727344021201 \t\t Validation Accuracy: 0.9983201408386231\n",
            "Validation Loss Decreased(0.168074--->0.167986) \t Saving The Model\n",
            "Epoch 888 \t\t Training Accuracy: 0.9996731050685048 \t\t Validation Accuracy: 0.9983210229873657\n",
            "Validation Loss Decreased(0.167986--->0.167898) \t Saving The Model\n",
            "Epoch 889 \t\t Training Accuracy: 0.9996734748780728 \t\t Validation Accuracy: 0.9983219021558761\n",
            "Validation Loss Decreased(0.167898--->0.167810) \t Saving The Model\n",
            "Epoch 890 \t\t Training Accuracy: 0.9996738438308239 \t\t Validation Accuracy: 0.9983227795362473\n",
            "Validation Loss Decreased(0.167810--->0.167722) \t Saving The Model\n",
            "Epoch 891 \t\t Training Accuracy: 0.9996742120757699 \t\t Validation Accuracy: 0.9983236555755138\n",
            "Validation Loss Decreased(0.167722--->0.167634) \t Saving The Model\n",
            "Epoch 892 \t\t Training Accuracy: 0.9996745796501636 \t\t Validation Accuracy: 0.9983245299756527\n",
            "Validation Loss Decreased(0.167634--->0.167547) \t Saving The Model\n",
            "Epoch 893 \t\t Training Accuracy: 0.9996749461814761 \t\t Validation Accuracy: 0.9983254019916058\n",
            "Validation Loss Decreased(0.167547--->0.167460) \t Saving The Model\n",
            "Epoch 894 \t\t Training Accuracy: 0.9996753120049834 \t\t Validation Accuracy: 0.9983262722194195\n",
            "Validation Loss Decreased(0.167460--->0.167373) \t Saving The Model\n",
            "Epoch 895 \t\t Training Accuracy: 0.9996756771206856 \t\t Validation Accuracy: 0.9983271403610706\n",
            "Validation Loss Decreased(0.167373--->0.167286) \t Saving The Model\n",
            "Epoch 896 \t\t Training Accuracy: 0.9996760414168239 \t\t Validation Accuracy: 0.9983280076086521\n",
            "Validation Loss Decreased(0.167286--->0.167199) \t Saving The Model\n",
            "Epoch 897 \t\t Training Accuracy: 0.9996764047816395 \t\t Validation Accuracy: 0.9983288724720478\n",
            "Validation Loss Decreased(0.167199--->0.167113) \t Saving The Model\n",
            "Epoch 898 \t\t Training Accuracy: 0.9996767676994205 \t\t Validation Accuracy: 0.9983297352492809\n",
            "Validation Loss Decreased(0.167113--->0.167026) \t Saving The Model\n",
            "Epoch 899 \t\t Training Accuracy: 0.9996771294996143 \t\t Validation Accuracy: 0.9983305971324444\n",
            "Validation Loss Decreased(0.167026--->0.166940) \t Saving The Model\n",
            "Epoch 900 \t\t Training Accuracy: 0.9996774906292558 \t\t Validation Accuracy: 0.9983314567804337\n",
            "Validation Loss Decreased(0.166940--->0.166854) \t Saving The Model\n",
            "Epoch 901 \t\t Training Accuracy: 0.9996778510883451 \t\t Validation Accuracy: 0.998332314491272\n",
            "Validation Loss Decreased(0.166854--->0.166769) \t Saving The Model\n",
            "Epoch 902 \t\t Training Accuracy: 0.9996782106533647 \t\t Validation Accuracy: 0.9983331698179245\n",
            "Validation Loss Decreased(0.166769--->0.166683) \t Saving The Model\n",
            "Epoch 903 \t\t Training Accuracy: 0.9996785697713494 \t\t Validation Accuracy: 0.998334024399519\n",
            "Validation Loss Decreased(0.166683--->0.166598) \t Saving The Model\n",
            "Epoch 904 \t\t Training Accuracy: 0.9996789277717472 \t\t Validation Accuracy: 0.9983348767459392\n",
            "Validation Loss Decreased(0.166598--->0.166512) \t Saving The Model\n",
            "Epoch 905 \t\t Training Accuracy: 0.9996792852878571 \t\t Validation Accuracy: 0.9983357274532318\n",
            "Validation Loss Decreased(0.166512--->0.166427) \t Saving The Model\n",
            "Epoch 906 \t\t Training Accuracy: 0.9996796420589089 \t\t Validation Accuracy: 0.9983365762233735\n",
            "Validation Loss Decreased(0.166427--->0.166342) \t Saving The Model\n",
            "Epoch 907 \t\t Training Accuracy: 0.9996799976751208 \t\t Validation Accuracy: 0.9983374235033989\n",
            "Validation Loss Decreased(0.166342--->0.166258) \t Saving The Model\n",
            "Epoch 908 \t\t Training Accuracy: 0.9996803529188036 \t\t Validation Accuracy: 0.9983382686972618\n",
            "Validation Loss Decreased(0.166258--->0.166173) \t Saving The Model\n",
            "Epoch 909 \t\t Training Accuracy: 0.9996807070448994 \t\t Validation Accuracy: 0.9983391119539737\n",
            "Validation Loss Decreased(0.166173--->0.166089) \t Saving The Model\n",
            "Epoch 910 \t\t Training Accuracy: 0.9996810609474778 \t\t Validation Accuracy: 0.9983399541676045\n",
            "Validation Loss Decreased(0.166089--->0.166005) \t Saving The Model\n",
            "Epoch 911 \t\t Training Accuracy: 0.9996814137324691 \t\t Validation Accuracy: 0.9983407939970493\n",
            "Validation Loss Decreased(0.166005--->0.165921) \t Saving The Model\n",
            "Epoch 912 \t\t Training Accuracy: 0.9996817660331726 \t\t Validation Accuracy: 0.9983416324853898\n",
            "Validation Loss Decreased(0.165921--->0.165837) \t Saving The Model\n",
            "Epoch 913 \t\t Training Accuracy: 0.999682117253542 \t\t Validation Accuracy: 0.9983424691855908\n",
            "Validation Loss Decreased(0.165837--->0.165753) \t Saving The Model\n",
            "Epoch 914 \t\t Training Accuracy: 0.9996824679896236 \t\t Validation Accuracy: 0.9983433036506176\n",
            "Validation Loss Decreased(0.165753--->0.165670) \t Saving The Model\n",
            "Epoch 915 \t\t Training Accuracy: 0.9996828180179 \t\t Validation Accuracy: 0.9983441372215748\n",
            "Validation Loss Decreased(0.165670--->0.165586) \t Saving The Model\n",
            "Epoch 916 \t\t Training Accuracy: 0.9996831671893597 \t\t Validation Accuracy: 0.9983449679613113\n",
            "Validation Loss Decreased(0.165586--->0.165503) \t Saving The Model\n",
            "Epoch 917 \t\t Training Accuracy: 0.9996835156157613 \t\t Validation Accuracy: 0.9983457978069782\n",
            "Validation Loss Decreased(0.165503--->0.165420) \t Saving The Model\n",
            "Epoch 918 \t\t Training Accuracy: 0.9996838635578752 \t\t Validation Accuracy: 0.9983466263115406\n",
            "Validation Loss Decreased(0.165420--->0.165337) \t Saving The Model\n",
            "Epoch 919 \t\t Training Accuracy: 0.9996842104569077 \t\t Validation Accuracy: 0.998347452133894\n",
            "Validation Loss Decreased(0.165337--->0.165255) \t Saving The Model\n",
            "Epoch 920 \t\t Training Accuracy: 0.9996845568343997 \t\t Validation Accuracy: 0.9983482764661312\n",
            "Validation Loss Decreased(0.165255--->0.165172) \t Saving The Model\n",
            "Epoch 921 \t\t Training Accuracy: 0.999684902317822 \t\t Validation Accuracy: 0.9983490997552872\n",
            "Validation Loss Decreased(0.165172--->0.165090) \t Saving The Model\n",
            "Epoch 922 \t\t Training Accuracy: 0.999685247130692 \t\t Validation Accuracy: 0.9983499209582806\n",
            "Validation Loss Decreased(0.165090--->0.165008) \t Saving The Model\n",
            "Epoch 923 \t\t Training Accuracy: 0.9996855914220214 \t\t Validation Accuracy: 0.9983507406711578\n",
            "Validation Loss Decreased(0.165008--->0.164926) \t Saving The Model\n",
            "Epoch 924 \t\t Training Accuracy: 0.9996859350427986 \t\t Validation Accuracy: 0.9983515584468842\n",
            "Validation Loss Decreased(0.164926--->0.164844) \t Saving The Model\n",
            "Epoch 925 \t\t Training Accuracy: 0.999686277769506 \t\t Validation Accuracy: 0.9983523747324944\n",
            "Validation Loss Decreased(0.164844--->0.164763) \t Saving The Model\n",
            "Epoch 926 \t\t Training Accuracy: 0.9996866196021438 \t\t Validation Accuracy: 0.9983531893789768\n",
            "Validation Loss Decreased(0.164763--->0.164681) \t Saving The Model\n",
            "Epoch 927 \t\t Training Accuracy: 0.9996869609132409 \t\t Validation Accuracy: 0.99835400223732\n",
            "Validation Loss Decreased(0.164681--->0.164600) \t Saving The Model\n",
            "Epoch 928 \t\t Training Accuracy: 0.9996873015165328 \t\t Validation Accuracy: 0.9983548133075237\n",
            "Validation Loss Decreased(0.164600--->0.164519) \t Saving The Model\n",
            "Epoch 929 \t\t Training Accuracy: 0.9996876412630081 \t\t Validation Accuracy: 0.998355623036623\n",
            "Validation Loss Decreased(0.164519--->0.164438) \t Saving The Model\n",
            "Epoch 930 \t\t Training Accuracy: 0.9996879807114601 \t\t Validation Accuracy: 0.9983564315736294\n",
            "Validation Loss Decreased(0.164438--->0.164357) \t Saving The Model\n",
            "Epoch 931 \t\t Training Accuracy: 0.9996883190609515 \t\t Validation Accuracy: 0.9983572372794152\n",
            "Validation Loss Decreased(0.164357--->0.164276) \t Saving The Model\n",
            "Epoch 932 \t\t Training Accuracy: 0.9996886567026376 \t\t Validation Accuracy: 0.9983580420911312\n",
            "Validation Loss Decreased(0.164276--->0.164196) \t Saving The Model\n",
            "Epoch 933 \t\t Training Accuracy: 0.9996889938414096 \t\t Validation Accuracy: 0.998358845859766\n",
            "Validation Loss Decreased(0.164196--->0.164115) \t Saving The Model\n",
            "Epoch 934 \t\t Training Accuracy: 0.999689330291003 \t\t Validation Accuracy: 0.9983596470952034\n",
            "Validation Loss Decreased(0.164115--->0.164035) \t Saving The Model\n",
            "Epoch 935 \t\t Training Accuracy: 0.9996896660327912 \t\t Validation Accuracy: 0.9983604469895363\n",
            "Validation Loss Decreased(0.164035--->0.163955) \t Saving The Model\n",
            "Epoch 936 \t\t Training Accuracy: 0.9996900012530386 \t\t Validation Accuracy: 0.9983612449467182\n",
            "Validation Loss Decreased(0.163955--->0.163876) \t Saving The Model\n",
            "Epoch 937 \t\t Training Accuracy: 0.9996903354488313 \t\t Validation Accuracy: 0.9983620412647725\n",
            "Validation Loss Decreased(0.163876--->0.163796) \t Saving The Model\n",
            "Epoch 938 \t\t Training Accuracy: 0.9996906692534685 \t\t Validation Accuracy: 0.9983628368377686\n",
            "Validation Loss Decreased(0.163796--->0.163716) \t Saving The Model\n",
            "Epoch 939 \t\t Training Accuracy: 0.9996910022199154 \t\t Validation Accuracy: 0.9983636297285556\n",
            "Validation Loss Decreased(0.163716--->0.163637) \t Saving The Model\n",
            "Epoch 940 \t\t Training Accuracy: 0.9996913342736662 \t\t Validation Accuracy: 0.9983644218742848\n",
            "Validation Loss Decreased(0.163637--->0.163558) \t Saving The Model\n",
            "Epoch 941 \t\t Training Accuracy: 0.9996916660107672 \t\t Validation Accuracy: 0.9983652122318745\n",
            "Validation Loss Decreased(0.163558--->0.163479) \t Saving The Model\n",
            "Epoch 942 \t\t Training Accuracy: 0.999691996872425 \t\t Validation Accuracy: 0.9983660005033016\n",
            "Validation Loss Decreased(0.163479--->0.163400) \t Saving The Model\n",
            "Epoch 943 \t\t Training Accuracy: 0.9996923272125423 \t\t Validation Accuracy: 0.9983667875826359\n",
            "Validation Loss Decreased(0.163400--->0.163321) \t Saving The Model\n",
            "Epoch 944 \t\t Training Accuracy: 0.9996926568448543 \t\t Validation Accuracy: 0.998367573171854\n",
            "Validation Loss Decreased(0.163321--->0.163243) \t Saving The Model\n",
            "Epoch 945 \t\t Training Accuracy: 0.9996929858066141 \t\t Validation Accuracy: 0.9983683568239212\n",
            "Validation Loss Decreased(0.163243--->0.163164) \t Saving The Model\n",
            "Epoch 946 \t\t Training Accuracy: 0.9996933138370514 \t\t Validation Accuracy: 0.9983691388368606\n",
            "Validation Loss Decreased(0.163164--->0.163086) \t Saving The Model\n",
            "Epoch 947 \t\t Training Accuracy: 0.9996936415694654 \t\t Validation Accuracy: 0.998369920104742\n",
            "Validation Loss Decreased(0.163086--->0.163008) \t Saving The Model\n",
            "Epoch 948 \t\t Training Accuracy: 0.9996939683891832 \t\t Validation Accuracy: 0.9983706988394261\n",
            "Validation Loss Decreased(0.163008--->0.162930) \t Saving The Model\n",
            "Epoch 949 \t\t Training Accuracy: 0.9996942947246135 \t\t Validation Accuracy: 0.9983714769780636\n",
            "Validation Loss Decreased(0.162930--->0.162852) \t Saving The Model\n",
            "Epoch 950 \t\t Training Accuracy: 0.9996946203522384 \t\t Validation Accuracy: 0.9983722530305386\n",
            "Validation Loss Decreased(0.162852--->0.162775) \t Saving The Model\n",
            "Epoch 951 \t\t Training Accuracy: 0.9996949454396963 \t\t Validation Accuracy: 0.9983730280399322\n",
            "Validation Loss Decreased(0.162775--->0.162697) \t Saving The Model\n",
            "Epoch 952 \t\t Training Accuracy: 0.9996952694654465 \t\t Validation Accuracy: 0.9983738009631634\n",
            "Validation Loss Decreased(0.162697--->0.162620) \t Saving The Model\n",
            "Epoch 953 \t\t Training Accuracy: 0.9996955931745469 \t\t Validation Accuracy: 0.9983745719492435\n",
            "Validation Loss Decreased(0.162620--->0.162543) \t Saving The Model\n",
            "Epoch 954 \t\t Training Accuracy: 0.9996959161758423 \t\t Validation Accuracy: 0.998375342041254\n",
            "Validation Loss Decreased(0.162543--->0.162466) \t Saving The Model\n",
            "Epoch 955 \t\t Training Accuracy: 0.999696238283068 \t\t Validation Accuracy: 0.9983761112391949\n",
            "Validation Loss Decreased(0.162466--->0.162389) \t Saving The Model\n",
            "Epoch 956 \t\t Training Accuracy: 0.9996965600550175 \t\t Validation Accuracy: 0.9983768777549267\n",
            "Validation Loss Decreased(0.162389--->0.162312) \t Saving The Model\n",
            "Epoch 957 \t\t Training Accuracy: 0.9996968809515238 \t\t Validation Accuracy: 0.9983776432275772\n",
            "Validation Loss Decreased(0.162312--->0.162236) \t Saving The Model\n",
            "Epoch 958 \t\t Training Accuracy: 0.9996972015313804 \t\t Validation Accuracy: 0.9983784072101116\n",
            "Validation Loss Decreased(0.162236--->0.162159) \t Saving The Model\n",
            "Epoch 959 \t\t Training Accuracy: 0.9996975211799145 \t\t Validation Accuracy: 0.9983791700005531\n",
            "Validation Loss Decreased(0.162159--->0.162083) \t Saving The Model\n",
            "Epoch 960 \t\t Training Accuracy: 0.9996978403441608 \t\t Validation Accuracy: 0.9983799307048321\n",
            "Validation Loss Decreased(0.162083--->0.162007) \t Saving The Model\n",
            "Epoch 961 \t\t Training Accuracy: 0.999698158800602 \t\t Validation Accuracy: 0.9983806903660297\n",
            "Validation Loss Decreased(0.162007--->0.161931) \t Saving The Model\n",
            "Epoch 962 \t\t Training Accuracy: 0.9996984765492379 \t\t Validation Accuracy: 0.9983814485371113\n",
            "Validation Loss Decreased(0.161931--->0.161855) \t Saving The Model\n",
            "Epoch 963 \t\t Training Accuracy: 0.9996987935900689 \t\t Validation Accuracy: 0.9983822046220303\n",
            "Validation Loss Decreased(0.161855--->0.161780) \t Saving The Model\n",
            "Epoch 964 \t\t Training Accuracy: 0.9996991100907325 \t\t Validation Accuracy: 0.9983829596638679\n",
            "Validation Loss Decreased(0.161780--->0.161704) \t Saving The Model\n",
            "Epoch 965 \t\t Training Accuracy: 0.9996994259022176 \t\t Validation Accuracy: 0.9983837132155895\n",
            "Validation Loss Decreased(0.161704--->0.161629) \t Saving The Model\n",
            "Epoch 966 \t\t Training Accuracy: 0.9996997415833175 \t\t Validation Accuracy: 0.9983844651281834\n",
            "Validation Loss Decreased(0.161629--->0.161553) \t Saving The Model\n",
            "Epoch 967 \t\t Training Accuracy: 0.9997000561654568 \t\t Validation Accuracy: 0.9983852154016495\n",
            "Validation Loss Decreased(0.161553--->0.161478) \t Saving The Model\n",
            "Epoch 968 \t\t Training Accuracy: 0.9997003700770438 \t\t Validation Accuracy: 0.9983859646320343\n",
            "Validation Loss Decreased(0.161478--->0.161404) \t Saving The Model\n",
            "Epoch 969 \t\t Training Accuracy: 0.9997006832621992 \t\t Validation Accuracy: 0.9983867122232915\n",
            "Validation Loss Decreased(0.161404--->0.161329) \t Saving The Model\n",
            "Epoch 970 \t\t Training Accuracy: 0.9997009960748255 \t\t Validation Accuracy: 0.9983874581754207\n",
            "Validation Loss Decreased(0.161329--->0.161254) \t Saving The Model\n",
            "Epoch 971 \t\t Training Accuracy: 0.999701308235526 \t\t Validation Accuracy: 0.9983882029354573\n",
            "Validation Loss Decreased(0.161254--->0.161180) \t Saving The Model\n",
            "Epoch 972 \t\t Training Accuracy: 0.999701619874686 \t\t Validation Accuracy: 0.9983889459073544\n",
            "Validation Loss Decreased(0.161180--->0.161105) \t Saving The Model\n",
            "Epoch 973 \t\t Training Accuracy: 0.9997019307874143 \t\t Validation Accuracy: 0.9983896884322166\n",
            "Validation Loss Decreased(0.161105--->0.161031) \t Saving The Model\n",
            "Epoch 974 \t\t Training Accuracy: 0.9997022409923375 \t\t Validation Accuracy: 0.9983904282748699\n",
            "Validation Loss Decreased(0.161031--->0.160957) \t Saving The Model\n",
            "Epoch 975 \t\t Training Accuracy: 0.9997025508619845 \t\t Validation Accuracy: 0.9983911667764187\n",
            "Validation Loss Decreased(0.160957--->0.160883) \t Saving The Model\n",
            "Epoch 976 \t\t Training Accuracy: 0.9997028596699238 \t\t Validation Accuracy: 0.9983919048309327\n",
            "Validation Loss Decreased(0.160883--->0.160810) \t Saving The Model\n",
            "Epoch 977 \t\t Training Accuracy: 0.9997031683288514 \t\t Validation Accuracy: 0.9983926405012608\n",
            "Validation Loss Decreased(0.160810--->0.160736) \t Saving The Model\n",
            "Epoch 978 \t\t Training Accuracy: 0.999703476075083 \t\t Validation Accuracy: 0.9983933751285076\n",
            "Validation Loss Decreased(0.160736--->0.160662) \t Saving The Model\n",
            "Epoch 979 \t\t Training Accuracy: 0.9997037832997739 \t\t Validation Accuracy: 0.9983941078186035\n",
            "Validation Loss Decreased(0.160662--->0.160589) \t Saving The Model\n",
            "Epoch 980 \t\t Training Accuracy: 0.9997040900029242 \t\t Validation Accuracy: 0.9983948402106761\n",
            "Validation Loss Decreased(0.160589--->0.160516) \t Saving The Model\n",
            "Epoch 981 \t\t Training Accuracy: 0.9997043958306313 \t\t Validation Accuracy: 0.9983955702185631\n",
            "Validation Loss Decreased(0.160516--->0.160443) \t Saving The Model\n",
            "Epoch 982 \t\t Training Accuracy: 0.9997047015093267 \t\t Validation Accuracy: 0.9983962997794151\n",
            "Validation Loss Decreased(0.160443--->0.160370) \t Saving The Model\n",
            "Epoch 983 \t\t Training Accuracy: 0.999705006480217 \t\t Validation Accuracy: 0.9983970269560813\n",
            "Validation Loss Decreased(0.160370--->0.160297) \t Saving The Model\n",
            "Epoch 984 \t\t Training Accuracy: 0.9997053105197847 \t\t Validation Accuracy: 0.9983977535367012\n",
            "Validation Loss Decreased(0.160297--->0.160225) \t Saving The Model\n",
            "Epoch 985 \t\t Training Accuracy: 0.9997056138701736 \t\t Validation Accuracy: 0.998398477435112\n",
            "Validation Loss Decreased(0.160225--->0.160152) \t Saving The Model\n",
            "Epoch 986 \t\t Training Accuracy: 0.9997059170901775 \t\t Validation Accuracy: 0.9983992008864879\n",
            "Validation Loss Decreased(0.160152--->0.160080) \t Saving The Model\n",
            "Epoch 987 \t\t Training Accuracy: 0.9997062194161117 \t\t Validation Accuracy: 0.9983999234437942\n",
            "Validation Loss Decreased(0.160080--->0.160008) \t Saving The Model\n",
            "Epoch 988 \t\t Training Accuracy: 0.9997065211832523 \t\t Validation Accuracy: 0.998400643914938\n",
            "Validation Loss Decreased(0.160008--->0.159936) \t Saving The Model\n",
            "Epoch 989 \t\t Training Accuracy: 0.9997068226523698 \t\t Validation Accuracy: 0.9984013624489307\n",
            "Validation Loss Decreased(0.159936--->0.159864) \t Saving The Model\n",
            "Epoch 990 \t\t Training Accuracy: 0.9997071231901645 \t\t Validation Accuracy: 0.9984020808339119\n",
            "Validation Loss Decreased(0.159864--->0.159792) \t Saving The Model\n",
            "Epoch 991 \t\t Training Accuracy: 0.9997074234485627 \t\t Validation Accuracy: 0.9984027971327305\n",
            "Validation Loss Decreased(0.159792--->0.159720) \t Saving The Model\n",
            "Epoch 992 \t\t Training Accuracy: 0.9997077227570117 \t\t Validation Accuracy: 0.9984035123884678\n",
            "Validation Loss Decreased(0.159720--->0.159649) \t Saving The Model\n",
            "Epoch 993 \t\t Training Accuracy: 0.9997080215625465 \t\t Validation Accuracy: 0.9984042252600193\n",
            "Validation Loss Decreased(0.159649--->0.159577) \t Saving The Model\n",
            "Epoch 994 \t\t Training Accuracy: 0.9997083200328052 \t\t Validation Accuracy: 0.9984049378335476\n",
            "Validation Loss Decreased(0.159577--->0.159506) \t Saving The Model\n",
            "Epoch 995 \t\t Training Accuracy: 0.9997086179442704 \t\t Validation Accuracy: 0.9984056489169597\n",
            "Validation Loss Decreased(0.159506--->0.159435) \t Saving The Model\n",
            "Epoch 996 \t\t Training Accuracy: 0.9997089149989188 \t\t Validation Accuracy: 0.9984063585102558\n",
            "Validation Loss Decreased(0.159435--->0.159364) \t Saving The Model\n",
            "Epoch 997 \t\t Training Accuracy: 0.9997092115320265 \t\t Validation Accuracy: 0.9984070666134357\n",
            "Validation Loss Decreased(0.159364--->0.159293) \t Saving The Model\n",
            "Epoch 998 \t\t Training Accuracy: 0.9997095075063407 \t\t Validation Accuracy: 0.9984077735245228\n",
            "Validation Loss Decreased(0.159293--->0.159223) \t Saving The Model\n",
            "Epoch 999 \t\t Training Accuracy: 0.9997098031640053 \t\t Validation Accuracy: 0.9984084792435169\n",
            "Validation Loss Decreased(0.159223--->0.159152) \t Saving The Model\n",
            "Epoch 1000 \t\t Training Accuracy: 0.9997100979089737 \t\t Validation Accuracy: 0.9984091831743718\n",
            "Validation Loss Decreased(0.159152--->0.159082) \t Saving The Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plotting accuracy"
      ],
      "metadata": {
        "id": "5zOBviK5LRAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.array([i for i in range(1000)])\n",
        "plt.plot(epochs, np.reshape(train_acc, 1000), 'g', label='Training Accuracy')\n",
        "plt.plot(epochs, np.reshape(valid_acc, 1000), 'b', label='validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XGn_mrK3LP5n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "109331c1-c539-40ae-ef1e-523e32dff31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plotting decision boundary"
      ],
      "metadata": {
        "id": "AUnoY6_HaUsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coordinates for the first input pairs\n",
        "x1_1, x2_1 = X_train[0]\n",
        "\n",
        "# coordinates for the second input pairs\n",
        "x1_2, x2_2 = X_train[1]\n",
        "\n",
        "nodes = 5\n",
        "x_coord = []\n",
        "y_coord = []\n",
        "\n",
        "for i in range(nodes):\n",
        "    w1, w2 = model.fc1.weight[i]\n",
        "    b = model.fc1.bias[i]\n",
        "    new_x1_1 = (-x2_1 * w2 - b) / w1\n",
        "    new_x1_2 = (-x2_2 * w2 - b) / w1\n",
        "    x_coord.append([new_x1_1, new_x1_2])\n",
        "    y_coord.append([x2_1, x2_2])\n"
      ],
      "metadata": {
        "id": "khGJziYAoaba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(validloader):\n",
        "    X_valid = batch[:, :2]\n",
        "    y_valid = batch[:, 2] \n",
        "    plt.scatter(X_valid[:,0],X_valid[:,1],s=40,c=y_valid)\n",
        "    plt.title('Decision boundaries with 5 nodes')\n",
        "\n",
        "for i in range(nodes):  \n",
        "    plt.plot(x_coord[i], y_coord[i])\n",
        "\n",
        "\n",
        "plt.xlim([-6, 6])\n",
        "plt.ylim([-6, 6])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "gDr--2XC2BUp",
        "outputId": "1990951b-4c8b-491a-d021-d489b7b332be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}